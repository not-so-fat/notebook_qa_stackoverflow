[{"url": "/questions/78799215/issues-with-delete-operation-and-indexing-for-milvus-standalone-collection-with", "question_title": "Issues with Delete Operation and Indexing for Milvus Standalone Collection with MMAP Enabled", "question": "We are working on a project where we have a running Milvus standalone with approximately 70M data inserted.However, we encountered some issue regarding the delete operation. We attempted to delete entities based on this code linked here:https://github.com/milvus-io/pymilvus/blob/131989ac95587ce4664109ba9a2e2323cea42f33/pymilvus/orm/collection.py#L522.Looks like the call to delete function was successful, but the entities were not deleted, so total entity number remained still the same, and collection was loaded with MMAP enabled. Why did the delete call have no effect and what is supposed to be the correct way to delete entities from collection?We also have another problem when inserting into existed collection. In the project there will be daily new data inserted into the collection, so do we need to callcreate_indexevery time new data is inserted and load again, or will Milvus build the index for new data automatically given the collection has MMAP enabled? Also, do we need to release the collection to build index for new data?Thanks in advance!", "answers": [], "num_vote": "1", "num_answer": "0", "num_view": "11"}, {"url": "/questions/78794816/return-value-empty-when-searching-with-milvus", "question_title": "Return Value Empty When Searching with Milvus", "question": "I was attempting to create a Milvus collection using this function belowdef __generate_collection(self):\n    if self.collection_name not in self.milvusClient.list_collections():\n        schema = MilvusClient.create_schema(auto_id=True, enable_dynamic_field=True, primary_field=\"id\")\n\n        schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n        schema.add_field(field_name=\"source\", datatype=DataType.VARCHAR, max_length=50000)\n        schema.add_field(field_name=\"embeddings\", datatype=DataType.FLOAT_VECTOR, dim=self.truncate_dim)\n\n        index_params = MilvusClient.prepare_index_params()\n\n        index_params.add_index(field_name=\"id\", index_type=\"STL_SORT\")\n        index_params.add_index(\n            field_name=\"embeddings\",\n            index_type=\"IVF_FLAT\",\n            metric_type=\"L2\",\n            params={\"nlist\": 1024},\n        )\n\n        self.milvusClient.create_collection(\n            collection_name=self.collection_name, schema=schema, index_params=index_params\n        )I then proceeded to inserting data into the collection with the code shown belowentities = [\n    {\"source\": \"hello\", \"embeddings\": [1,2,3,4, etc..]}\n]\ninsert_result = self.milvusClient.insert(collection_name=self.collection_name, data=entities)\nprint(insert_result)\n\n#outputs {'insert_count': 1, 'ids': [450065725884255501], 'cost': 0}Based on the outputs, the insertion of data into the collection seems successful. However, nothing is being returned when I tried to search using Milvus.def search_and_query(\n    self,\n    search_vectors: list,\n    search_field: str = \"embeddings\",\n    search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n    vector_search_limit=10,\n):\n    result = self.milvusClient.search(\n        collection_name=self.collection_name,\n        data=search_vectors,\n        search_params=search_params,\n        anns_field=search_field,\n        limit=vector_search_limit,\n        output_fields=[\"source\"],\n    )\n        \n    print(result)\n    return result\n\n# output: data: ['[]'] , extra_info: {'cost': 0}I am confused what could cause the problem. Can anyone please take a look for me?", "answers": [{"content": "A potential root cause would be your \"embeddings\" in your collection orsearch_vectorshas invalid element to compute the distance.Here's a simplified example (disclaimer: I am running it on local and myindex_typeis \"FLAT\"):Forvector_for_dbandvector_for_querypair with big distance search returned the inserted record as expectedWhen I put some pair L2 distance results nan: e.g.vector_for_query=[1, 2, 3, float(\"nan\")]it returned no results.Thus I suspect this would be one potential root cause.from pymilvus import (\n    MilvusClient,\n    DataType\n)\n\n\ndef test_search(vector_for_db, vector_for_query):\n    truncate_dim = 4\n    collection_name = \"test\"\n    vector_field_name = \"embeddings\"\n\n    client = MilvusClient(\"test.db\")\n    schema = client.create_schema(auto_id=True, enable_dynamic_field=True, primary_field=\"id\")\n    \n    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n    schema.add_field(field_name=\"source\", datatype=DataType.VARCHAR, max_length=50000)\n    schema.add_field(field_name=vector_field_name, datatype=DataType.FLOAT_VECTOR, dim=truncate_dim)\n    \n    index_params = client.prepare_index_params()\n    index_params.add_index(\n        field_name=vector_field_name,\n        index_type=\"FLAT\",\n        metric_type=\"L2\",\n        params={\"nlist\": 1024}\n    )\n    \n    if client.has_collection(collection_name=collection_name):\n        client.drop_collection(collection_name=collection_name)\n    client.create_collection(\n        collection_name=collection_name, schema=schema, index_params=index_params\n    )\n    entities = [\n        {\"source\": \"hello\", vector_field_name: vector_for_db}\n    ]\n    insert_result = client.insert(collection_name=collection_name, data=entities)\n    \n    return client.search(\n        collection_name=collection_name,\n        data=[vector_for_query],\n        search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n        anns_field=vector_field_name,\n        limit=10,\n        output_fields=[\"source\"],\n    )\n\nprint(test_search([1, 2, 3, 4], [1e10, 1e10, 1e10, 1e10])) # Returns the inserted record\nprint(test_search([1, 2, 3, 4], [1, 2, 3, float(\"nan\")])) # Results in 0 search results", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "17"}, {"url": "/questions/78791420/i-was-stuck-at-the-step-create-collection-hello-milvus-when-trying-to-test-out", "question_title": "I was stuck at the step \"Create collection hello_milvus when trying to test out the Integration between Amazon Serverless MSK and Milvus on EKS", "question": "I am having trouble setting up Crossplane to deploy a distributed Milvus and serverless MSK cluster. Both deployments appear to be successful. I am using the example script fromhttps://github.com/milvus-io/pymilvus/blob/master/examples/hello_milvus.pyto test the setup, and it runs through to the end without issues.However, when I try to test the integration, the process gets stuck at the step \"Create collection hello_milvus\". All pods are in the ready state. Does anyone have any ideas on how I can proceed or how to debug this issue?The config in values.yaml looks like thisexternalKafka:\n    enabled: true\n    brokerList: boot-uw2boktt.c3.kafka-serverless.us-west-2.amazonaws.com:9098\n    securityProtocol: SASL_SSL\n    sasl:\n      mechanisms: OAUTHBEARER", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "10"}, {"url": "/questions/78789590/why-is-my-milvus-app-sdk-latency-high-when-increasing-search-parameter-k-despit", "question_title": "Why is my Milvus app SDK latency high when increasing search parameter k, despite good proxy search latency?", "question": "I am performance testing Milvus 2.4.5 with a distributed setup and a dataset of approximately 5 million items. I'm using HNSW indexing with parameters M=16 and EF construction 100. With a search parameter k=100 and 10K QPS, the Milvus proxy search latency is around 30ms, and the app SDK latency is good.However, when I increase k to 250, the proxy search latency increases to 40-60ms, but the app SDK search latency spikes to several seconds. Has anyone experienced this issue? What might be the cause, and what parameters should we tune? If the proxy search latency is good, does it mean everything is fine with Milvus, or is there something that needs to be adjusted at the Milvus proxy nodes?", "answers": [{"content": "When you increase the topk value from 100 to 250, more id-distance pairs are returned by the proxy node. Each id-distance pair is 12 bytes. At 10K QPS and topk=100, the proxy returns approximately 11.5MB of data per second. Increasing topk to 250 results in about 28.6MB of data per second. This data increase shouldn't typically cause network bandwidth issues unless you have additional output_fields configured, which could exacerbate the problem by returning more data.", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "9"}, {"url": "/questions/78786651/i-ran-into-permission-issue-when-building-milvus-from-source-on-remote-cluster", "question_title": "I ran into permission issue when building Milvus from source on remote cluster (linux)", "question": "I would like to create a milvus server on a remote cluster. Since I prefer not to use docker there, I tried to build from source:# Clone github repository.\n$ git clone https://github.com/milvus-io/milvus.git\n\n# Install third-party dependencies.\n$ cd milvus/\n$ ./scripts/install_deps.sh\n\n# Compile Milvus.\n$ makeand then set the path sudo ldconfig /opt/milvus/internal/core/output/lib/but when i try to run the start_cluster.sh (https://github.com/milvus-io/milvus/blob/master/scripts/start_cluster.sh) file I get permission issues:Starting rootcoord...\nStarting datacoord...\nStarting datanode...\n./scripts/start_cluster.sh: line 31: /tmp/rootcoord.log: Permission denied\nStarting proxy...\n./scripts/start_cluster.sh: line 34: /tmp/datacoord.log: Permission denied\nStarting querycoord...\nStarting querynode...\nStarting indexcoord...\n./scripts/start_cluster.sh: line 40: /tmp/proxy.log: Permission denied\nStarting indexnode...\n./scripts/start_cluster.sh: line 43: /tmp/querycoord.log: Permission denied\n./scripts/start_cluster.sh: line 46: /tmp/querynode.log: Permission deniedI also tried to create a milvus database using the milvus-cli(https://github.com/zilliztech/milvus_cli), but I have an issue to connect:milvus_cli > connect -uri http://127.0.0.1:19530\nConnect to Milvus error!<MilvusException: (code=2, message=Fail connecting to server on 127.0.0.1:19530, illegal connection params or server unavailable)>I saw this issue:https://discuss.huggingface.co/t/runtime-error-milvusexception-code-2-message-fail-connecting-to-server-on-127-0-0-1-19530-timeout/45338but (I think) it does not provide a solution for me.Important to note that I have sudo rights on that cluster so should not be a permission error to connect to the server.Please feel free to ask for clarifications, probably there is some imporant information I didn't share here?Many thanks for any help from a new Milvus user!", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "18"}, {"url": "/questions/78784717/cant-load-collection-after-milvus-standalone-restart-on-docker-compose-with-aws", "question_title": "Can't Load Collection after Milvus-Standalone Restart on Docker-Compose with AWS S3 Storage", "question": "I'm running Milvus-standalone v2.2.11 via Docker Compose on Windows 10 and have configured AWS S3 storage for it:minio:\n  address: s3.us-east-2.amazonaws.com\n  port: 80\n  accessKeyID: <>\n  secretAccessKey: <>\n  useSSL: false \n  bucketName: <>After building and running it, I create a collection and load it successfully. However, when the Docker container is restarted, and I try to load the collection again, it hangs.Here are the logs:2023-07-14 14:41:44 [2023/07/14 11:41:44.295 +00:00] [INFO] [querynode/impl.go:342] [\"watchDmChannels start \"] [collectionID=442848867353100457] [nodeID=519] [channels=\"[by-dev-rootcoord-dml_4_442848867353100457v0]\"] [timeInQueue=30.841\u00b5s]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.295 +00:00] [INFO] [querynode/watch_dm_channels_task.go:84] [\"Starting WatchDmChannels ...\"] [collectionID=442848867353100457] [vChannels=\"[by-dev-rootcoord-dml_4_442848867353100457v0]\"] [replicaID=442848906787160065] [loadType=LoadCollection] [collectionName=clip_index] [metricType=IP]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.295 +00:00] [INFO] [querynode/shard_cluster_service.go:81] [\"successfully add shard cluster\"] [collectionID=442848867353100457] [replica=442848906787160065] [vchan=by-dev-rootcoord-dml_4_442848867353100457v0]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.295 +00:00] [INFO] [querynode/watch_dm_channels_task.go:243] [\"loading growing segments in WatchDmChannels...\"] [collectionID=442848867353100457] [unFlushedSegmentIDs=\"[442848867353300479]\"]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.295 +00:00] [INFO] [querynode/segment_loader.go:125] [\"segmentLoader start loading...\"] [collectionID=442848867353100457] [segmentType=Growing] [segmentNum=1] [msgID=358]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.298 +00:00] [INFO] [querynode/segment_loader.go:890] [\"predict memory and disk usage while loading (in MiB)\"] [collectionID=442848867353100457] [concurrency=1] [memLoadingUsage=360] [memUsageAfterLoad=360] [diskUsageAfterLoad=0] [currentUsedMem=360] [currentAvailableFreeMemory=10885] [currentTotalMemory=11245]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.298 +00:00] [INFO] [querynode/segment.go:240] [\"create segment\"] [collectionID=442848867353100457] [partitionID=442848867353100458] [segmentID=442848867353300479] [segmentType=Growing] [vchannel=by-dev-rootcoord-dml_4_442848867353100457v0]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.298 +00:00] [INFO] [querynode/segment_loader.go:220] [\"start to load segments in parallel\"] [collectionID=442848867353100457] [segmentType=Growing] [segmentNum=1] [concurrencyLevel=1]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.298 +00:00] [INFO] [querynode/segment_loader.go:273] [\"start loading segment data into memory\"] [collectionID=442848867353100457] [partitionID=442848867353100458] [segmentID=442848867353300479] [segmentType=Growing]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.581 +00:00] [WARN] [querynode/cgo_helper.go:56] [\"LoadFieldData failed, C Runtime Exception: [UnexpectedError] Error:GetObjectSize[errcode:400, exception:, errmessage:No response body.]\\n\"]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.585 +00:00] [INFO] [gc/gc_tuner.go:84] [\"GC Tune done\"] [\"previous GOGC\"=200] [\"heapuse \"=55] [\"total memory\"=360] [\"next GC\"=137] [\"new GOGC\"=200] [gc-pause=60.835\u00b5s] [gc-pause-end=1689334904583782139]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.586 +00:00] [ERROR] [querynode/segment_loader.go:205] [\"load segment failed when load data into memory\"] [collectionID=442848867353100457] [segmentType=Growing] [partitionID=442848867353100458] [segmentID=442848867353300479] [error=\"[UnexpectedError] Error:GetObjectSize[errcode:400, exception:, errmessage:No response body.]\"] [stack=\"github.com/milvus-io/milvus/internal/querynode.(*segmentLoader).LoadSegment.func3\\n\\t/go/src/github.com/milvus-io/milvus/internal/querynode/segment_loader.go:205\\ngithub.com/milvus-io/milvus/internal/util/funcutil.ProcessFuncParallel.func3\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/funcutil/parallel.go:83\"]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.586 +00:00] [ERROR] [funcutil/parallel.go:85] [loadSegmentFunc] [error=\"[UnexpectedError] Error:GetObjectSize[errcode:400, exception:, errmessage:No response body.]\"] [idx=0] [stack=\"github.com/milvus-io/milvus/internal/util/funcutil.ProcessFuncParallel.func3\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/funcutil/parallel.go:85\"]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.586 +00:00] [DEBUG] [funcutil/parallel.go:51] [loadSegmentFunc] [total=1] [\"time cost\"=288.075209ms]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.586 +00:00] [INFO] [querynode/segment.go:289] [\"delete segment from memory\"] [collectionID=442848867353100457] [partitionID=442848867353100458] [segmentID=442848867353300479] [segmentType=Growing]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.590 +00:00] [WARN] [querynode/watch_dm_channels_task.go:249] [\"failed to load segment\"] [collection=442848867353100457] [error=\"[UnexpectedError] Error:GetObjectSize[errcode:400, exception:, errmessage:No response body.]\"]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.590 +00:00] [INFO] [querynode/shard_cluster.go:185] [\"Close shard cluster\"] [collectionID=442848867353100457] [channel=by-dev-rootcoord-dml_4_442848867353100457v0] [replicaID=442848906787160065]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.590 +00:00] [INFO] [gc/gc_tuner.go:84] [\"GC Tune done\"] [\"previous GOGC\"=200] [\"heapuse \"=55] [\"total memory\"=360] [\"next GC\"=136] [\"new GOGC\"=200] [gc-pause=43.943\u00b5s] [gc-pause-end=1689334904588677729]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.590 +00:00] [INFO] [querynode/shard_cluster.go:394] [\"Shard Cluster update state\"] [collectionID=442848867353100457] [channel=by-dev-rootcoord-dml_4_442848867353100457v0] [replicaID=442848906787160065] [\"old state\"=2] [\"new state\"=2] [caller=github.com/milvus-io/milvus/internal/querynode.(*ShardCluster).Close.func1]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.590 +00:00] [INFO] [querynode/collection.go:153] [\"remove vChannel from collection\"] [collectionID=442848867353100457] [channel=by-dev-rootcoord-dml_4_442848867353100457v0]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.590 +00:00] [WARN] [querynode/impl.go:360] [\"failed to subscribe channel\"] [collectionID=442848867353100457] [nodeID=519] [channels=\"[by-dev-rootcoord-dml_4_442848867353100457v0]\"] [error=\"failed to load growing segments, err: [UnexpectedError] Error:GetObjectSize[errcode:400, exception:, errmessage:No response body.]\"]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.591 +00:00] [WARN] [retry/retry.go:44] [\"retry func failed\"] [\"retry time\"=0] [error=\"failed to load growing segments, err: [UnexpectedError] Error:GetObjectSize[errcode:400, exception:, errmessage:No response body.]\"]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.591 +00:00] [WARN] [task/executor.go:445] [\"failed to subscribe DmChannel\"] [taskID=358] [collectionID=442848867353100457] [channel=by-dev-rootcoord-dml_4_442848867353100457v0] [node=519] [source=1] [reason=\"failed to load growing segments, err: [UnexpectedError] Error:GetObjectSize[errcode:400, exception:, errmessage:No response body.]\"]\n2023-07-14 14:41:44 [2023/07/14 11:41:44.591 +00:00] [INFO] [task/executor.go:209] [\"execute action done, remove it\"] [taskID=358] [step=0] [error=\"failed to subscribe DmChannel[RpcFailed]\"]I've set the following permissions on the bucket:{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::{bucket-name}\",\n                \"arn:aws:s3:::{bucket-name}/*\"\n            ]\n        }\n    ]\n}Despite this, the issue persists. I also see \"insert logs\" and \"stats logs\" folders in the S3 bucket. Is this correct? Should some index objects be written to this bucket as well?Could someone please help?", "answers": [], "num_vote": "1", "num_answer": "0", "num_view": "14"}, {"url": "/questions/78781743/i-have-trouble-using-parsefromarray-method", "question_title": "I have trouble using \"ParseFromArray()\" method", "question": "I am going through the code and under internal/core/src/query/plan.cpp there are many calls to ParseFromArray() but I am not able to figure out the source of this function.I have tried using the make generated-proto it doesn't seem to work.\nI have also tried cloning the code of milvus-proto github repo but to no result.https://github.com/milvus-io/milvus/blob/fda720b880fb77cb2bc16db37f6a1af0c189e734/internal/core/src/query/Plan.cpp#L89", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "11"}, {"url": "/questions/78779860/no-results-shown-for-image-similarity-search-with-milvus", "question_title": "No Results Shown for Image Similarity Search with Milvus", "question": "I am referring to the notebook in this following link to work on image similarity search:https://github.com/towhee-io/examples/blob/main/image/reverse_image_search/1_build_image_search_engine.ipynbBelow is the code I am working with:import csv\nfrom glob import glob\nfrom pathlib import Path\nfrom statistics import mean\n\nfrom towhee import pipe, ops, DataCollection\nfrom pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n\n\n# Towhee parameters\nMODEL = 'resnet50'\n\n# Milvus parameters\nHOST = [MY_HOST]\nPORT = [MY_PORT]\nTOPK = 5\nDIM = 2048 \nCOLLECTION_NAME = 'images'\nINDEX_TYPE = 'IVF_FLAT'\nMETRIC_TYPE = 'L2'\n\nindex_params = {\n    'metric_type': METRIC_TYPE,\n    'index_type': INDEX_TYPE,\n    'params': {\"nlist\": 2048}\n}\n\ncollection.create_index(field_name='image', index_params=index_params, index_name = \"image_index\")I only performed vectorization for images, and I proceeded with saving it with metadata# Load image path\ndef load_image(x):\n    if x.endswith('csv'):\n        with open(x) as f:\n            reader = csv.reader(f)\n            next(reader)\n            for item in reader:\n                yield item[1]\n    else:\n        for item in glob(x):\n            yield item\n            \n# Embedding pipeline\np_embed = (\n    pipe.input('src')\n        .flat_map('src', 'img_path', load_image)\n        .map('img_path', 'img', ops.image_decode())\n        .map('img', 'vec', ops.image_embedding.timm(model_name=MODEL))\n)\n\n\nimage_save_dir = [MY_IMAGE_PATH]\np_display = p_embed.output('img_path', 'img', 'vec') \nresult = DataCollection(p_display(image_save_dir))\n\n# check result\nresult.show()\nprint(result[0]['img_path'])\nprint(result[0]['vec'])    \n\nconnections.connect(alias='default', host=HOST, port=PORT)\ncollection_name = \"clothes\"\ncollection = Collection(name = collection_name)\n\nfor i, r in enumerate(result):\n    vector = r['vec']\n    collection.insert([\n        {\n            \"clothes_id\" : i,\n            \"category\" : \"top\",\n            \"color\" : \"black\",\n            \"image\" : vector,\n            \"gender\" : [\"F\"],\n            \"style\" : [\"casual\"],\n            \"thickness\" : [],\n            \"season\" : [\"spring\"]\n        }\n    ])And then I was attempting to get the ID value by doing image similarity searchp_search_pre = (\n        p_embed.map('vec', ('search_res'), ops.ann_search.milvus_client(\n                    host=HOST, port=PORT, limit=5,\n                    collection_name=\"clothes\"))\n               .map('search_res', 'pred', lambda x: [y[0] for y in x]) # get id\n)\n\np_search = p_search_pre.output('img_path', 'pred') \n\n# Search for example query image(s)\ncollection.load()\ndc = p_search('[MY_IMAGE_PATH]/test37.png')\n\n# Display search results with image paths\nDataCollection(dc).show()However, there was no results when I displayed. In order to ensure the search was done properly, I downloaded the images stored in Milvus, saved them in the path, and calculated the cosine similarity for the same image.from numpy import dot\nfrom numpy.linalg import norm\nimport numpy as np\n\ndef cos_similarity(A, B):\n    return dot(A, B) / (norm(A) * norm(B))\n\ntest_image = '[MY_IMAGE_PATH]/test38.png'\np_display = p_embed.output('img_path', 'img', 'vec') \nresult = DataCollection(p_display(image_save_dir))\n\ncollection_name = 'clothes' \ncollection = Collection(name=collection_name)\n\n# get image vector where clothes_id=38\nsave_results = collection.query(\n    expr=\"clothes_id == 38\",\n    output_fields=[\"clothes_id\", \"category\", \"color\", \"gender\", \"style\", \"thickness\", \"season\", \"image\"]\n)\n\nif save_results:\n    saved_image_vector = save_results[0][\"image\"]\n    result_vector = np.array(result[0]['vec'])\n    saved_image_vector = np.array(saved_image_vector)\n\n    # get similarity\n    similarity = cos_similarity(result_vector, saved_image_vector)\n    print(f\"cosine similarity : {similarity}\")\nelse:\n    print(\"there is no images\")The result I got was about 1. I am confused what the problem could be, and I am not sure why I cannot perform proper image similarity search.", "answers": [{"content": "I followed your code structure and attempted the code below for the section where you performed the similarity search to get the ID value and it succeeded in returning me the result.p_search_pre = (\n        p_embed.map('vec', ('search_res'), ops.ann_search.milvus_client(\n                    host=HOST, port=PORT, limit=5, collection_name=\"clothes\"))\n                .map('search_res', 'pred', lambda x: [(y[0]) for y in x]) \n)", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "13"}, {"url": "/questions/78777286/why-is-the-gpu-not-working-with-docker-2-4-5-gpu", "question_title": "Why is the GPU not working with Docker 2.4.5-gpu?", "question": "I deploy milvus-standalone with gpu,\nthis is my yaml file.deploy:resources:reservations:devices:driver: nvidiacapabilities: [\"gpu\"]device_ids: [\"0\"]I find gpu cpu is 0%, How do I make sure I'm deployed correctly?", "answers": [{"content": "please make sure your NVIDIA drivers and CUDA toolkit are properly installed on your host machine.\nThis is command to check GPU status:nvidia-smiThis will show you the GPU utilization & other details. If the command is not found or shows an error, you need to install or fix your GPU drivers and CUDA toolkit.You should have to install NVIDIA Container Toolkit, Here's you can install it:distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\nsudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart dockerUpdate your docker compose File and please ensure Docker Compose file or deployment YAML file is correctly configured to use the GPU.example:version: '3.7'\nservices:\n  milvus-standalone:\n    image: milvusdb/milvus:latest-gpu\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]run the Docker Container with gpudocker run --gpus all milvusdb/milvus:latest-gpuCheck gpu usage in the container inside your container, for this you can use nvidia-smi. or you need to install nvidia-smi inside the container only if it's not already available.Also please verify milvus configuration, check the container logs for any errors:docker logs <containerID>", "votes": 1}], "num_vote": "-1", "num_answer": "1", "num_view": "23"}, {"url": "/questions/78770526/milvus-collection-error-with-duplicate-data-when-inserted-once", "question_title": "Milvus Collection Error with Duplicate Data When Inserted Once", "question": "I am working on an image similarity search project. I performed embedding insertion and created a collection through Milvus, but the retrieved results contain duplicate images and I have only inserted embeddings once in this code. I am wondering what caused the situation and how I would proceed to fix this issue.Below is the related code segment. Happy to provide more information if that's helpful. Thanks in advance.@st.cache_resource\ndef get_milvus_client(uri):\n    logger.info(\"Setting up Milvus client\")\n    return MilvusClient(uri=uri)\n\nroot = \"./train\"\n\nextractor = load_model(\"resnet34\")\ndef insert_embeddings(client):\n    print('inserting')\n    global extractor\n    root = \"./train\"\n    for dirpath, foldername, filenames in os.walk(root):\n        for filename in filenames:\n            if filename.endswith(\".JPEG\"):\n                filepath = os.path.join(dirpath, filename)\n                img = Image.open(filepath)\n                image_embedding = extractor(img)\n                client.insert(\n                    \"image_embeddings\",\n                    {\"vector\": image_embedding, \"filename\": filepath},\n                )\n\nclient = get_milvus_client(uri=\"example.db\")\nclient.create_collection(\n            collection_name=\"image_embeddings\",\n            vector_field_name=\"vector\",\n            dimension=512,\n            auto_id=True,\n            enable_dynamic_field=True,\n            metric_type=\"COSINE\",\n        )\ninsert_embeddings(client)", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "12"}, {"url": "/questions/78759705/pymilvus-failed-to-create-new-connection-using-03f6034f5bdb454fbf42ffd5c13d5a33", "question_title": "pymilvus: Failed to create new connection using 03f6034f5bdb454fbf42ffd5c13d5a33", "question": "I am trying to make a script which summarizes the text fed it to a model and saves it on milvus.My project is extracting content from 3 digital textbooks which have at least 300 pages each. This extracted content should be used to create a vector database using MILVUS with RAPTOR indexing.It should extract the content from the selected textbooks thoroughly, ensuring that all relevant text is captured; then chunk the extracted content into short, contiguous texts of approximately 100 tokens each, preserving sentence boundaries.\nThe chunked texts should be embedded using SBERT (Sentence-BERT) to create vector representations.\nClustering the embedded chunks using Gaussian Mixture Models (GMMs) with soft clustering, allowing nodes to belong to multiple clusters.\nSummarizing the clusters using an LLM (e.g., GPT-3.5-turbo) to create concise representations of the grouped texts.\nRe-embedding the summarized texts and recursively applying the clustering and summarization process until a hierarchical tree structure is formed.\nThe resulting RAPTOR index should capture both high-level and low-level details of the textbooks, enabling efficient retrieval at different levels of granularity.\nStore the RAPTOR index in a MILVUS vector database, along with relevant metadata such as the textbook title and page number.This is my codeimport fitz  # PyMuPDF\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom pymilvus import MilvusClient, connections, utility, FieldSchema, CollectionSchema, DataType, Collection\nfrom transformers import pipeline\n\n# Initialize models and pipelines\nsbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Connect to Milvus Lite\nclient = MilvusClient(\"milvus_demo.db\")\n\ndef create_milvus_collection(collection_name):\n    if utility.has_collection(collection_name):\n        utility.drop_collection(collection_name)\n\n    fields = [\n        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n        FieldSchema(name=\"metadata\", dtype=DataType.STRING),\n    ]\n    schema = CollectionSchema(fields, \"Textbook Content Embeddings\")\n    collection = Collection(name=collection_name, schema=schema)\n    collection.create_index(\"embedding\", {\"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 1024}})\n    return collection\n\ncollection_name = \"textbook_embeddings\"\ncollection = create_milvus_collection(collection_name)\n\ndef extract_text_from_pdf(pdf_path):\n    doc = fitz.open(pdf_path)\n    text = \"\"\n    for page_num in range(len(doc)):\n        page = doc.load_page(page_num)\n        text += page.get_text()\n    return text\n\ndef chunk_text(text, chunk_size=100):\n    words = text.split()\n    chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n    return chunks\n\ndef embed_texts(texts):\n    embeddings = sbert_model.encode(texts)\n    return embeddings\n\n# Example usage\npdf_paths = [\"Introduction_to_Psychology.pdf\", \"Principles_of_Economics.pdf\", \"Fundamentals_of_Biology.pdf\"]\nall_text_chunks = []\nall_embeddings = []\n\nfor pdf_path in pdf_paths:\n    text = extract_text_from_pdf(pdf_path)\n    chunks = chunk_text(text)\n    embeddings = embed_texts(chunks)\n    \n    all_text_chunks.extend(chunks)\n    all_embeddings.extend(embeddings)\n\n# Insert data into Milvus Lite\ndata = [{\"id\": i, \"vector\": all_embeddings[i], \"text\": all_text_chunks[i], \"subject\": \"textbook\"} for i in range(len(all_embeddings))]\nres = client.insert(\n    collection_name=collection_name,\n    data=data\n)\n\n# Example search\nsearch_embedding = sbert_model.encode([\"Psychology is the scientific study of behavior and mental processes.\"])\nres = client.search(\n    collection_name=collection_name,\n    data=[search_embedding],\n    filter=\"subject == 'textbook'\",\n    limit=3,\n    output_fields=[\"text\", \"subject\"],\n)\nprint(res)\n\n# Example query\nres = client.query(\n    collection_name=collection_name,\n    filter=\"subject == 'textbook'\",\n    output_fields=[\"text\", \"subject\"],\n)\nprint(res)Now I am having this error. I have never worked with Milvus and pymilvus and have zero idea what's happening:PS F:\\text extractor answerer> py .\\process_textbooks.py    \nFailed to create new connection using: 03f6034f5bdb454fbf42ffd5c13d5a33\nTraceback (most recent call last):\n  File \"F:\\text extractor answerer\\process_textbooks.py\", line 12, in <module>\n    client = MilvusClient(\"milvus_demo.db\")\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ramin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymilvus\\milvus_client\\milvus_client.py\", line 58, in __init__\n    self._using = self._create_connection(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ramin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymilvus\\milvus_client\\milvus_client.py\", line 651, in _create_connection\n    raise ex from ex\n  File \"C:\\Users\\ramin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymilvus\\milvus_client\\milvus_client.py\", line 648, in _create_connection\n    connections.connect(using, user, password, db_name, token, uri=uri, **kwargs)\n  File \"C:\\Users\\ramin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymilvus\\orm\\connections.py\", line 378, in connect \n    from milvus_lite.server_manager import server_manager_instance\nModuleNotFoundError: No module named 'milvus_lite'", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "41"}, {"url": "/questions/78757672/embedding-and-vector-search-with-milvus", "question_title": "Embedding and Vector Search with Milvus", "question": "I am trying to make a RAG-based chatbot application that lets the user prompt in natural language and receive relevant information that can be retrieved a collection of from multiple tables, all of which are independent of each other. These tables range from either > 300 documents to 1000--10000 documents. Each table includes a \"Part Number\" attribute, along with information about that Part Number relating to whatever table it is in (for example: all repair logs of a part are on that repairlogs table). I am having trouble accurately searching through my vector database and returning the relevant rows that the user might be looking for.I am using Milvus as my vector database and all-minilm-l6-v2 as my embedding model.At first, I tested with one table and I attempted to embed the Part Number of every row/document in that table, and then embed the user query and use that embedding to search in the vector database, which only includes document IDs and embeddings of the part number in that document. This mostly worked, but due to changing project requirements for more general QnA where the part number may not be mentioned and so a question can be like: What part numbers did John Doe validate? - I would expect it to return rows that have John Doe under the Validator attribute, which I could later use as context - so I have to change how I am embedding.I tried embedding the entire document for every document in the table and storing and performing searches on those with the embedded query, but my results were not great as they returned irrelevant data. I'm not really sure how to go about this or continue; how to improve the accuracy of results with general QnA, if there are better ways to go about embedding, really anything at all helps. Disclaimer: I am really new to this field so please correct me if I am doing something incorrect from term usage to how I am approaching this.", "answers": [], "num_vote": "-1", "num_answer": "0", "num_view": "30"}, {"url": "/questions/78757621/how-to-insert-data-in-sizes-of-100-200gb-to-a-collection-faster-pymilvus-2-4-3", "question_title": "How to insert data in sizes of 100-200GB to a collection faster? (pymilvus 2.4.3)", "question": "I am currently using pymilvus 2.4.3 and my data contains sparse vector.I am currently using client.insert() but it has a 64mb rpc limit. I split my ~115GB data table into 1750 files using pyspark to a location on databricks and upload file by file. However, this takes about 1 minute per file, which means 1750 minutes will take a whopping 29 hours!How do I insert my data to a collection faster? I know there is spark-milvus connector but it currently does not support sparse vector.I also saw there was do_bulk_insert but i kept getting an error that says- taskID          : 450235310995975498,\n    - state           : Failed,\n    - row_count       : 0,\n    - infos           : {'failed_reason': 'typeutil.GetDim should not invoke on sparse vector type', 'progress_percent': '0'},\n    - id_ranges       : [],\n    - create_ts       : 2024-06-04 15:44:37\n>I think this may be a bug, not sure why the bulk insert process is looking for a dimension value for sparse vector type.Thanks!", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "16"}, {"url": "/questions/78756679/milvus-throws-oom-exception-when-trying-to-load-collection", "question_title": "Milvus throws OOM exception when trying to load collection", "question": "I am experiencing an issue when attempting to load a collection into memory. Here are the details:The collection was created from a CSV file that is 175KB in size and contains 461 rows.Inserting the entities works fine, but I am unable to load them.Here is the schema of the collection:{\n  \"auto_id\": false,\n  \"fields\": [\n    {\n      \"name\": \"catalog_veing_entity_id\",\n      \"type\": \"<DataType.INT64: 5>\",\n      \"is_primary\": true,\n      \"auto_id\": false\n    },\n    {\n      \"name\": \"document_name\",\n      \"type\": \"<DataType.VARCHAR: 21>\",\n      \"params\": {\n        \"max_length\": 200\n      }\n    },\n    {\n      \"name\": \"sql_query\",\n      \"type\": \"<DataType.VARCHAR: 21>\",\n      \"params\": {\n        \"max_length\": 10000\n      }\n },\n    {\n      \"name\": \"Question\",\n      \"description\": \"Text\",\n      \"type\": \"<DataType.VARCHAR: 21>\",\n      \"params\": {\n        \"max_length\": 10000\n      }\n    },\n    {\n      \"name\": \"question_vector_val\",\n      \"type\": \"<DataType.FLOAT_VECTOR: 101>\",\n      \"params\": {\n        \"dim\": 1536\n      }\n    }\n  ],\n  \"enable_dynamic_field\": true\n}And here is the error message provided:RPC error: [get_loading_progress], <MilvusException: (code=65535, message=show collection failed: load segment failed, OOM if load, maxSegmentSize = 0.8347320556640625 MB, concurrency = 1, memUsage = 931.40234375 MB, predictMemUsage = 932.2370758056641 MB, totalMem = 1024 MB thresholdFactor = 0.900000)>, <Time:{'RPC start': '2024-04-18 17:18:43.707672', 'RPC error': '2024-04-18 17:18:43.711645'}>\nRPC error: [wait_for_loading_collection]Any ideas why this is the case? Thanks a lot", "answers": [{"content": "In your error message, it says \"totalMem = 1024 MB\", which implies the Milvus RAM capacity is only 1GB. According to this doc here, the hardware requirement is at least 8GB RAM for Milvus standalone:https://milvus.io/docs/v2.3.x/prerequisite-docker.md#Hardware-requirements", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "17"}, {"url": "/questions/78753181/why-cant-datanode-connect-to-kafka", "question_title": "Why can't Datanode connect to kafka?", "question": "I am running Milvus 2.3.11 and using bitnami kafka for my log broker.Due to many critical vulnerabilities in bitnami/kafka:3.1.0-debian-10-r52, I upgraded kafka to 3.7.0-debian-12-r6\nAfter the upgrade datanode started throwing the following errorsFAIL|rdkafka#consumer-6| [thrd:GroupCoordinator]: GroupCoordinator: milvus-kafka-0.milvus-kafka-headless.milvus-database.svc.cluster.local:9092: Failed to \u2502 \u2502  resolve 'milvus-kafka-0.milvus-kafka-headless.milvus-database.svc.cluster.local:9092': Name or service not known (after 2ms in state CONNECT, 1 identical error(s) suppressed)I believe it is not able to connect to kafka. I rannslookup milvus-kafka-0.milvus-kafka-headless.milvus-database.svc.cluster.localon a pod in the same namespace and it was able to resolve the host. But I have not idea why my datanode cannot connect to kafka.I reverted back to the previous deployment but I keep getting the same error. I have tried restarting both kafka and datanode pods but I get the same errors.Now, I am not sure if the error started before my kafka upgrade but I noticed it after the upgrade.Thanks in advance for your help.", "answers": [{"content": "I believe it is most likely a deployment issue, specifically might be with the network configurations on the Kafka side. Make sure your error level is set correctly so the all the info logs will display accordingly.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "9"}, {"url": "/questions/78752216/why-cant-i-create-partition-in-milvus-running-on-docker-compose", "question_title": "Why can't I create partition in Milvus running on Docker Compose", "question": "I'm new to Milvus so I'm trying to get familiar with it (not Milvus Lite) and I'm running it on Docker Compose. I want to create a partition but I'm receiving an error. How can I fix this issue? Just to make clear, it's not Milvus Lite, it's normal Milvus so partitions should be supported.My codefrom pymilvus import MilvusClient, DataType\nimport json\n\nclient = MilvusClient(\n    uri=\"localhost\",\n    token=\"root:Milvus\"\n)\n\n# drop collection if exists\nif client.has_collection(collection_name=\"dockerDB\"):\n    client.drop_collection(collection_name=\"dockerDB\")\n\n# need to create schema for customized setup\ntestSchema = MilvusClient.create_schema(\n    enable_dynamic_field=True,\n    partition_key_field=\"sku\",\n    num_partitions=16\n)\n\ntestSchema.add_field(field_name=\"Auto_id\", datatype=DataType.INT64, description=\"The Primary Key\", is_primary=True,\n                     auto_id=True)\ntestSchema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, metric_type=\"COSINE\", dim=5)\ntestSchema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=8192)\ntestSchema.add_field(field_name=\"sku\", datatype=DataType.VARCHAR, max_length=32)\ntestSchema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n\n# add index\nindexParams = MilvusClient.prepare_index_params()\nindexParams.add_index(\n    field_name=\"vector\",\n    metric_type=\"COSINE\",\n    index_name=\"test-index\"\n)\n\n# creating collection named 'dockerDB'\nclient.create_collection(\n    collection_name=\"dockerDB\",\n    schema=testSchema,\n    index_params=indexParams,\n    auto_id=False\n)\n# View collections\ncollection = client.describe_collection(\n    collection_name=\"dockerDB\",\n)\n\nprint(\"TEST COLLECTION DESCRIPTION:\\n\\n\", collection)\nprint(\"\\n\\n\\n\")\n\nclient.create_partition(\n    collection_name=\"dockerDB\",\n    partition_name=\"partitionA\"\n)Error I'm receiving:grpc RpcError: [create_partition], <_MultiThreadedRendezvous: StatusCode.UNIMPLEMENTED, >, <Time:{'RPC start': '2024-07-15 23:15:07.309182', 'gRPC error': '2024-07-15 23:15:07.311418'}>\nTraceback (most recent call last):\n  File \"/var/www/html/milvus-docker-test/milvus_docker_intro.py\", line 59, in <module>\n    client.create_partition(\n  File \"/home/vagrant/.local/lib/python3.10/site-packages/pymilvus/milvus_client/milvus_client.py\", line 776, in create_partition\n    conn.create_partition(collection_name, partition_name, timeout=timeout, **kwargs)\n  File \"/home/vagrant/.local/lib/python3.10/site-packages/pymilvus/decorators.py\", line 161, in handler\n    raise e from e\n  File \"/home/vagrant/.local/lib/python3.10/site-packages/pymilvus/decorators.py\", line 143, in handler\n    return func(*args, **kwargs)\n  File \"/home/vagrant/.local/lib/python3.10/site-packages/pymilvus/decorators.py\", line 182, in handler\n    return func(self, *args, **kwargs)\n  File \"/home/vagrant/.local/lib/python3.10/site-packages/pymilvus/decorators.py\", line 91, in handler\n    raise e from e\n  File \"/home/vagrant/.local/lib/python3.10/site-packages/pymilvus/decorators.py\", line 87, in handler\n    return func(*args, **kwargs)\n  File \"/home/vagrant/.local/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py\", line 403, in create_partition\n    response = rf.result()\n  File \"/home/vagrant/.local/lib/python3.10/site-packages/grpc/_channel.py\", line 883, in result\n    raise self\ngrpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n        status = StatusCode.UNIMPLEMENTED\n        details = \"\"\n        debug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2024-07-15T23:15:07.310883188+00:00\", grpc_status:12, grpc_message:\"\"}\"", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "26"}, {"url": "/questions/78752135/inconsistent-query-results-based-on-output-fields-selection-in-milvus-dashboard", "question_title": "Inconsistent Query Results Based on Output Fields Selection in Milvus Dashboard", "question": "I'm experiencing an issue with the Milvus dashboard where the search results change based on the selected output fields.I'm working on a RAG project using text data converted into embeddings, stored in a Milvus collection with around 8000 elements. Last week, my retrieval results matched my expectations (\"good\" results), however, this week, the results have degraded (\"bad\" results).I found that when I exclude theembeddings_vectorfield from the output fields in the Milvus dashboard, I get the \"good\" results; Including theembeddings_vectorfield in the output changes the results to \"bad\".I've attached two screenshots showing the difference in the results based on the selected output fields.Any ideas on what's causing this or how to fix it?Environment:Python 3.11\npymilvus 2.3.2\nllama_index 0.8.64Thanks in advance!from llama_index.vector_stores import MilvusVectorStore\nfrom llama_index import ServiceContext, VectorStoreIndex\n\n# Some other lines..\n\n# Setup for MilvusVectorStore and query execution\nvector_store = MilvusVectorStore(uri=MILVUS_URI,\n                                 token=MILVUS_API_KEY,\n                                 collection_name=collection_name,\n                                 embedding_field='embeddings_vector',\n                                 doc_id_field='chunk_id',\n                                 similarity_metric='IP',\n                                 text_key='chunk_text')\n\nembed_model = get_embeddings()\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\nquery_engine = index.as_query_engine(similarity_top_k=5, streaming=True)\n\nrag_result = query_engine.query(prompt)Here is the \"good\" result:\"good\" resultAnd here is the \"bad\" result:\"bad\" result", "answers": [{"content": "I would like to suggest you to follow below considerations.Ensure that your Milvus collection is correctly indexed. Indexing plays a crucial role in how search results are retrieved and ordered. If the index configuration has changed or is not optimized, it might affect the retrieval quality.In your screenshots, the consistency level is set to \"Bounded\". Try experimenting with different consistency levels (e.g., \"Strong\" or \"Eventually\") to see if it impacts the results. Consistency settings can influence the real-time availability of the indexed data.Review the query parameters, especially the similarity_metric. Since you're using IP (Inner Product) as the similarity metric, ensure that your embedding vectors are normalized correctly. Inner Product search works best with normalized vectors.Verify that the embedding vectors are of consistent quality and scale. If there were changes in the embedding model or preprocessing steps, it could lead to variations in the search results.The inclusion of the embeddings_vector field in the output might affect the way Milvus scores and ranks the results. It's possible that returning the raw embeddings affects the internal ranking logic. Ensure that including this field does not inadvertently alter the search behavior.Check the Milvus server logs and performance metrics to identify any anomalies or changes in the search behavior. This might provide insights into why the results differ when the embeddings_vector field is included.Ensure that there are no version mismatches between the client (pymilvus) and the Milvus server. Sometimes, discrepancies between versions can cause unexpected behavior.As a last resort, try modifying your code to exclude the embeddings_vector field programmatically during retrieval and compare the results. This can help isolate whether the issue is indeed caused by including the embeddings in the output.Please try out this code if it helps.vector_store = MilvusVectorStore(uri=MILVUS_URI,\n                                 token=MILVUS_API_KEY,\n                                 collection_name=collection_name,\n                                 embedding_field='embeddings_vector',\n                                 doc_id_field='chunk_id',\n                                 similarity_metric='IP',\n                                 text_key='chunk_text',\n                                 output_fields=['chunk_id', 'chunk_text'])  # Exclude embeddings_vector\n\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\nquery_engine = index.as_query_engine(similarity_top_k=5, streaming=True)\n\nrag_result = query_engine.query(prompt)", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "33"}, {"url": "/questions/78751195/why-is-diskann-index-type-not-supported-on-datatype-float-vector-with-ip-metric", "question_title": "Why is DISKANN index type not supported on DataType.FLOAT_VECTOR with IP metric?", "question": "I'm attempting to build an index using the DiskANN type, but I'm encountering an error stating that the index type is not supported. Here is my schema. I'm confused because it\u2019s based on a float vector with the inner product (IP) metric, and according to the Milvus documentationhttps://milvus.io/docs/disk_index.md, this should be supported. Could someone explain why this might be happening?fields = [\n    FieldSchema(\n        name=\"id\",\n        dtype=DataType.INT64,\n        is_primary=True,\n        auto_id=False,\n        max_length=100,\n    ),\n    FieldSchema(name=\"priority\", dtype=DataType.FLOAT),\n    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=dim), ## dim = 100\n]\nschema = CollectionSchema(fields, \"points with id and priority\")\npoints = Collection(\"points\", schema, consistency_level=\"Strong\")\npoints.insert(entities)\npoints.flush()\n\nindex_params: {\n    \"index_type\": \"DISKANN\",\n    \"metric_type\": \"IP\",\n    \"params\": {},\n}\n\n\n points.create_index(\"embeddings\", index_params)", "answers": [{"content": "The default value forindexNode.enableDiskis true. This configuration determines whether disk indexing is supported. If set to true, disk indexing is enabled; if set to false, disk indexing is not supported. Disk indexing requires a hard disk. Sometimes, Milvus is deployed on hosts without a hard disk, which is why this configuration option is provided.The type of index created depends on theindex_paramsyou specify when callingcreate_index():FLAT/IVF_FLAT/HNSW/IVF_SQ8/IVF_PQ\u2014 fully in memoryDISKINDEX\u2014 partly in memory, partly on disk\nIt is not possible to store aDISKINDEXentirely in memory.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "27"}, {"url": "/questions/78748991/i-have-trouble-installing-python3-debmutate-package-which-is-dependency-for-ins", "question_title": "I have trouble Installing python3-debmutate Package, which is dependency for installing brz-debian_2.8.42_all.deb", "question": "For using build-deb.sh I need to install brz-debian_2.8.42_all.deb (this specific version).\npython3-debmutate is dependency for it.\nI'm currently facing an issue while trying to install the python3-debmutate package on my Ubuntu system. When attempting to install it using pip3, I receive the following error:ERROR: Could not find a version that satisfies the requirement python3-debmutate (from versions: none)\nERROR: No matching distribution found for python3-debmutateI attempted to clone the repository directly from GitHub (git clonehttps://github.com/example/python3-debmutate.git), but authentication failed due to changes in GitHub's authentication methods.\nI am using Ubuntu 20.04 and Python Version: 3.8.10If anyone has encountered a similar issue or knows of a workaround, I would greatly appreciate your insights and suggestions.Thank you!", "answers": [{"content": "The error message you encountered suggests that this package name is not in the pypi index. The  package you are looking for is nameddebmutateon pypi. You can also check the availability on thepypi website.To install this package use:pip install debmutate", "votes": 1}], "num_vote": "0", "num_answer": "1", "num_view": "12"}, {"url": "/questions/78748399/is-there-any-c-grpc-server-emebeded-in-the-milvus-system", "question_title": "is there any C++ grpc server emebeded in the milvus system?", "question": "I recently found an error which should be throwed from C++ code (after searching the error text write index to fd error, errorCode is).\nI know there is a HandleCStatus function that logs all the C errors in GO (as long as the C code is called in Go, any error should be logged, right?). But I cannot find any CStatus returns err logs.How could this heppen?im wondering if there is any C++ server in the system such that its C code is not handled by Go?milvus: 2.3.5", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "13"}, {"url": "/questions/78736686/running-milvus-in-cluster-mode", "question_title": "Running Milvus in cluster mode", "question": "I want to run Milvus on my local cluster.\nI thought to use docker compose file anddocker stack/docker swarm, but when I run my compose file something is not working. I can see all services started on the target nodes but for some reasonmilvus-proxylistening port is not available.$ docker ps | grep milvus_milvus-proxy\n854486cdb68a   milvusdb/milvus:v2.2.10                \"/tini -- milvus run\u2026\"   51 seconds ago   Up 49 seconds    **NO PORT HERE**    milvus_milvus-proxy.1.jzkt5tq8ck1fq34lx9odgr2jpminio & etcd listening ports are shown in the docker ps outputThese are the steps I've made:\nI defined a master node and joined a worker usingdocker swarmdocker swarm init --advertise-addr <master-ip>\ndocker swarm join --token <token> <master-ip>Then I defined the compose file:services:\n  etcd:\n    image: quay.io/coreos/etcd:v3.5.5\n    container_name: etcd\n    environment:\n      - ETCD_AUTO_COMPACTION_RETENTION=1\n      - ETCD_QUOTA_BACKEND_BYTES=4294967296\n      - ALLOW_NONE_AUTHENTICATION=yes\n      - ETCDCTL_API=3\n    ports:\n      - \"2379:2379\"\n      - \"2380:2380\"\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  minio:\n    image: minio/minio\n    container_name: minio\n    environment:\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n    command: minio server /minio_data\n    volumes:\n      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data\n    ports:\n      - \"9000:9000\"\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  milvus-proxy:\n    image: milvusdb/milvus:v2.2.10\n    container_name: milvus-proxy\n    command: [\"milvus\", \"run\", \"proxy\"]\n    environment:\n      ETCD_ENDPOINTS: <master_node_ip>:2379\n      MINIO_ADDRESS: <master_node_ip>:9000\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n      CUDA_VISIBLE_DEVICES: 0\n    ports:\n      - \"<master_node_ip>:19530:19530\"\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  milvus-rootcoord:\n    image: milvusdb/milvus:v2.2.10\n    container_name: milvus-rootcoord\n    command: [\"milvus\", \"run\", \"rootcoord\"]\n    environment:\n      ETCD_ENDPOINTS: <master_node_ip>:2379\n      MINIO_ADDRESS: <master_node_ip>:9000\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n      CUDA_VISIBLE_DEVICES: 0\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  milvus-datacoord:\n    image: milvusdb/milvus:v2.2.10\n    container_name: milvus-datacoord\n    command: [\"milvus\", \"run\", \"datacoord\"]\n    environment:\n      ETCD_ENDPOINTS: <master_node_ip>:2379\n      MINIO_ADDRESS: <master_node_ip>:9000\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n      CUDA_VISIBLE_DEVICES: 0\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  milvus-indexcoord:\n    image: milvusdb/milvus:v2.2.10\n    container_name: milvus-indexcoord\n    command: [\"milvus\", \"run\", \"indexcoord\"]\n    environment:\n      ETCD_ENDPOINTS: <master_node_ip>:2379\n      MINIO_ADDRESS: <master_node_ip>:9000\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n      CUDA_VISIBLE_DEVICES: 0\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  milvus-querycoord:\n    image: milvusdb/milvus:v2.2.10\n    container_name: milvus-querycoord\n    command: [\"milvus\", \"run\", \"querycoord\"]\n    environment:\n      ETCD_ENDPOINTS: <master_node_ip>:2379\n      MINIO_ADDRESS: <master_node_ip>:9000\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n      CUDA_VISIBLE_DEVICES: 0\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  milvus-datanode-1:\n    image: milvusdb/milvus:v2.2.10\n    container_name: milvus-datanode-1\n    command: [\"milvus\", \"run\", \"datanode\"]\n    environment:\n      ETCD_ENDPOINTS: <master_node_ip>:2379\n      MINIO_ADDRESS: <master_node_ip>:9000\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n      CUDA_VISIBLE_DEVICES: 0\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  milvus-datanode-2:\n     --same as milvus-datanode-1 config --\n        constraints: [node.hostname == worker_node]\n\n  milvus-indexnode-1:\n    image: milvusdb/milvus:v2.2.10\n    container_name: milvus-indexnode-1\n    command: [\"milvus\", \"run\", \"indexnode\"]\n    environment:\n      ETCD_ENDPOINTS: <master_node_ip>:2379\n      MINIO_ADDRESS: <master_node_ip>:9000\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n      CUDA_VISIBLE_DEVICES: 0\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  milvus-indexnode-2:\n    --same as milvus-indexnode-1 config --\n    deploy:\n      placement:\n        constraints: [node.hostname == worker_node]\n\n  milvus-querynode-1:\n    image: milvusdb/milvus:v2.2.10\n    container_name: milvus-querynode-1\n    command: [\"milvus\", \"run\", \"querynode\"]\n    environment:\n      ETCD_ENDPOINTS: <master_node_ip>:2379\n      MINIO_ADDRESS: <master_node_ip>:9000\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n      CUDA_VISIBLE_DEVICES: 0\n    deploy:\n      placement:\n        constraints: [node.hostname == master_node]\n\n  milvus-querynode-2:\n  --same as milvus-querynode-1 config --\n    deploy:\n      placement:\n        constraints: [node.hostname == worker_node]\n\nvolumes:\n  minio-data:And these are the milvus-proxy logs:[\"Proxy internal server listen on tcp\"] [port=19529]\n[2024/07/11 15:35:05.996 +00:00] [INFO] [proxy/service.go:263] [\"Proxy internal server already listen on tcp\"] [port=19529]\n[2024/07/11 15:35:05.996 +00:00] [INFO] [proxy/service.go:281] [\"create Proxy internal grpc server\"] [\"enforcement policy\"=\"{\\\"MinTime\\\":5000000000,\\\"PermitWithoutStream\\\":true}\"] [\"server parameters\"=\"{\\\"MaxConnectionIdle\\\":0,\\\"MaxConnectionAge\\\":0,\\\"MaxConnectionAgeGrace\\\":0,\\\"Time\\\":60000000000,\\\"Timeout\\\":10000000000}\"]\n[2024/07/11 15:35:05.996 +00:00] [INFO] [proxy/service.go:155] [\"Proxy server listen on tcp\"] [port=19530]\n[2024/07/11 15:35:05.996 +00:00] [INFO] [proxy/service.go:162] [\"Proxy server already listen on tcp\"] [port=19530]\n[2024/07/11 15:35:05.996 +00:00] [INFO] [proxy/service.go:170] [\"Get proxy rate limiter done\"] [port=19530]\n[2024/07/11 15:35:05.996 +00:00] [INFO] [proxy/service.go:233] [\"create Proxy grpc server\"] [\"enforcement policy\"=\"{\\\"MinTime\\\":5000000000,\\\"PermitWithoutStream\\\":true}\"] [\"server parameters\"=\"{\\\"MaxConnectionIdle\\\":0,\\\"MaxConnectionAge\\\":0,\\\"MaxConnectionAgeGrace\\\":0,\\\"Time\\\":60000000000,\\\"Timeout\\\":10000000000}\"]\n[2024/07/11 15:35:05.996 +00:00] [INFO] [proxy/service.go:363] [\"register http server of proxy\"]", "answers": [{"content": "I've used the example fromhereI run it on a single machine and it worked. I think it can be extended easily to work on multiple machine (Need to dispatch with docker swarm).\nFrom what I see my compose file was missing of a distributed messaging framework.\nThe example compose file is using Apache Pulsar for communication between the services from what I understand.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "20"}, {"url": "/questions/78733555/fit-token-to-dimension", "question_title": "Fit token to dimension", "question": "I\u2019m using Python with LangChain to transform .txt files into chunks, where each chunk contains 512 tokens.Next, I generate embeddings for each chunk and insert them into Milvus. The embedding dimension is set to 512.I initially thought that 1 token would correspond to 1 dimension, but this isn\u2019t the case. Milvus requires the full dimensionality (512) for each vector. Because of this, I am forced to produce vectors with empty dimensions when the number of tokens doesn\u2019t fully occupy the 512 dimensions, leading to errors or the need to generate filler.Does anyone have suggestions on how to effectively utilize all dimensions without needing to resort to fillers? Ideally, I\u2019d like to insert into Milvus without using all 512 dimensions, but the pymilvus library seems to require full dimensionality or it throws an error.\"Error processing text file: <MilvusException: (code=0, message=the length(5) of float data should divide the dim(512))>\"", "answers": [{"content": "To generate embeddings, we use an embedding model. So basically we send a piece of text (chunk) as input to theembedding model(NOT a tokenizer, there's a huge difference), and the embedding model always returns a vector ofFIXEDdimension as an output.The text input to the embedding model can range from a single word to the maximum token limit of the embedding model, but the output would remainCONSTANT.There is no correlation between the number of tokens in the input and the embedding size generated.Just ensure that the embedding dimension of the vector database is set equal to the embedding dimensions of the vectors generated by the embedding model you use. For example, OpenAI embeddings would ALWAYS return embedding vectors of 1536. Hence to store those in a vector database, you need to set the embedding size in the vector database to 1536.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "13"}, {"url": "/questions/78731133/starting-milvus-after-building-milvus-from-source-code", "question_title": "starting Milvus after building milvus from source code", "question": "I built milvus from source on my database cluster.For that I followed the simple instructions on themilvus repo readme:# Clone github repository.\n$ git clone https://github.com/milvus-io/milvus.git\n\n# Install third-party dependencies.\n$ cd milvus/\n$ ./scripts/install_deps.sh\n\n# Compile Milvus.\n$ makeI also set the path:sudo ldconfig /opt/milvus/internal/core/output/lib/.Then I run the command:./scripts/start_cluster.shBut I get a permission denied response (although I have sudo rights on that that cluster):Starting rootcoord...\nStarting datacoord...\nStarting datanode...\n./scripts/start_cluster.sh: line 31: /tmp/rootcoord.log: Permission denied\nStarting proxy...\n./scripts/start_cluster.sh: line 34: /tmp/datacoord.log: Permission denied\nStarting querycoord...\nStarting querynode...\nStarting indexcoord...\n./scripts/start_cluster.sh: line 40: /tmp/proxy.log: Permission denied\nStarting indexnode...\n./scripts/start_cluster.sh: line 43: /tmp/querycoord.log: Permission denied\n./scripts/start_cluster.sh: line 46: /tmp/querynode.log: Permission deniedFor convenience, here is thesource code of the start_cluster.sh file.Any help with resolving the issue will be appreciated.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "28"}, {"url": "/questions/78728453/datanotmatchexception-datanotmatchexception-code-1-message-the-input-data-t", "question_title": "DataNotMatchException: <DataNotMatchException: (code=1, message=The Input data type is inconsistent", "question": "image is existing schema from zilliz, message.txt is error log, sample_input.txt is sample input text.\nsample input langchain document object[Document(metadata={'id_pk': {'array': [288204]}, 'url': {'array': ['example.com']}, 'url_id': 'example.com', 'categories': {'array': ['culture', 'music']}, 'locations': {'array': ['france', 'brooklyn', 'new york', 'manhattan', 'socceroof', 'sunset park', 'united states']}, 'country': {'array': ['united states']}, 'province': {'array': ['new york']}, 'city': {'array': ['new york', 'brooklyn']}, 'poi': {'array': ['socceroof', 'sunset park', 'manhattan']}, 'published_year': 2024, 'content_type': {'array': ['posts']}, 'source': {}}, page_content=\"My content to be added\")]", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "30"}, {"url": "/questions/78724135/i-have-attu-forbiddenerror-and-dont-know-if-i-should-set-user", "question_title": "I have attu ForbiddenError and don't know if i should set user", "question": "I can't access milvus attu. ForbiddenError.\nand I was using attu 2.3.2 and then upgraded to attu 2.4.2.\nI haven't set anything like User, Role, etc.here is the error:POST 200 /api/v1/milvus/connect 154.498 ms @ ...\nGET /api/v1/users 403 ForbiddenError: Can not find your connection, please check your connection settings.\n GET 403 /api/v1/users 0.579 ms ...\n GET 304 /api/v1/milvus/version 0.368 ms ...My question is do I need to set user?But, when connecting with pymilvus client, it connects without any authentication such as user, token, password, etc.Thanks!", "answers": [{"content": "please upgrade your attu to the latest version.https://github.com/zilliztech/attu", "votes": 0}, {"content": "Based on the information provided, it appears that the issue may be related to a connection or authentication problem with Milvus. First, ensure that you are using Attu version 2.4.x, as it is compatible with Milvus 2.4. If you encounter a403 ForbiddenErrorwhen attempting to get Milvus user info, it could indicate a connection loss or authentication failure. To resolve this, try restarting your Attu container, as this may re-establish the connection and resolve the authentication request failure.", "votes": 0}], "num_vote": "0", "num_answer": "2", "num_view": "19"}, {"url": "/questions/78723817/optimizing-search-speed-for-vector-similarity-in-a-filtered-collection-schema", "question_title": "Optimizing Search Speed for Vector Similarity in a Filtered Collection Schema", "question": "My collection schema is as follows:fields = [\n    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=512),\n    FieldSchema(name=\"word\", dtype=DataType.VARCHAR, max_length=512),\n    FieldSchema(name=\"tag\", dtype=DataType.VARCHAR, max_length=16),\n    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=768)\n]\n\nschema = CollectionSchema(fields)I want to filter by word == \"apple\" and calculate the vector similarity among the filtered data to retrieve the tag values of the top 10 most similar data points. How can I optimize the search speed for this task?Thanks.", "answers": [{"content": "You can specifypartition_key_fieldto \"apple\" when you create the collection schema, so that Milvus distributes incoming entities into different partitions according to their respective partition values in this field. More detailed information can be found here on Milvus documentation:https://milvus.io/docs/use-partition-key.md#Use-Partition-Key", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "15"}, {"url": "/questions/78720765/querynode-chunk-cache", "question_title": "QueryNode Chunk Cache", "question": "We have a cluster deployment of Milvus on the k8s and our dataset sizes are of the order of a 150 million. The querynodes are distributed across 16 replicas with 40G of memory each. The current bottleneck for us in the partition load times and we exploring ways to improve the load times. We were referring tohttps://milvus.io/docs/chunk_cache.mdand tried setting this up in our deployment. However, we do not see any noticeable improvements in the load times and from the logs, there is not indication that the chunk cache is being loaded. There are a couple of questions we have in this area:Is chunk caching supposed to speed up partition load times in general or does it only come into play during vector search?\nHow to verify if chunk cache is working as expected?\nIs the cache stored in the local storage of the querynodes under the localStorage directory which in our case happens to be /var/lib/milvus/data which is currently empty?\nWhich grafana panel can be used as an indication of checking the cache?\nWhich logs should specifically indicate that chunk cache is working?\nThe user.yaml file on the query nodes have the below configuration in them:> kubectl exec -it qa-milvus-querynode -- cat /milvus/configs/user.yaml\nDefaulted container \"querynode\" out of: querynode, config (init)\ncommon:\n  security:\n    authorizationEnabled: true\nproxy:\n  maxUserNum: 500\n  maxRoleNum: 100\nqueryNode:\n  cache:\n    enabled: true\n    warmup: async", "answers": [{"content": "Milvus manages data by segments, and each segment has an independent index that progresses asynchronously. If a segment's index is not ready, the query node will load its original vector data into memory to perform a brute-force search.The time required to load a collection or partition primarily depends on the number of segments or indexes within it. When warmup is enabled, the load process will download the original vector files to the local disk. Once these files are stored locally, search operations withoutput_fieldswill read data from the local files instead of S3.The size of the index relative to the original vectors varies by index type:IVF_FLAT/HNSW: A bit larger than the original vector size.IVF_SQ8/IVF_PQ: 25% to 30% of the original vector size.DISKANN: 1/4 to 1/6 of the original vector size.The time taken to output vector data depends on the number of vectors read. For instance, a search request withNQ = 1andtopk = 1will only read one vector from storage, having minimal impact on search performance. However, settingtopk = 1000will require readingNQ * 1000vectors, significantly impacting search performance.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "22"}, {"url": "/questions/78720002/milvus-standalone-always-exited-2-on-macos-12-5-1", "question_title": "milvus-standalone always exited (2) on Macos 12.5.1", "question": "Milvus-standalone worked well when my MacOS was with 12.3.\nWhen I upgrade MacOS to 12.5.1, I cannot install Milvus-standalone correctly.\nAs you see below, Milvus-standalone is always exited (2).milvus-etcd \"etcd -advertise-cli\u2026\" etcd running 2379-2380/tcp\nmilvus-minio \"/usr/bin/docker-ent\u2026\" minio running (starting) 9000/tcp\nmilvus-standalone \"/tini -- milvus run\u2026\" standalone exited (2)And I found this message from the log in Docker:\npanic: Corruption: Corruption: IO error: No such file or directoryWhile open a file for random read: /var/lib/milvus/rdb_data/000185.ldb: No such file or directoryPlease help because I have used Milvus for a couple of projects until now.Thanks", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "19"}, {"url": "/questions/78713576/unable-to-integrate-milvus-to-spring-boot-application", "question_title": "Unable to integrate milvus to spring boot application", "question": "I am trying to integrate Milvus with my Spring Boot Application but I am getting status 500: Internal Server error as soon as I create a milvus client.\nThis is the code I am writing :String CLUSTER_ENDPOINT = \"http://localhost:19530/\";\nConnectConfig connectConfig = ConnectConfig.builder().uri(CLUSTER_ENDPOINT).build();\nMilvusClientV2 client = new MilvusClientV2(connectConfig);Can anyone help me with this error?", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "27"}, {"url": "/questions/78713020/milvus-backup-tool-is-failing", "question_title": "Milvus backup tool is failing", "question": "I am using the latest version of Milvus Backup. When attempting to back up the entire Milvus cluster, the process only backs up a few collections before getting stuck with the following error. Additionally, the meta directory is not created in the bucket.Error:\n[2024/05/21 00:23:51.409 +00:00] [INFO] [core/backup_impl_create_backup.go:517] [\"Begin copy data\"] [dbName=misc_db] [collectionName=mld_rscls_USA] [segmentNum=637]\n[2024/05/21 00:24:14.357 +00:00] [ERROR] [core/backup_impl_create_backup.go:525] [\"Fail to fill segment backup info\"] [collection_id=448161342919437791] [partition_id=448161342919437792] [segment_id=448161342941988531] [group_id=0] [error=\"Get empty input path, but segment should not be empty, milvus/s3-access/insert_log/448161342919437791/448161342919437792/448161342941988531/\"] [stack=\"github.com/zilliztech/milvus-backup/core.(BackupContext).backupCollectionExecute\\n\\t/app/core/backup_impl_create_backup.go:525\\ngithub.com/zilliztech/milvus-backup/core.(BackupContext).executeCreateBackup.func2\\n\\t/app/core/backup_impl_create_backup.go:657\\ngithub.com/zilliztech/milvus-backup/internal/common.(WorkerPool).work.func1\\n\\t/app/internal/common/workerpool.go:70\\ngolang.org/x/sync/errgroup.(Group).Go.func1\\n\\t/go/pkg/mod/golang.org/x/[email\u00a0protected]/errgroup/errgroup.go:75\"", "answers": [{"content": "You can consider lowering the backup.parallelism.copydata parameter in the backup.yaml configuration file of the tool. You might want to set it to a lower number, such as 8 or 16.Here's the link to the relevant section in the configuration file:https://github.com/zilliztech/milvus-backup/blob/6f2556082165379bd40c782e630be812acd8d919/configs/backup.yaml#L52", "votes": 0}], "num_vote": "-2", "num_answer": "1", "num_view": "37"}, {"url": "/questions/78704927/multivector-hybridsearch-function-is-not-working", "question_title": "Multivector hybridsearch function is not working", "question": "I followed the documentation:https://milvus.io/docs/multi-vector-search.mdto perform a multivector hybrid search, but I encountered an error when performing Step 3: Perform a Hybrid Search.res = collection.hybrid_search(\n    reqs, # List of AnnSearchRequests created in step 1\n    rerank, # Reranking strategy specified in step 2\n    limit=2 # Number of final search results to return\n)\n\nprint(res)\n----------------------------------------\ngrpc RpcError: [hybrid_search], <_InactiveRpcError: StatusCode.UNIMPLEMENTED, unknown method HybridSearch for service milvus.proto.milvus.MilvusService>, <Time:{'RPC start': '2024-06-20 04:18:00.304044', 'gRPC error': '2024-06-20 04:18:00.352937'}>It suggests that the HybridSearch method is not implemented or recognized by the Milvus service. I checked my version of both Milvus and the Python client, and they are both 2.4.x, which should include this hybrid_search method.Is there any workaround for this issue?", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "36"}, {"url": "/questions/78704743/milvus-cluster-not-running-with-milvus-operator", "question_title": "Milvus Cluster not Running with Milvus Operator", "question": "I am trying to install milvus db with milvus operator on my EKS cluster. I have a running milvus operator pod. The logs of pod  doesnot show any error. But when I try to create a milvus standalone cluster using  this command : kubectl apply -fhttps://raw.githubusercontent.com/zilliztech/milvus-operator/main/config/samples/milvus_default.yamlI get this error:\nError from server (InternalError): error when creating \"https://raw.githubusercontent.com/zilliztech/milvus-operator/main/config/samples/milvus_default.yaml\": Internal error occurred: failed calling webhook \"mmilvus.kb.io\": failed to call webhook: Post \"https://milvus-operator-webhook-service.milvus-operator.svc/mutate-milvus-io-v1beta1-milvus?timeout=10s\": Address is not allowedI am using the documentation for milvus operator  to create the cluster.  Any help is appreciated.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "38"}, {"url": "/questions/78701266/milvus-exception-index-not-found-in-flat-collection", "question_title": "Milvus exception: index not found in FLAT collection", "question": "I am having a strange issue in milvus vector search collection.\nJust for a test case i create a generic vectors samples. When i define the collection search method to be FLAT,create index and then try to load it I get the error :<MilvusException: (code=25, message=index not found[collection=\"my_collection_name\"]However when trying any other search method, everything works fine.\nI tried also to omit the create index (not necessary for FLAT method) and got the same exception.Has anyone ever encountered this error and able to assist.Relevant Code::def create_vector_collection(collection_name, fields, description, host, port):\n    connections.connect(host=host, port=port)\n    schema = CollectionSchema(fields=fields, description=description)\n    return Collection(name=collection_name, schema=schema)\n\nif __name__ == '__main__':\n    from pymilvus import connections, Collection\n    connections.connect(host=HOST, port=PORT)\n    fields = [\n        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1024),\n        FieldSchema(name=\"smiles\", dtype=DataType.VARCHAR, max_length=3000)\n    ]\n    description = \"A collection with vectors and metadata\"\n    brute_force_collection_name = \"molecule_brute_force_collection\"\n    brute_force_collection = create_vector_collection(brute_force_collection_name, fields, description, host=HOST, port=PORT)\n\n    create_fp_array('MarketSelectSmiles.smi', chunk_size=1000, collections=[brute_force_collection])\n\n    brute_force_index_params = {\n        \"index_type\": \"FLAT\",\n        \"metric_type\": \"L2\",\n        \"params\": {}\n    }\n\n    res = brute_force_collection.create_index(field_name=\"embedding\", index_params=brute_force_index_params)\n    collection = Collection(brute_force_collection_name)\n    collection.load()#FAILS at this point :(\n    print(f\"Collection '{brute_force_collection_name}' loaded successfully.\")create_fp_array upload from a file 1000 vectors to the collection each time, as already been said other index methods such as IVF_FLAT are operational(probelm exists only with FLAT).", "answers": [], "num_vote": "1", "num_answer": "0", "num_view": "58"}, {"url": "/questions/78700883/node-js-sdk-what-connection-params-should-be-used", "question_title": "Node.JS SDK , what connection params should be used?", "question": "Can someone please verify the keys I am using for Node SDK for milvus?const milvusClient = new MilvusClient({\n   address: '<server-name>:port',\n   username: 'username',\n   password: 'password',\n   ssl: true ,\n   tls:{\n    rootCertPath:'/root/milvus.crt'\n   },\n  });", "answers": [{"content": "const milvusClient = new MilvusClient({\n   address: '<server-name>:port', // your milvus address\n   username: 'username', // your milvus username\n   password: 'password',  // your milvus password\n   ssl: true , // I don't think you need it\n   tls:{\n    rootCertPath:'/root/milvus.crt' // path to your milvus cert file if you need to connect to the tls enabled milvus \n   },\n  });", "votes": 1}, {"content": "I am unfortunately not able to validate your parameters since they should be configured based on your specific environment.Address is the milvus IP addressUsername is your username used to connect to MilvusPassword is your password used to connect to MilvusSSL is false by default; Set it to true if you are using Zilliz Cloud.\nYou can refer to this page for more information:https://milvus.io/api-reference/node/v2.2.x/Client/MilvusClient.md", "votes": 0}], "num_vote": "1", "num_answer": "2", "num_view": "22"}, {"url": "/questions/78700492/milvus-2-3-11-error-incomplete-query-result-missing-id-xxxxxxx-lensearchids", "question_title": "milvus-2.3.11 error=\"incomplete query result, missing id xxxxxxx, len(searchIDs) amd64", "question": "We are getting frequent errors like below and our searches are failing:\n\"Error received from Milvus from method:getData: incomplete query result, missing id f3aecfdf-c3e6-4082-84e1-6f67c83d3c41, len(searchIDs) = 200, len(queryIDs) = 155, collection=450749972193045101\"We found some relevant issue here:https://github.com/milvus-io/milvus/issues/34021Is the issue being addressed?Can we expect a fix backported to 2.3.11?", "answers": [{"content": "It's under fix. But I don't think it will backported to 2.3.11, since we only backport to latest 2.3.x version.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "15k"}, {"url": "/questions/78696129/float8-embeddings-returned-by-querying-milvus-python-client", "question_title": "Float8 embeddings returned by querying Milvus python client", "question": "I am using the Attu client to manage the data from Milvus and all the data looks good inside.But when i am trying to query the same data with the python client, the embeddings results i get are in a float8 format instead of a float32 one.Why is that? Is there a limitation or conversion on the retrieval process?Here is a snippet of the query i am running:self.milvusClient.query(\n    collection_name='someCollection',\n    output_fields=['embedding'],\n    filter='objectId in [' +  ','.join(objectIdList) + ']',\n    limit=limit\n)Also here is a snippet of the creation command for the embedding field:schema.add_field(field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=512, is_primary=False, auto_id=False)\n\nindex_params.add_index(\n    field_name=\"embedding\", \n    index_type=\"IVF_FLAT\",\n    metric_type=\"L2\",\n    params={\n        \"nlist\": 2048\n    }\n)Value stored: -0.24403861165046692\nValue retrieved: -0.24403861Thank you!", "answers": [{"content": "The same discussions might answer the question:https://github.com/milvus-io/milvus/discussions/32737https://github.com/milvus-io/milvus/discussions/32224In short: A Float32 value has a precision of ~7 decimal digits. Even with a Float64 value, such as 1.3476964684980388, Milvus stores it as 1.347696.", "votes": 0}, {"content": "Float vectors are in fact stored and computed using the float32 data type in Milvus with a default storage with a precision of about 7 decimal digits.However, in Python, standard float values are processed as float64. When float64 values are passed to Milvus, the SDK automatically converts them to float32, and the values are returned as float32 as well when being retrieved.There are several reasons for using float32 instead of float64 in approximate nearest neighbor (ANN) search:Memory and Storage Efficiency:Float64 consumes more memory and disk space compared to float32.Computation Speed:CPUs compute float64 operations much slower than float32 operations.Result Accuracy:ANN search is an approximate search method, and the difference in results between using float32 and float64 is negligible.", "votes": 0}], "num_vote": "0", "num_answer": "2", "num_view": "25"}, {"url": "/questions/78695653/data-coordinator-fails-to-start-stuck-in-initializing", "question_title": "Data Coordinator fails to start. Stuck in \"Initializing\"", "question": "I've got a bunch of logs that look like this[2024/07/01 17:42:53.684 +00:00] [WARN] [datacoord/index_service.go:849] [\"DataCoord 12139 is not ready\"] [traceID=1a56c8bd789bd9dde94e8fdee0e927c1] [collectionID=448339039224594620] [error=\"service not ready[datacoord=12139]: Initializing\"]\n[2024/07/01 17:42:53.746 +00:00] [INFO] [datacoord/services.go:797] [\"get recovery info request received\"] [traceID=9d4c0a04cc578a1abeba6a9bbe0fbccb] [collectionID=448339039224915395] [partitionIDs=\"[]\"]\n[2024/07/01 17:42:53.746 +00:00] [INFO] [datacoord/services.go:797] [\"get recovery info request received\"] [traceID=ff858e069b81baa6cfbad28caa1e28fe] [collectionID=448339039243990697] [partitionIDs=\"[]\"]\n[2024/07/01 17:42:53.746 +00:00] [INFO] [datacoord/services.go:797] [\"get recovery info request received\"] [traceID=3a528ee771464452f5479a8d290f5822] [collectionID=448339039224854911] [partitionIDs=\"[]\"]That basically repeat until the k8s deployment gives up waiting for ready and kills the pod.\nI'm using milvus 2.3.10 with version 4.1.21 of the official helm chart.\nI'm looking for any advice on debugging what's going on here and how I might try to recover, or at least learn more about the problem.  Logs from the other components look relatively normal, best I can tell.\nThanks!", "answers": [{"content": "The troubleshooting steps are as follows:Check the dependencies of Milvus, such as etcd, the message queue (Pulsar/Kafka), and object storage, to ensure they are working properly.Check whether rootcoord is functioning normally.You can use the script fromhttps://github.com/milvus-io/milvus/blob/master/deployments/export-log/export-milvus-log.shto upload logs to us.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "18"}, {"url": "/questions/78679269/milvus-connectionrefusederror-how-to-connect-locally", "question_title": "Milvus ConnectionRefusedError: how to connect locally", "question": "I am trying to run a RAG pipeline using haystack & Milvus.Its my first time using Milvus, and I seem to have an issue with it.I'm following this tutorial, with some basic changes:https://milvus.io/docs/integrate_with_haystack.mdHere is my code:import os\nimport urllib.request\n\nfrom haystack import Pipeline\nfrom haystack.components.converters import MarkdownToDocument\nfrom haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder, OllamaTextEmbedder\n\nfrom haystack.components.preprocessors import DocumentSplitter\nfrom haystack.components.writers import DocumentWriter\n\nfrom milvus_haystack import MilvusDocumentStore\nfrom milvus_haystack.milvus_embedding_retriever import MilvusEmbeddingRetriever\n\nurl = \"https://www.gutenberg.org/cache/epub/7785/pg7785.txt\"\nfile_path = \"./davinci.txt\"\n\nif not os.path.exists(file_path):\n    urllib.request.urlretrieve(url, file_path) \n\ndocument_store = MilvusDocumentStore(\n    connection_args={\"uri\": \"./milvus.db\"},\n    drop_old=True,\n)\n\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(\"converter\", MarkdownToDocument())\nindexing_pipeline.add_component(\n    \"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=2)\n)\nindexing_pipeline.add_component(\"embedder\", OllamaDocumentEmbedder())\nindexing_pipeline.add_component(\"writer\", DocumentWriter(document_store))\nindexing_pipeline.connect(\"converter\", \"splitter\")\nindexing_pipeline.connect(\"splitter\", \"embedder\")\nindexing_pipeline.connect(\"embedder\", \"writer\")\n\nindexing_pipeline.draw('./pipeline_diagram.png')\n\nindexing_pipeline.run({\"converter\": {\"sources\": [file_path]}})It all works well until the last line, where I get a ConnectionRefusedError.\nFirst the conversion (from markdown to document) runs well, but then the code fails.I am not sure why it happens, as I see themilvus.dbandmilvus.db.lockfiles created as expected.The full error is:---------------------------------------------------------------------------\nConnectionRefusedError                    Traceback (most recent call last)\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/connection.py:203, in HTTPConnection._new_conn(self)\n    202 try:\n--> 203     sock = connection.create_connection(\n    204         (self._dns_host, self.port),\n    205         self.timeout,\n    206         source_address=self.source_address,\n    207         socket_options=self.socket_options,\n    208     )\n    209 except socket.gaierror as e:\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/util/connection.py:85, in create_connection(address, timeout, source_address, socket_options)\n     84 try:\n---> 85     raise err\n     86 finally:\n     87     # Break explicitly a reference cycle\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/util/connection.py:73, in create_connection(address, timeout, source_address, socket_options)\n     72     sock.bind(source_address)\n---> 73 sock.connect(sa)\n     74 # Break explicitly a reference cycle\n\nConnectionRefusedError: [Errno 61] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nNewConnectionError                        Traceback (most recent call last)\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/connectionpool.py:791, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\n    790 # Make the request on the HTTPConnection object\n--> 791 response = self._make_request(\n    792     conn,\n    793     method,\n    794     url,\n    795     timeout=timeout_obj,\n    796     body=body,\n    797     headers=headers,\n    798     chunked=chunked,\n    799     retries=retries,\n    800     response_conn=response_conn,\n    801     preload_content=preload_content,\n    802     decode_content=decode_content,\n    803     **response_kw,\n    804 )\n    806 # Everything went great!\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/connectionpool.py:497, in HTTPConnectionPool._make_request(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\n    496 try:\n--> 497     conn.request(\n    498         method,\n    499         url,\n    500         body=body,\n    501         headers=headers,\n    502         chunked=chunked,\n    503         preload_content=preload_content,\n    504         decode_content=decode_content,\n    505         enforce_content_length=enforce_content_length,\n    506     )\n    508 # We are swallowing BrokenPipeError (errno.EPIPE) since the server is\n    509 # legitimately able to close the connection after sending a valid response.\n    510 # With this behaviour, the received response is still readable.\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/connection.py:395, in HTTPConnection.request(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\n    394     self.putheader(header, value)\n--> 395 self.endheaders()\n    397 # If we're given a body we start sending that in chunks.\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/http/client.py:1289, in HTTPConnection.endheaders(self, message_body, encode_chunked)\n   1288     raise CannotSendHeader()\n-> 1289 self._send_output(message_body, encode_chunked=encode_chunked)\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/http/client.py:1048, in HTTPConnection._send_output(self, message_body, encode_chunked)\n   1047 del self._buffer[:]\n-> 1048 self.send(msg)\n   1050 if message_body is not None:\n   1051 \n   1052     # create a consistent interface to message_body\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/http/client.py:986, in HTTPConnection.send(self, data)\n    985 if self.auto_open:\n--> 986     self.connect()\n    987 else:\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/connection.py:243, in HTTPConnection.connect(self)\n    242 def connect(self) -> None:\n--> 243     self.sock = self._new_conn()\n    244     if self._tunnel_host:\n    245         # If we're tunneling it means we're connected to our proxy.\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/connection.py:218, in HTTPConnection._new_conn(self)\n    217 except OSError as e:\n--> 218     raise NewConnectionError(\n    219         self, f\"Failed to establish a new connection: {e}\"\n    220     ) from e\n    222 # Audit hooks are only available in Python 3.8+\n\nNewConnectionError: <urllib3.connection.HTTPConnection object at 0x30ca49690>: Failed to establish a new connection: [Errno 61] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nMaxRetryError                             Traceback (most recent call last)\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/requests/adapters.py:486, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)\n    485 try:\n--> 486     resp = conn.urlopen(\n    487         method=request.method,\n    488         url=url,\n    489         body=request.body,\n    490         headers=request.headers,\n    491         redirect=False,\n    492         assert_same_host=False,\n    493         preload_content=False,\n    494         decode_content=False,\n    495         retries=self.max_retries,\n    496         timeout=timeout,\n    497         chunked=chunked,\n    498     )\n    500 except (ProtocolError, OSError) as err:\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/connectionpool.py:845, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\n    843     new_e = ProtocolError(\"Connection aborted.\", new_e)\n--> 845 retries = retries.increment(\n    846     method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n    847 )\n    848 retries.sleep()\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/urllib3/util/retry.py:515, in Retry.increment(self, method, url, response, error, _pool, _stacktrace)\n    514     reason = error or ResponseError(cause)\n--> 515     raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    517 log.debug(\"Incremented Retry for (url='%s'): %r\", url, new_retry)\n\nMaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x30ca49690>: Failed to establish a new connection: [Errno 61] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionError                           Traceback (most recent call last)\nCell In[15], line 1\n----> 1 indexing_pipeline.run({\"converter\": {\"sources\": [file_path]}})\n      3 print(\"Number of documents:\", document_store.count_documents())\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/haystack/core/pipeline/pipeline.py:197, in Pipeline.run(self, data, debug, include_outputs_from)\n    195 span.set_content_tag(\"haystack.component.input\", last_inputs[name])\n    196 logger.info(\"Running component {component_name}\", component_name=name)\n--> 197 res = comp.run(**last_inputs[name])\n    198 self.graph.nodes[name][\"visits\"] += 1\n    200 if not isinstance(res, Mapping):\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/haystack_integrations/components/embedders/ollama/document_embedder.py:139, in OllamaDocumentEmbedder.run(self, documents, generation_kwargs)\n    136     raise TypeError(msg)\n    138 texts_to_embed = self._prepare_texts_to_embed(documents=documents)\n--> 139 embeddings, meta = self._embed_batch(\n    140     texts_to_embed=texts_to_embed, batch_size=self.batch_size, generation_kwargs=generation_kwargs\n    141 )\n    143 for doc, emb in zip(documents, embeddings):\n    144     doc.embedding = emb\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/haystack_integrations/components/embedders/ollama/document_embedder.py:107, in OllamaDocumentEmbedder._embed_batch(self, texts_to_embed, batch_size, generation_kwargs)\n    105 batch = texts_to_embed[i]  # Single batch only\n    106 payload = self._create_json_payload(batch, generation_kwargs)\n--> 107 response = requests.post(url=self.url, json=payload, timeout=self.timeout)\n    108 response.raise_for_status()\n    109 result = response.json()\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/requests/api.py:115, in post(url, data, json, **kwargs)\n    103 def post(url, data=None, json=None, **kwargs):\n    104     r\"\"\"Sends a POST request.\n    105 \n    106     :param url: URL for the new :class:`Request` object.\n   (...)\n    112     :rtype: requests.Response\n    113     \"\"\"\n--> 115     return request(\"post\", url, data=data, json=json, **kwargs)\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/requests/api.py:59, in request(method, url, **kwargs)\n     55 # By using the 'with' statement we are sure the session is closed, thus we\n     56 # avoid leaving sockets open which can trigger a ResourceWarning in some\n     57 # cases, and look like a memory leak in others.\n     58 with sessions.Session() as session:\n---> 59     return session.request(method=method, url=url, **kwargs)\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n    584 send_kwargs = {\n    585     \"timeout\": timeout,\n    586     \"allow_redirects\": allow_redirects,\n    587 }\n    588 send_kwargs.update(settings)\n--> 589 resp = self.send(prep, **send_kwargs)\n    591 return resp\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs)\n    700 start = preferred_clock()\n    702 # Send the request\n--> 703 r = adapter.send(request, **kwargs)\n    705 # Total elapsed time of the request (approximately)\n    706 elapsed = preferred_clock() - start\n\nFile /opt/anaconda3/envs/haystack_milvus_playground/lib/python3.11/site-packages/requests/adapters.py:519, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)\n    515     if isinstance(e.reason, _SSLError):\n    516         # This branch is for urllib3 v1.22 and later.\n    517         raise SSLError(e, request=request)\n--> 519     raise ConnectionError(e, request=request)\n    521 except ClosedPoolError as e:\n    522     raise ConnectionError(e, request=request)\n\nConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x30ca49690>: Failed to establish a new connection: [Errno 61] Connection refused'))any help resolving this would be appreciated. My assumption is that it is something very simple in creating the milvus database local connection, but I dont know where it is.", "answers": [{"content": "looks like OLLAMA embedding failed, you can try to check your ollama servie is normal, or just change to another embedding", "votes": 2}, {"content": "As suggested in earlier replies, Ollama was not running locally.To resolve this I needed to:1 -download and install Ollama2 - pull (or run) in the terminalollama pull nomic-embed-textfor the embedders. This is not clear enough in thehaystack documentation for the ollama integrationbut once this is done it should run.Also I would suggest to change from:indexing_pipeline.add_component(\"embedder\", OllamaDocumentEmbedder())to:indexing_pipeline.add_component(\"embedder\", OllamaDocumentEmbedder(model=\"nomic-embed-text\", url=\"http://localhost:11434/api/embeddings\"))", "votes": 2}, {"content": "This requires Milvus Lite and PyMilvus be very recent versions.  What version are you running?https://github.com/milvus-io/milvus-haystackhttps://haystack.deepset.ai/integrations/milvus-document-storeWhich operating system are you running?I am runningpymilvus-2.4.4\nmilvus-haystack-0.0.8\nhaystack-ai-2.2.3", "votes": 1}], "num_vote": "1", "num_answer": "3", "num_view": "128"}, {"url": "/questions/78678245/llama-3-text-and-images", "question_title": "LLama 3: Text and images", "question": "We are trying to deploy Llama 3 in RAG configuration for one of the engineering teams who has thousands of documents that contain text, tables and images. We are able to extract text and tables from PDFs and HTML files and transform them in a way the LLM understands. But we don't have a solution on how to save images and retrieve them in the chatbot. We are using milvus vector db and langchain. Kindly suggest a solution.", "answers": [{"content": "Start here:https://milvus.io/docs/integrate_with_langchain.mdInstall and Run on K8https://milvus.io/docs/prerequisite-helm.mdJoin the Milvus Discord to help you on your journey to a billion vectorshttps://milvus.io/communityRAG Articleshttps://medium.com/@zilliz_learn/kickstart-your-local-rag-setup-a-beginners-guide-to-using-llama-3-with-ollama-milvus-and-daffa7e16d6fhttps://zilliz.com/blog/local-agentic-rag-with-langraph-and-llama3", "votes": 0}], "num_vote": "-1", "num_answer": "1", "num_view": "170"}, {"url": "/questions/78675558/search-fails-because-of-type-is-illegal", "question_title": "Search fails because of type is illegal", "question": "When I try to run client.search() on my collection, I get the following error:/usr/local/lib/python3.10/dist-packages/pymilvus/client/check.py in _raise_param_error(param_name, param_value)\n    220 def _raise_param_error(param_name: str, param_value: Any) -> None:\n--> 221     raise ParamError(message=f\"{param_name} value {param_value} is illegal\")Here's the description of my collection:{'collection_name': 'TRIAL0_collection',\n 'auto_id': False,\n 'num_shards': 0,\n 'description': 'RAG database collection',\n 'fields': [{'field_id': 100,\n   'name': 'id',\n   'description': 'id',\n   'type': <DataType.VARCHAR: 21>,\n   'params': {'max_length': 50000},\n   'is_primary': True},\n  {'field_id': 101,\n   'name': 'text',\n   'description': 'chunk text',\n   'type': <DataType.VARCHAR: 21>,\n   'params': {'max_length': 50000}},\n  {'field_id': 102,\n   'name': 'vector',\n   'description': 'embedding of the chunk',\n   'type': <DataType.FLOAT_VECTOR: 101>,\n   'params': {'dim': 768}},\n  {'field_id': 103,\n   'name': 'first_page',\n   'description': 'first page the chunk appears in',\n   'type': <DataType.VARCHAR: 21>,\n   'params': {'max_length': 50000}},\n  {'field_id': 104,\n   'name': 'source',\n   'description': 'file the chunk comes from',\n   'type': <DataType.VARCHAR: 21>,\n   'params': {'max_length': 50000}}],\n 'aliases': [],\n 'collection_id': 0,\n 'consistency_level': 0,\n 'properties': {},\n 'num_partitions': 0,\n 'enable_dynamic_field': True}My data is simply the embedding of a query. It is done with the exact same model I embedded the chunks with. I checked the shape and size, and they are the same. My version of pymilvus is 2.4.4. I also checked this with a vector of just 0s and I get the same error.", "answers": [], "num_vote": "1", "num_answer": "0", "num_view": "31"}, {"url": "/questions/78675301/error-occurred-when-creating-backup-for-milvus", "question_title": "error occurred when creating backup for milvus", "question": "Hello I'm a newbie in using milvus-backup tool\nMilvus version: from 2.3.12\nDeployment mode(standalone or cluster): cluster\nmilvus-backup version: v0.4.14I tried to backup my collection in a remote instance of milvus to my local minio.\nHowever, when I run the command:\n./milvus-backup create -n test_backupI get this error:[2024/06/13 15:06:06.852 +08:00] [INFO] [core/backup_impl_create_backup.go:523] [\"Begin copy data\"] [dbName=default] [collectionName=chapters] [segmentNum=2]\n[2024/06/13 15:06:06.852 +08:00] [DEBUG] [core/backup_impl_create_backup.go:531] [\"copy segment\"] [collection_id=444879892039202356] [partition_id=444879892039202357] [segment_id=444879892139483548] [group_id=0]\n[2024/06/13 15:06:06.852 +08:00] [DEBUG] [core/backup_impl_create_backup.go:888] [insertPath] [bucket=a-bucket] [insertPath=files/insert_log/444879892039202356/444879892039202357/444879892139483548/]\n[2024/06/13 15:06:06.853 +08:00] [ERROR] [core/backup_impl_create_backup.go:531] [\"Fail to fill segment backup info\"] [collection_id=444879892040221795] [partition_id=444879892040221796] [segment_id=444879892009040118] [group_id=0] [error=\"Get empty input path, but segment should not be empty, files/insert_log/444879892040221795/444879892040221796/444879892009040118/\"] [stack=\"github.com/zilliztech/milvus-backup/core.(*BackupContext).backupCollectionExecute...]Can anyone help me how I can fix this \"error=\"Get empty input path, but segment should not be empty, \"", "answers": [{"content": "This error typically occurs when the Milvus backup tool expects to find data files in the specified path but doesn't find any. Here are some possible ways I suggest you can try to troubleshoot this issue:Verify Data in Milvus: Ensure the collection chapters contains data:from pymilvus import connections, Collection\nconnections.connect(host='your-milvus-host', port='your-milvus-port')\ncollection = Collection('chapters')\nprint(f\"Number of entities: {collection.num_entities}\")Check MinIO Configuration: Ensure the MinIO bucket is correctly configured and accessible:mc ls myminio/a-bucket/files/insert_log/444879892040221795/444879892040221796/444879892009040118/Permissions and Access:\nVerify Milvus backup tool has the necessary read/write permissions for the specified paths.", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "48"}, {"url": "/questions/78672503/i-am-currently-working-with-the-following-setup", "question_title": "I am currently working with the following setup", "question": "I am currently working with the following setup:Milvus version 2.3.7, pymilvus version 2.3.6.\nA database in Milvus containing 4 million 768-dimensional vectors.\nMy challenge involves performing a vector search across a large number of IDs, ranging from 10,000 to 500,000. For instance, if I have a query that matches 100,000 documents but only 70,000 of those are available in my inventory. I need to filter out the available items based on this information which I have in the form of a bitset.The current workflow is as follows:Query Milvus: Execute a query within Milvus to retrieve document IDs for matched vectors without availability consideration.Post-Processing: Implement post-filtering using the bitset to isolate only those documents with their corresponding availability bit set to '1'. This results in an O(n) computational complexity, where n is the size of the matched documents.I am seeking a method or strategy that either reduces this operation to O(1) time complexity or allows me to directly fetch available items via Milvus without needing any post-filtering.Milvus does support filtering with bitsets (applying filters prior to running the actual approximate nearest neighbor (ANN) search). As I already have an availability bitset on hand, is there any way to utilize this within Milvus?I also thought of other approach where i have updated my availability in form of arrays in documents like this`Items :\n\n[\n{\n\"product_id\": \"12345\",\n\"product_title-vector\": [0.23, 0.45, 0.87, ....] #this is the vector field of required dimension,\n\"store_availability\": [0, 2, 14, 3, 5, ...] #upto 2000 Stores\n},\n{\n\"product_id\": \"12346\",\n\"product_title-vector\": [0.23, 0.45, 0.87, ....] #this is the vector field of required dimension,\n\"store_availability\": [0, 5, ...] #upto 2000 Stores\n},\n{\n\"product_id\": \"12347\",\n\"product_title-vector\": [0.23, 0.45, 0.87, ....] #this is the vector field of required dimension,\n\"store_availability\": [0, 5, ...] #upto 2000 Stores\n},\n{\n\"product_id\": \"12348\",\n\"product_title-vector\": [0.23, 0.45, 0.87, ....] #this is the vector field of required dimension,\n\"store_availability\": [0, 3, 5, ...] #upto 2000 Stores\n},\n{\n\"product_id\": \"12349\",\n\"product_title-vector\": [0.23, 0.45, 0.87, ....] #this is the vector field of required dimension,\n\"store_availability\": [0, 2, 14, 3, 5, ...] #upto 2000 Stores\n},\n]To search available items in relevant stores i was trying to use boolean expressionsbool_expr = \"(store_availability in [0] && store_availability[0]==4) && \" +\n\"(store_availability in [1] && store_availability[1]==2) && \" +\n\"(store_availability in [2] && store_availability[2]==14)\"\n\nsearch_results = collection.search(\ndata=query_vector_resized,\nanns_field=\"product_title_vector\",\nparam=search_params,\nlimit=10,\nexpr=bool_expr,\noutput_fields=['product_id', 'product_name', 'store_availability']\n)However the major challenge is inventory updates every 10 Minutes and i can not do so many bulk updates so often\nI appreciate any guidance or suggestions on how this can be achieved efficiently.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "22"}, {"url": "/questions/78671058/are-campempath-serverpempath-parameters-required-for-milvusclient-if-the-milvus", "question_title": "Are CamPemPath/ServerPemPath parameters required for MilvusClient if the Milvus cert has been imported into the java trust store?", "question": "If I want to communicate with a Milvus over TLS, can I import the pem into the default java trust store or do I have to configure the client with the CaPemPath/ServerPemPath?IE: at the moment I'm doing this:ConnectParam.Builder builder = ConnectParam\n                .newBuilder()\n                .withHost(host)\n                .withPort(port)\n                .withSecure(secure);\n\n        if (StringUtils.isNotBlank(caPemPath)) {\n            LOGGER.info(\"Using ca pem path: {}\", caPemPath);\n            builder.withCaPemPath(caPemPath);\n        }\n        if (StringUtils.isNoneBlank(serverPemPath, serverName)) {\n            LOGGER.info(\"Using server pem path: {} and server name: {}\", serverPemPath, serverName);\n            builder.withServerPemPath(serverPemPath).withServerName(serverName);\n        } else if (!StringUtils.isAllBlank(serverPemPath, serverName)) {\n            throw new IllegalArgumentException(\"Server milvus.server-pem-path and milvus.server-name must both be set or both be blank\");\n        }But if I've imported Milvus' pem into the java trust store, do I need to do any of this at all? Or will it work without it?", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "13"}, {"url": "/questions/78665235/link-issue-for-new-powerpc-milvus-knowhere-support", "question_title": "Link issue for new powerpc Milvus Knowhere support", "question": "I am working on a patch to add PowerPC optimization to the Milvus Knowhere source tree. I have add the new files:src/simd/distances_powerpc.cc\n\n+namespace faiss {\n+\n+float\n+fvec_L2sqr_ref_ppc(const float* x, const float* y, size_t d) {\n\nsize_t i;\n...\n+float\n+fvec_L1_ref_ppc(const float* x, const float* y, size_t d) {\nsize_t i;\nfloat res = 0;\n/* PowerPC, vectorize\nfor the various functions.\n\nAnd src/simd/distances_powerpc.h\n+namespace faiss {\n+\n+/// Squared L2 distance between two vectors\n+float\n+fvec_L2sqr_ref_ppc(const float* x, const float* y, size_t d);\n+\n+/// inner product\n+float\n+fvec_inner_product_ref_ppc(const float* x, const float* y, size_t d);\n+\n+/// L1 distance\n+float\n+fvec_L1_ref_ppc(const float* x, const float* y, size_t d);\n+\n+/// infinity distance\n+float\n+fvec_Linf_ref_ppc(const float* x, const float* y, size_t d);\n+The file src/simd/hook.cc was updated as follows:Linking CXX executable knowhere_tests\n/opt/at16.0/lib/gcc/powerpc64le-linux-gnu/12.3.1/../../../../powerpc64le-linux-gnu/bin/ld: ../../libknowhere.so: undefined reference to `faiss::fvec_L2sqr_ref_ppc(float const*, float const*, unsigned long)'\n/opt/at16.0/lib/gcc/powerI am encountering a issue when I try to build milvus.I created the build directoryknowhere/build\ncd into the build directory and run\nconan install .. --build=missing -o with_ut=True -s compiler.libcxx=libstdc++11 -s build_type=Release\nwhich seems to run OKThen I do conan build ..I get the following errors:[100%] Linking CXX executable knowhere_tests\n/opt/at16.0/lib/gcc/powerpc64le-linux-gnu/12.3.1/../../../../powerpc64le-linux-gnu/bin/ld: ../../libknowhere.so: undefined reference to faiss::fvec_L2sqr_ref_ppc(float const*, float const*, unsigned long)' /opt/at16.0/lib/gcc/powerpc64le-linux-gnu/12.3.1/../../../../powerpc64le-linux-gnu/bin/ld: ../../libknowhere.so: undefined reference to rust_demangle_callback'\n/opt/at16.0/lib/gcc/powerpc64le-linux-gnu/12.3.1/../../../../powerpc64le-linux-gnu/bin/ld: ../../libknowhere.so: undefined reference to `faiss::fvec_madd_ref_ppc(unsigned long, float const*, float, float const*, float*)'I am guessing that I need to add the new files to the cmake but as of yet I have not been able to figure out what I need to add and to which cmake file.Does anyone have any suggestions on how the cmake files should be updated? Thanks.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "15"}, {"url": "/questions/78664154/what-does-num-entities-in-milvus-collection-increase-when-one-overwrites-existin", "question_title": "What does num_entities in Milvus collection increase when one overwrites existing entities", "question": "I had thought thatnum_entitieswould indicate the number of records (or whatever the correct term is) within a Milvus collection. However, I created 1 file -test_milvus.pyto create a simple collection like so:import numpy as np\nfrom pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType\n\nconnections.connect(alias='default',host='localhost', port='19530')\n\n# Define the schema\nschema = CollectionSchema([FieldSchema(\"id\", DataType.INT64, is_primary=True, max_length=100),\n                           FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=2)])\n\n# Create a collection\ncollection = Collection(\"test_collection\", schema)\n\n# Insert data\ndata = [{\"id\":i, \"vector\": np.array([i, i],dtype=np.float32)} for i in range(10)]\ncollection.insert(data)\n\n# Flush data\ncollection.flush()\n\n# Disconnect from the server\nconnections.disconnect(alias='default')and another to get information on collections within a Milvus database =milvus_info.py- like so:from pymilvus import Collection, connections, db, utility\n\ndef get_info (host: str = \"localhost\", port: str = \"19530\"):\n\n    # Connect to Milvus (replace with your connection details)\n    connections.connect(alias=\"default\", host=host, port=port)  # Replace with your connection parameters\n\n    # Print the list of databases and collections\n    db_list = db.list_database()\n    for db_name in db_list:\n        print(f\"Database: {db_name}\")\n        collection_list = utility.list_collections(using=db_name)\n        if len(collection_list) == 0:\n            print(\"  No collections\")\n        for collection_name in collection_list:\n            print(f\"  Collection: {collection_name}\")\n            temp_collection = Collection(name=collection_name)\n            for info in temp_collection.describe():\n                print(f\"    {info}: {temp_collection.describe()[info]}\")\n            temp_collection.flush() #Note: Adding this line does not fix problem.\n            print(f\"   Number of entities: {temp_collection.num_entities}\")\n          \n    # Disconnect from Milvus\n    connections.disconnect(alias='default')\n\n\nif __name__ == \"__main__\":\n    get_info()The first time I rantest_milvusfollowed bymilvus_info.py, I got this output:$ python test_milvus.py \n  $ python milvus_info.py \n    Database: default\n      Collection: test_collection\n        collection_name: test_collection\n        auto_id: False\n        num_shards: 1\n        description: \n        fields: [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 2}}]\n        aliases: []\n        collection_id: 450687678279804785\n        consistency_level: 2\n        properties: {}\n        num_partitions: 1\n        enable_dynamic_field: False\n       Number of entities: 10which struck me as odd, because there were only 2 vectors in the db.However, if I run `test_milvus.py' again, the number of entities goes up to 20, even though no new vectors have been added:$ python test_milvus.py \n$ python milvus_info.py \nDatabase: default\n  Collection: test_collection\n    collection_name: test_collection\n    auto_id: False\n    num_shards: 1\n    description: \n    fields: [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 2}}]\n    aliases: []\n    collection_id: 450687678279804785\n    consistency_level: 2\n    properties: {}\n    num_partitions: 1\n    enable_dynamic_field: False\n   Number of entities: 20This happens even though I've only tried to add records that were already there. I would've expectednum_entitiesto be 10, no matter how many time I run these files. The documentation says it returns the number of rows, but I can drive it arbitrarily high while still having only 10 rows. Is num_entities supposed to track all rows that ever existed???Note: This also happens when I replaceinsertwithupsert", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "54"}, {"url": "/questions/78660434/why-cant-i-connect-to-the-default-port-on-localhost-when-using-milvus", "question_title": "Why can't I connect to the default port on localhost when using Milvus?", "question": "I'm just starting to use Milvus, but cannot connect to the port given in the example. I am running directly on my machine (i.e. not a Docker image), but cannot get initial exampe to work:$ python\nPython 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pymilvus\n>>> pymilvus.__version__\n'2.4.4'\n>>> host=\"localhost\"\n>>> port = 19530\n>>> from pymilvus import connections\n>>> connections.connect(\"default\",host=host,port=port)\nTraceback (most recent call last):\n  File \"/home/me/anaconda3/envs/ShopTalk/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py\", line 147, in _wait_for_channel_ready\n    grpc.channel_ready_future(self._channel).result(timeout=timeout)\n  File \"/home/me/anaconda3/envs/ShopTalk/lib/python3.10/site-packages/grpc/_utilities.py\", line 162, in result\n    self._block(timeout)\n  File \"/home/me/anaconda3/envs/ShopTalk/lib/python3.10/site-packages/grpc/_utilities.py\", line 106, in _block\n    raise grpc.FutureTimeoutError()\ngrpc.FutureTimeoutError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/me/anaconda3/envs/ShopTalk/lib/python3.10/site-packages/pymilvus/orm/connections.py\", line 447, in connect\n    connect_milvus(**kwargs, user=user, password=password, token=token, db_name=db_name)\n  File \"/home/me/anaconda3/envs/ShopTalk/lib/python3.10/site-packages/pymilvus/orm/connections.py\", line 398, in connect_milvus\n    gh._wait_for_channel_ready(timeout=timeout)\n  File \"/home/me/anaconda3/envs/ShopTalk/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py\", line 150, in _wait_for_channel_ready\n    raise MilvusException(\npymilvus.exceptions.MilvusException: <MilvusException: (code=2, message=Fail connecting to server on localhost:19530, illegal connection params or server unavailable)>\n>>>I've tried to guarantee that 19530 is listening:$ sudo ufw status\nStatus: active\n\nTo                         Action      From\n--                         ------      ----\n22/tcp                     ALLOW       Anywhere                  \n19530/tcp                  ALLOW       Anywhere                  \n22/tcp (v6)                ALLOW       Anywhere (v6)             \n19530/tcp (v6)             ALLOW       Anywhere (v6)But this doesn't seem to make a difference. I suppose there's something `networky' that I'm missing here, but I have no idea what it is. The only installation instructions I saw were 'pip install -U milvus', which I did. What am I not doing here?", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "73"}, {"url": "/questions/78641692/are-the-array-fields-advanced-filters-supported-on-go-sdk", "question_title": "Are the array fields advanced filters supported on Go SDK?", "question": "The advanced filters described in this document:https://milvus.io/docs/array_data_type.md(ARRAY_CONTAINS, ARRAY_CONTAINS_ALL, ARRAY_CONTAINS_ANY, and ARRAY_LENGTH) require a filter field in the arguments of client.search or client.query. I don't see this filter argument in the search and query source code.Search(ctx context.Context, collName string, partitions []string,\n        expr string, outputFields []string, vectors []entity.Vector, vectorField string, metricType entity.MetricType, topK int, sp entity.SearchParam, opts ...SearchQueryOptionFunc) ([]SearchResult, error)\n\n    Query(ctx context.Context, collectionName string, partitionNames []string, expr string, outputFields []string, opts ...SearchQueryOptionFunc) (ResultSet, error)I see we can add them as part of expr but they do not work as intended. e.g ARRAY_CONTAINS_ANY(x, y) doesn't return true for rows where some elements in x exist in y. Though it works when y only has a single element.", "answers": [{"content": "Theexprparameter is a string used for filtering, and all SDKs use the same expression syntax for this purpose.For advanced filtering withARRAY_CONTAINS_ANY, you can refer to the examples provided in the Milvus documentation:https://milvus.io/docs/array_data_type.md#Advanced-filteringAssume there is an Array field namedcolor_coordin a collection. For instance, a row might be inserted as:{\"id\": xxx, \"vector\": [xxxx], \"color_coord\": [1, 2, 3, 4, 5]}To filter usingARRAY_CONTAINS_ANY, the expression should be:expr=\"ARRAY_CONTAINS_ANY(color_coord, [1, 3, 5])\"", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "14"}, {"url": "/questions/78641058/if-delete-a-deprecated-database-how-to-delete-the-corresponding-slice-sst-file", "question_title": "If delete a deprecated database how to delete the corresponding slice .sst file", "question": "Message queue is a key component for Milvus.\nFor Milvus cluster, Pulsar is the message queue.\nFor Milvus standalone, a RocksDB is embedded in milvus, and the data path of the RocksDB is rbd_data. sst file is RocksDB file format.Message queue is like a WAL(write-ahead-log) component for Milvus. Each message queue manages its data by itself.\nFor Milvus standalone, the configurations for the rocksdb. The retentionTimeInMinutes controls the retention time of the rocksdb.\nDefault value is 3 days, which means data will be kept in rocksdb for 3 days. If you have inserted 10GB of data in the past 3 days, you will see the size of rdb_data increase.\nYou can reset this configuration to a small value. Don't set the value to a tiny value. At least 1 hour. Restart milvus after resetting the configuration.rocksmq:\n  # The path where the message is stored in rocksmq\n  # please adjust in embedded Milvus: /tmp/milvus/rdb_data\n  path: /var/lib/milvus/rdb_data\n  lrucacheratio: 0.06 # rocksdb cache memory ratio\n  rocksmqPageSize: 67108864 # 64 MB, 64 * 1024 * 1024 bytes, The size of each page of messages in rocksmq\n  retentionTimeInMinutes: 4320 # 3 days, 3 * 24 * 60 minutes, The retention time of the message in rocksmq.\n  retentionSizeInMB: 8192 # 8 GB, 8 * 1024 MB, The retention size of the message in rocksmq.\n  compactionInterval: 86400 # 1 day, trigger rocksdb compaction every day to remove deleted data\n  compressionTypes: 0,0,7,7,7 # compaction compression type, only support use 0,7. 0 means not compress, 7 will use zstd. Length of types means num of rocksdb level.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "19"}, {"url": "/questions/78640686/handling-documentation-overlap-in-product-feature-queries", "question_title": "Handling Documentation Overlap in Product Feature Queries", "question": "I have two products, X and Y, each with its own set of documentation detailing their features and functionalities. I've processed these documents by dividing them into segments based on word count and stored these segments in vector database for both products.However, there's a challenge I'm facing:There is a high likelihood of overlap between the documents. When discussing the products in an article, I don\u2019t mention the product name in every paragraph, but it is implied that the features described are directly linked to the respective product.I also have other documentation that largely overlaps with the previous product documentation but contains subtle differences.The problem arises when I ask a question about a feature in product X that doesn\u2019t exist in X but is available in Y. The system mistakenly selects the documentation that includes the feature (from product Y), assuming it exists in X, and provides an incorrect answer.How can I handle this issue to ensure that the system correctly identifies whether a feature exists in the queried product and avoids providing incorrect answers based on overlapping documentation?Any advice or approach would be greatly appreciated!. If this is not right forum to ask such questions, apologies.", "answers": [], "num_vote": "1", "num_answer": "0", "num_view": "30"}, {"url": "/questions/78612552/unable-to-create-gpu-ivf-pq-index", "question_title": "Unable to create GPU_IVF_PQ index", "question": "so I deploy a milvus cluster 2.4 to awk eks, and was hoping to try out gpu indices. however when I try to create gpu index on a tiny collection I got something like[2024/06/04 12:37:05.472 +00:00] [INFO] [indexnode/task.go:516] [\"index params are ready\"] [buildID=450232097587185780] [\"index params\"=\"{\\\"cache_dataset_on_device\\\":\\\"false\\\",\\\"dim\\\":\\\"768\\\",\\\"index_type\\\":\\\"GPU_IVF_PQ\\\",\\\"m\\\":\\\"16\\\",\\\"metric_type\\\":\\\"IP\\\",\\\"nbits\\\":\\\"8\\\",\\\"nlist\\\":\\\"2048\\\"}\"]\nE20240604 12:37:05.594095    45 index_factory.cc:49] [KNOWHERE][Create][milvus] failed to find index GPU_IVF_PQ_fp32 in factory\n[2024/06/04 12:37:05.595 +00:00] [WARN] [indexcgowrapper/helper.go:71] [\"failed to create index, C Runtime Exception: index not supported\\n\"]milvus version is milvusdb/milvus:v2.4.3\ndeployed to an awk eks cluster\nI don't see any gpu specific config except for\ngpu:\ninitMemSize: 0\nmaxMemSize: 0\nthe gpu nodes are g5.2xlarge, driver version 550.54.15nvidia-smi\nWed Jun  5 08:11:43 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n|  0%   20C    P8             12W /  300W |       0MiB /  23028MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+so wondering what am I missing?", "answers": [{"content": "you have to use GPU  docker image.see document herehttps://milvus.io/docs/install_cluster-helm-gpu.md", "votes": 1}, {"content": "https://milvus.io/docs/index-with-gpu.md.  Can you show your sample code?Thanks.This should work# Set up the index parameters\nindex_params = {\n    \"metric_type\": \"L2\",\n    \"index_type\": \"GPU_IVF_PQ\",\n    \"params\": {\n        \"nlist\": 1024,\n        \"m\": 4,\n        \"nbits\": 8\n    }\n}\n\n# Create the index\nclient.create_index(\n    collection_name=\"example_collection\",\n    field_name=\"vector_field\",\n    index_params=index_params\n)", "votes": 0}], "num_vote": "1", "num_answer": "2", "num_view": "27"}, {"url": "/questions/78612132/getting-rootcoordclient-mess-key-not-exist-when-running-a-custom-build-of-mi", "question_title": "Getting *\"RootCoordClient mess key not exist\"* when running a custom build of Milvus standalone without docker with embedded etcd", "question": "When running Milvus server from the deb file installation - it works OK, But when trying to run it from a custom build of standalone (without docker, with embedded etcd, with the same milvus.yaml file of the installation) - I am getting \"RootCoordClient mess key not exist\". It seems that the RootCoord client is unable to find a necessary key in its configuration. Specifically, it's indicating that a key named \"rootcoord\" does not exist, which the client is attempting to use. Anyone is familiar with this issue?", "answers": [{"content": "Checkout this:https://github.com/milvus-io/milvus/issues/27536What version are you using of Milvus?   Your system?   Your operating system?", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "39"}, {"url": "/questions/78606143/im-using-milvus226-version-trying-to-do-embedding-and-some-issues-came-up", "question_title": "I'm using Milvus226 version trying to do embedding and some issues came up", "question": "[2023/04/19 02:01:18.810 +00:00] [DEBUG] [querynode/flow_graph_insert_node.go:412] [\"Do insert done\"] [collectionID=440891468920129788] [segmentID=440891468920339016] [len=498]\n[2023/04/19 02:01:18.810 +00:00] [DEBUG] [querynode/flow_graph_service_time_node.go:76] [\"update tSafe:\"] [collectionID=440891468920129788] [tSafe=440892036356767745] [tSafe_p=2023/04/19 02:00:45.526 +00:00] [tsLag=33.284817902s] [channel=by-dev-rootcoord-dml_1_440891468920129788v1]\n[2023/04/19 02:01:19.180 +00:00] [WARN] [retry/retry.go:44] [\"retry func failed\"] [\"retry time\"=0] [error=\"role rootcoord[nodeID: 0] is not serving, reason: sate code: Abnormal\"]\n[2023/04/19 02:01:19.247 +00:00] [WARN] [retry/retry.go:44] [\"retry func failed\"] [\"retry time\"=0] [error=\"role rootcoord[nodeID: 0] is not serving, reason: sate code: Abnormal\"]\n[2023/04/19 02:01:19.977 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=76774bf99f1002f0] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:19.977 +00:00] [WARN] [retry/retry.go:44] [\"retry func failed\"] [\"retry time\"=0] [error=\"role datacoord[nodeID: 3] is not serving, reason: sate code: Abnormal\"]\n[2023/04/19 02:01:19.980 +00:00] [DEBUG] [proxy/impl.go:3956] [Proxy.GetProxyMetrics] [traceID=691da5c24bb8874a] [nodeID=8] [req=\"{\"metric_type\":\"system_info\"}\"] [metricType=system_info]\n[2023/04/19 02:01:20.028 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=2eee42e476c7b16e] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:20.129 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=33f7f3d246725d19] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:20.330 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=24487b852eab2fde] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:20.731 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=788439b0b6734d40] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:21.533 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=55752e85bf38555c] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:22.534 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=4d06aafdc4cc2bcc] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:23.536 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=7cf45abe46d2be43] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:23.778 +00:00] [WARN] [datacoord/compaction.go:142] [\"unable to alloc timestamp\"] [error=\"role rootcoord[nodeID: 0] is not serving, reason: sate code: Abnormal\"]\n[2023/04/19 02:01:23.778 +00:00] [WARN] [retry/retry.go:44] [\"retry func failed\"] [\"retry time\"=0] [error=\"role rootcoord[nodeID: 0] is not serving, reason: sate code: Abnormal\"]\n[2023/04/19 02:01:24.537 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=5fddb3a681007f3a] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:25.345 +00:00] [INFO] [proxy/proxy.go:411] [\"close scheduler\"] [role=proxy]\n[2023/04/19 02:01:25.345 +00:00] [INFO] [proxy/proxy.go:419] [\"close channels time ticker\"] [role=proxy]\n[2023/04/19 02:01:25.345 +00:00] [WARN] [proxy/impl.go:641] [\"DescribeCollection failed to enqueue\"] [error=\"syncTimeStamp Failed:role rootcoord[nodeID: 0] is not serving, reason: sate code: Abnormal\"] [traceID=99f2874bb62159d] [role=proxy] [db=] [collection=sucai999]\n[2023/04/19 02:01:25.345 +00:00] [INFO] [proxy/impl.go:689] [\"DescribeCollection done\"] [traceID=591db0a7ff1dbf81] [role=proxy] [MsgID=440892037266931726] [BeginTS=440892037266931726] [EndTS=440892037266931726] [db=] [collection=sucai999]\n[2023/04/19 02:01:25.346 +00:00] [WARN] [retry/retry.go:44] [\"retry func failed\"] [\"retry time\"=0] [error=\"role rootcoord[nodeID: 0] is not serving, reason: sate code: Abnormal\"]\n[2023/04/19 02:01:25.346 +00:00] [INFO] [msgstream/mq_msgstream.go:182] [\"start to close mq msg stream\"] [\"producer num\"=2] [\"consumer num\"=0]\n[2023/04/19 02:01:25.346 +00:00] [INFO] [proxy/channels_mgr.go:303] [\"all dml stream removed\"]\n[2023/04/19 02:01:25.539 +00:00] [WARN] [datacoord/services.go:896] [\"DataCoord.GetMetrics failed\"] [traceID=1349086299aad8ec] [nodeID=3] [req=\"{\"metric_type\":\"system_info\"}\"] [error=\"DataCoord 3 is not ready\"]\n[2023/04/19 02:01:25.738 +00:00] [WARN] [retry/retry.go:44] [\"retry func failed\"] [\"retry time\"=0] [error=\"role rootcoord[nodeID: 0] is not serving, reason: sate code: Abnormal\"]\n[2023/04/19 02:01:25.738 +00:00] [ERROR] [datanode/flow_graph_insert_buffer_node.go:503] [\"insertBufferNode flushBufferData failed, err = All attempts results:\\nattempt #1:role rootcoord[nodeID: 0] is not serving, reason: sate code: Abnormal\\nattempt #2:context canceled\\n\"] [stack=\"github.com/milvus-io/milvus/internal/datanode.(*insertBufferNode).Sync\\n\\t/go/src/github.com/milvus-io/milvus/internal/datanode/flow_graph_insert_buffer_node.go:503\\ngithub.com/milvus-io/milvus/internal/datanode.(*insertBufferNode).Operate\\n\\t/go/src/github.com/milvus-io/milvus/internal/datanode/flow_graph_insert_buffer_node.go:139\\ngithub.com/milvus-io/milvus/internal/util/flowgraph.(*nodeCtx).work\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/flowgraph/node.go:127\"]\npanic: insertBufferNode flushBufferData failed, err = All attempts results:\nattempt #1:role rootcoord[nodeID: 0] is not serving, reason: sate code: Abnormal\nattempt #2:context canceledgoroutine 22726 [running]:\ngithub.com/milvus-io/milvus/internal/datanode.(*insertBufferNode).Sync(0xc00693c680, 0xc00a7c8d20?, {0xc00548eb58?, 0x0?, 0x0?}, 0xc00b7c10e0)\n/go/src/github.com/milvus-io/milvus/internal/datanode/flow_graph_insert_buffer_node.go:504 +0x1285\ngithub.com/milvus-io/milvus/internal/datanode.(*insertBufferNode).Operate(0xc00693c680, {0xc002e8f4c0, 0x1, 0x1})\n/go/src/github.com/milvus-io/milvus/internal/datanode/flow_graph_insert_buffer_node.go:139 +0x485\ngithub.com/milvus-io/milvus/internal/util/flowgraph.(*nodeCtx).work(0xc00690eaf0)\n/go/src/github.com/milvus-io/milvus/internal/util/flowgraph/node.go:127 +0x2ac\ncreated by github.com/milvus-io/milvus/internal/util/flowgraph.(*nodeCtx).Start\n/go/src/github.com/milvus-io/milvus/internal/util/flowgraph/node.go:71 +0x79", "answers": [{"content": "It looks like you're using Go client. Where were you running Milvus, on Docker?Can you create a Github issue, with code snippet, and errors you're seeing, here?https://github.com/milvus-io/milvus-sdk-go/issues/new/choosePing it here, and I can raise it to our Go team.  Thanks!", "votes": 1}, {"content": "It is also possible that your cluster is having an issue.   Is everything running?   Are you using the latest version?   Can you check out your cluster with Attu?  Are you seeing data?   Are all the services running?   It's possible multiple services are not running.What versions of Milvus, Milvus Go, Docker or K8, OS, etc... are you running?See:https://github.com/milvus-io/milvus/discussions/32459", "votes": 0}], "num_vote": "1", "num_answer": "2", "num_view": "44"}, {"url": "/questions/78605729/failed-to-store-embeddings-in-milvus-error-insert-fail-some-field-does-not-ex", "question_title": "Failed to store embeddings in Milvus: Error: Insert fail: some field does not exist for this collection in line. 0", "question": "Trying to add some embeddings to milvus bd collection but met with the above error.Code:const { MilvusClient, DataType } = require('@zilliz/milvus2-sdk-node');\nconst express = require('express');\nconst app = express();\napp.use(express.json());\n\nconst client = new MilvusClient(\"localhost:19530\");\n\nlet idCounter = 0;\n\nasync function main() {\ntry {\nconst collections = await client.showCollections();\nconsole.log(\"List all collections:\\n\", collections);\n} catch (error) {\nconsole.error(\"Error listing collections:\", error);\n}\n}\n\nasync function storeEmbeddings(filename, embeddings) {\ntry {\nconst collectionName = 'embedding_test_2';\n\n    // Check if collection already exists\n    const collectionsInfo = await client.showCollections();\n    const existingCollections = collectionsInfo.collection_names || [];\n    console.log('Existing collections:', existingCollections);\n\n    if (!existingCollections.includes(collectionName)) {\n        await client.createCollection({\n            collection_name: collectionName,\n            fields: [\n                { name: \"my_id\", data_type: DataType.Int64, is_primary_key: true, auto_id: false }, // Ensure primary key is int64\n                { name: \"filename\", data_type: DataType.VarChar, max_length: 256 }, // varchar type for strings\n                { name: \"embedding\", data_type: DataType.FloatVector, dim: embeddings.length } // Correct field definition\n            ],\n        });\n        console.log(`Collection ${collectionName} created`);\n    } else {\n        console.log(`Collection ${collectionName} already exists`);\n    }\n\n    // Logging field names and data to be inserted\n    console.log('Inserting data:', {\n        collection_name: collectionName,\n        fields_data: [\n            { name: \"my_id\", values: idCounter + 1 },\n            { name: \"filename\", values: filename },\n            { name: \"embedding\", values: [embeddings] }\n        ]\n    });\n\n    // Log the embedding array\n    console.log('Embedding array:', embeddings);\n\n    await client.insert({\n        collection_name: collectionName,\n        fields_data: [\n            { name: \"my_id\", values: [idCounter++] },\n            { name: \"filename\", values: [filename] },\n            { name: \"embedding\", values: [embeddings] }\n        ]\n    });\n\n    console.log(\"Embeddings stored successfully in Milvus\");\n} catch (error) {\n    console.error('Failed to store embeddings in Milvus:', error);\n}\n}\n\nmodule.exports = {\nstoreEmbeddings,\nmain\n};error:Failed to store embeddings in Milvus: Error: Insert fail: some field does not exist for this collection in line. 0\nat /Users/razataiab/Desktop/custom-rag/node_modules/@zilliz/milvus2-sdk-node/dist/milvus/grpc/Data.js:195:31\nat Array.forEach ()\nat /Users/razataiab/Desktop/custom-rag/node_modules/@zilliz/milvus2-sdk-node/dist/milvus/grpc/Data.js:192:28\nat Array.forEach ()\nat MilvusClient. (/Users/razataiab/Desktop/custom-rag/node_modules/@zilliz/milvus2-sdk-node/dist/milvus/grpc/Data.js:184:30)\nat Generator.next ()\nat fulfilled (/Users/razataiab/Desktop/custom-rag/node_modules/@zilliz/milvus2-sdk-node/dist/milvus/grpc/Data.js:5:58)\nat process.processTicksAndRejections (node:internal/process/task_queues:95:5)Trying to add some embeddings to milvus bd collection but met with the above error.", "answers": [{"content": "you should prepare row based data here. like this:await client.insert({\n  collection_name: collectionName,\n  data: [\n    { filename: 'some-file-name1',  embedding: [0.1,0.2,...]},\n    { filename: 'some-file-name1',  embedding: [0.1,0.2,...]},\n  ]\n});For now, the Node sdk accepts row based file, thanks.", "votes": 1}], "num_vote": "1", "num_answer": "1", "num_view": "58"}, {"url": "/questions/78605643/filtering-search-results-by-availability-in-milvus-using-an-external-bitset", "question_title": "Filtering Search Results by Availability in Milvus Using an External Bitset", "question": "I am currently working with the following setup:Milvus version 2.3.7, pymilvus version 2.3.6.A database in Milvus containing 4 million 768-dimensional vectors.My challenge involves performing a vector search across a large number of IDs, ranging from 10,000 to 500,000. For instance, if I have a query that matches 100,000 documents but only 70,000 of those are available in my inventory. I need to filter out the available items based on this information which I have in the form of a bitset.The current workflow is as follows:Query Milvus: Execute a query within Milvus to retrieve document IDs for matched vectors without availability consideration.Post-Processing: Implement post-filtering using the bitset to isolate only those documents with their corresponding availability bit set to '1'. This results in an O(n) computational complexity, where n is the size of the matched documents.I am seeking a method or strategy that either reduces this operation to O(1) time complexity or allows me to directly fetch available items via Milvus without needing any post-filtering.Milvus does support filtering with bitsets (applying filters prior to running the actual approximate nearest neighbor (ANN) search). As I already have an availability bitset on hand, is there any way to utilize this within Milvus?I appreciate any guidance or suggestions on how this can be achieved efficiently.", "answers": [{"content": "will you change the bitset by query patterns? If you have a static bitset map(set available ones as \"1\"), I think you can store it as scalar field. Milvus's new version will intro new index type - bitset map, which seems can handle your case.", "votes": 1}, {"content": "You can directly filtering with expr inventoryid in [1,2,3,5,8...]\nI'm pretty sure filtering on such a list will not be efficient(Much better if you can do range inventoryid> 100).You can not mmaping the bitset to the data outside milvus because bitset offset is segment level and user have no idea about how data is splited into multiple segments", "votes": 0}], "num_vote": "1", "num_answer": "2", "num_view": "75"}, {"url": "/questions/78585336/why-it-always-returns-only-10-rows-of-result-even-if-collection-num-entities100", "question_title": "Why it always returns only 10 rows of result even if collection.num_entities>1000 and limit=10", "question": "Why it always returns only 10 rows of result even if collection.num_entities>1000 and limit=10The script is as belowed:collection = Collection(name=\"test_collection\")\n\ncollection.load()\nprint(collection.num_entities)\nsearch_params = {\n\"metric_type\": \"IP\",\n\"params\": {\"nprobe\": 10}\n}\n\nnp_query_embeddings = np.array(query_embeddings, dtype=np.float32)\nnorms = np.linalg.norm([np_query_embeddings], axis=1, keepdims=True)\nnormalized_query_embeddings = np_query_embeddings / norms\n\nquery_vectors = normalized_query_embeddings.tolist()\nlimit = 1000\n\nresults = collection.search(\ndata=query_vectors,\nanns_field=\"embedding\",\nparam=search_params,\nlimit=limit,\nconsistency_level=\"Strong\",\n)\nprint(results)Display:8503 [\"['id: 449617239426872838, distance: 0.7159370183944702, entity: {}', 'id: 449617239426877164, distance: 0.7090992331504822, entity: {}', 'id: 449617239426880450, distance: 0.699631929397583, entity: {}', 'id: 449617239426876554, distance: 0.6782248020172119, entity: {}', 'id: 449617239426876178, distance: 0.6769057512283325, entity: {}', 'id: 449617239426880350, distance: 0.6754627823829651, entity: {}', 'id: 449617239426875291, distance: 0.6698116064071655, entity: {}', 'id: 449617239426877126, distance: 0.6647711396217346, entity: {}', 'id: 449617239426876516, distance: 0.6610612869262695, entity: {}', 'id: 449617239426880437, distance: 0.6571118831634521, entity: {}']\"]", "answers": [{"content": "print() will print limited results(default to 10) to reduce printed message size, especially if you search back embeddings.Please get all results byres.hits,res.idsorres.distancesRef:https://milvus.io/api-reference/pymilvus/v2.4.x/ORM/Collection/search.md", "votes": 1}, {"content": "If you refer to the Milvus Documentation linked here:https://milvus.io/api-reference/pymilvus/v2.2.x/Collection/search().mdWe see that thesearch()method returns aSearchResultobject, andprint(results)in fact calls thestr()method of theSearchResultclass, which only prints at most 10 query results as shown below in both the link and the code snippet:https://github.com/milvus-io/pymilvus/blob/e95f5a5294ff28588c14a2f3f5aae19d9210a47e/pymilvus/client/abstract.py#L515def __str__(self) -> str:\n    \"\"\"Only print at most 10 query results\"\"\"\n    return f\"data: {list(map(str, self[:10]))} {'...' if len(self) > 10 else ''}, cost: {self.cost}\"To print all search results, you can try implementing the following code:for i, result in enumerate(results):\n    print(\"Search result for vector #{}: \".format(i))\n    for j, res in enumerate(result):\n        print(\"  Rank {}: {}\".format(j, res))", "votes": 0}], "num_vote": "1", "num_answer": "2", "num_view": "44"}, {"url": "/questions/78553950/how-can-i-store-boolean-array-of-size-3000-efficiently-in-milvus", "question_title": "How can I store boolean array of size 3000 efficiently in milvus?", "question": "I have a boolean array which represents store-availability of retail products across 3000 different stores.so, my schema looks like below:product_id = FieldSchema(\n    name=\"product_id\", \n    dtype=DataType.INT64, \n    is_primary=True,\n    auto_id=False\n)\n\nproduct_title_vector = FieldSchema(\n    name=\"product_title_vector\", \n    dtype=DataType.FLOAT_VECTOR,\n    dim=768,\n)\n\nstore_availability = FieldSchema(\n    name=\"store_availability\", \n    dtype=DataType.ARRAY, \n    element_type=DataType.BOOL,\n    max_capacity=3000\n)The catch here is as per the general rule, boolean array of size 3000 will take ~3KB per record.I have 5 million items/records. so in total store_availability field alone will take 15GB as per the below calculation.memory = 3000 bytes/item * 5,000,000 items = 15GBAnd then I perform a milvus search with expr for filtering results based on given store-id like below:query_vector = np.random.random((1, 128)).astype(np.float32)\nquery_vector_resized = np.resize(query_vector, (1, 768))\n\nstore_id = 1\n\nfilter_expr = \"store_availability[{}] == true\".format(store_id - 1)\nprint(\"Filter Expression : \", filter_expr)\n\nsearch_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 16}}\nsearch_results = collection.search(\n    data=query_vector_resized,\n    anns_field=\"product_title_vector\",\n    expr=filter_expr,\n    param=search_params,\n    limit=1\n    \n)I am finding this 15GB for a single metadata field as resource-intensive. Although it is better performant, what if the number of stores increase to 40,000 in future.I tried sparse-vector representation but most of the products are available on 80% of the stores and still it consumes a lot of space.Any approaches that can be suggested for making this space-efficient while having filter-expr within search will be of great help!", "answers": [], "num_vote": "1", "num_answer": "0", "num_view": "24"}, {"url": "/questions/78552809/rpc-error-search-paramerror-code-1-message-search-data-value", "question_title": "RPC error: [search], <ParamError: (code=1, message=`search_data` value", "question": "Getting the following error while performing the milvus vector search, I have created milvus collection and also pushed the data after creating the embeddings using google generative ai embedding model but not able to perform the similarity search.def chat_collection(collection_name, query):\nMILVUS_HOST = \"host\"\nMILVUS_PORT = \"port\"# Initialize result list\nall_results = []\n\ntry:\n    # Connect to Milvus\n    connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)\n    print(\"Connected to Milvus\")\nexcept Exception as e:\n    print(f\"Error connecting to Milvus: {e}\")\n    return\n\n# List of collections to search in\ncollections = [collection_name]\nprint(\"Collections to search:\", collections)\n\nfor collection_name in collections:\n    try:\n        print(\"Processing collection:\", collection_name)\n       \n        # Load the collection from Milvus\n        collection = Collection(collection_name)\n        collection.load()\n        print(\"Collection loaded\")\n    except Exception as e:\n        print(f\"Error loading collection {collection_name}: {e}\")\n        continue\n\n    try:\n        # Create embeddings for the query\n        embeddings = GoogleGenerativeAIEmbeddings(model=os.getenv('EMBEDDING_MODEL'))\n        vector = embeddings.embed_query(query)\n        \n    \n    except Exception as e:\n        print(f\"Unexpected error creating embeddings: {e}\")\n        continue\n\n    try:\n        # Define search parameters\n        search_param = {\n            \"data\": [vector],\n            \"consistency_level\": \"Session\",\n            \"anns_field\": \"embedding\",\n            \"param\": {\"metric_type\": \"COSINE\", \"params\": {\"radius\": 0.7, \"range_filter\": 0.9}},\n            \"limit\": 15,\n            \"output_fields\": ['text', 'n_tokens', 'similarities'],\n        }\n        print(\"Search parameters defined:\", search_param)\n    except Exception as e:\n        print(f\"Error defining search parameters: {e}\")\n        continue\n\n    try:\n        # Perform search on the collection\n        results = collection.search(**search_param)\n        print(\"Search completed\")\n    except Exception as e:\n        print(f\"Error performing search: {e}\")\n        continueI was tring to find the context based on the query provided from the specified milvus collection.", "answers": [], "num_vote": "2", "num_answer": "0", "num_view": "136"}, {"url": "/questions/78545754/storing-and-retreiving-data-with-milvus-and-langchain", "question_title": "Storing and retreiving data with Milvus and Langchain", "question": "I'm trying to create a vector DB which will be populated with embeddings of articles from my employer's blog.I've got a Milvus instance up and running and am able to followthe walkthrough on the Langchain website.Based on the walkthrough, my implementation so far looks something like this:def parseWPDataFile(filename):\n    # redacted for brevity\n    return {\n        'meta': parsed_headers,\n        'body': doc_body.strip()\n    }\n\nparsed_doc = parseWPDataFile('sample_data.txt')\ntext_splitter = RecursiveCharacterTextSplitter(is_separator_regex=True, separators=['\\n+'], chunk_size=5000, length_function=len)\ndocs = text_splitter.create_documents([parsed_doc['body']], [parsed_doc['meta']])\nembeddings = OpenAIEmbeddings()\nvector_db = Milvus.from_documents(docs, embeddings, connection_args={\"host\": \"127.0.0.1\", \"port\": \"19530\"})This being my first time using a vector database, I'm a little confused by that last line.The documentation forMilvus.from_documentsindicates that it creates a vectorstore from documents, I guess, in memory. What I want is a persistent vectorstore that I can load stuff into and then later, in a separate script, pull from. I can't find any Langchain examples of this.How do I create a persistent VectorStore, add to it, and get a reference to it later, in another script?", "answers": [{"content": "I used the Milvus library directly to achieve what I wanted..First create the DB...import sys\nfrom pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, MilvusClient\n\n# print out some info about the DB and kill the script, so we don't insert duplicate data as this has already been run\nclient = MilvusClient(uri=\"http://localhost:19530\")\nres = client.describe_collection(collection_name=\"ot_chatbot\")\nprint(res)\nsys.exit()\n\n\nconnections.connect(host=\"localhost\",port=\"19530\")\n\n# primary id key\nitem_id = FieldSchema(\n    name=\"id\",\n    dtype=DataType.INT64,\n    is_primary=True\n)\n\n# text field to hold text content of embeddings\ntext = FieldSchema(\n    name=\"text\",\n    dtype=DataType.VARCHAR,\n    max_length=50000\n)\n\n# vector field to hiold embeddings\nembeddings = FieldSchema(\n    name=\"embeddings\",\n    dtype=DataType.FLOAT_VECTOR,\n    dim=1536 # for the \"text-embedding-ada-002\" embedding model\n)\n\n# source of the embedding (id, ourtownamerica.com)\nsource = FieldSchema(\n    name=\"source\",\n    dtype=DataType.VARCHAR,\n    max_length=20\n)\n\n# a URL pointing to the source of the data, if any\nurl = FieldSchema(\n    name=\"url\",\n    dtype=DataType.VARCHAR,\n    max_length=250\n)\n\n# a title, if any, of the wordpress article, for example\ntitle = FieldSchema(\n    name=\"source_title\",\n    dtype=DataType.VARCHAR,\n    max_length=250\n)\n\n# define collection schema \nschema = CollectionSchema(\n    fields=[item_id, text, embeddings, source, url, title],\n    description=\"OT Chatbot\",\n    enable_dynamic_field=False,\n    auto_id=True\n)\n\n# define the collection\ncollection = Collection(\n    name=\"ot_chatbot\",\n    schema=schema,\n    using='default'\n)\n\n# create an index to speed up queries\ncollection.create_index(\n    field_name=\"embeddings\",\n    index_params={\"metric_type\":\"IP\",\"index_type\":\"IVF_FLAT\",\"params\":{\"nlist\":16384}}\n)Then I was able to insert data into the DB like this...from pymilvus import connections, MilvusClient\nimport os\nimport sys\nimport constants\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\n\nos.environ[\"OPENAI_API_KEY\"] = constants.APIKEY\nembeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\nclient = MilvusClient(uri=\"http://localhost:19530\")\n\n# Parses the txt documents in wp-data\ndef parseWPDataFile(filename):\n    # redacted for brevity\n    return {\n        'meta': parsed_headers,\n        'body': doc_body.strip()\n    }\n\nparsed_doc = parseWPDataFile('sample_data.txt')\ntext_splitter = RecursiveCharacterTextSplitter(is_separator_regex=True, separators=['\\n+'], chunk_size=50000, length_function=len)\ndocs = text_splitter.create_documents([parsed_doc['body']], [parsed_doc['meta']])\n\nfor doc in docs:\n    meta = doc.metadata\n    text = doc.page_content\n    vector = embeddings.embed_query(text)\n    print(vector[:5])\n    res = client.insert(\n        collection_name=\"ot_chatbot\",\n        data=[{\n            \"text\": text,\n            \"embeddings\": vector,\n            \"source\": \"wp_site\",\n            \"url\": meta['Post URL'],\n            \"source_title\": meta['Post title']\n        }]\n    )\n    print(res)", "votes": 0}, {"content": "Your data should be stored in your local Milvus instance after you runvector_db = Milvus.from_documents(docs, embeddings, connection_args={\"host\": \"127.0.0.1\", \"port\": \"19530\"})If you are building a RAG system, you could then continue and search over your data with the followingfrom langchain.chains import RetrievalQA\nfrom langchain import hub\n\nquery = input(\"\\nQuery: \")\nprompt = hub.pull(\"rlm/rag-prompt\")\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vector_db.as_retriever(), \n    chain_type_kwargs={\"prompt\": prompt}\n)That should solve your problem.", "votes": -1}, {"content": "What the following code does is to establish a connector with a Milvus server at 127.0.0.1:19530, convert the vectors from docs and store them in the Milvus server. The server is responsible to load all that date both in memory as vector index for efficient search and persist in storage so that the data is not never lost.Milvus.from_documents(docs, embeddings, connection_args={\"host\": \"127.0.0.1\", \"port\": \"19530\"})Note that it doesn't create the Milvus server:The server must be set up in advance, as a docker or k8s deployment, by following instructions (e.g. fordocker)If docker or k8s is too heavy for you, we have the just released Milvus Lite which runs as a python library. By specifying a file instead of network address it will use the Milvus Lite automatically (so you don't need to set up any server before hand). See thecode exampleand a fully-fledgeddemo. (Just note the integration code is just merged so it takes a day or so for LangChain to release it in latest package.)", "votes": -1}], "num_vote": "0", "num_answer": "3", "num_view": "426"}, {"url": "/questions/78456883/pymilvus-exceptions-milvusexception-milvusexception-code-2200-message-retry", "question_title": "pymilvus.exceptions.MilvusException: <MilvusException: (code=2200, message=Retry run out of 75 retry times, message=incomplete query result, missing", "question": "What is the following problem?:pymilvus.exceptions.MilvusException: <MilvusException: (code=2200, message=Retry run out of 75 retry times, message=incomplete query result, missing id %!s(int64=242807), len(searchIDs) = 5, len(queryIDs) = 2, collection=449580455553423281: inconsistent requery result)>I am trying to do a hybrid search on milvus db. I have two vector embedding fields. based on those to I am trying to do a search using rerank of course. but after plenty time of waiting I am getting the previous error. Any suggestion or any other comment would be great.UPDATE1:\nUnfortunately, I can not share code.But I have two vector field both of them is sentence transformer embedding, they are dense.apart from this two fields, I have other varchar fields.I am doing hybrid search on these two vector fields.When I set my limit value 1 it is working, no problem. however If I increase the limit number, it is throwing the error that I just shared.the important part of the error is the partlen(searchIDs) = 5, len(queryIDs) = 2in which my limit is 5 and it is somehow getting 2 from somewhere I do not know the details. I have tracked your source code, but no chance.", "answers": [{"content": "Did you check the doc pages?Examplemulti-vector searchwhen the vectors are all dense.Examplemulti-vector searchwhen mix of sparse and dense vectors.I've also created anexample bootcamp notebookthat uses Milvus built-in BGE-M3 embedder to create sparse and dense embeddings with Reranker.Since you did not include any sample code or logs, we would need to see those to figure out what's going on.You might want to join ourMilvus discordand/or filea github issueand attach your code and error logs to it, to find out more.", "votes": 1}], "num_vote": "-1", "num_answer": "1", "num_view": "135"}, {"url": "/questions/78438922/milvusexception-milvusexception-code-65535-message-maximum-vector-fields-n", "question_title": "MilvusException: <MilvusException: (code=65535, message=maximum vector field's number should be limited to 4)>", "question": "I am getting the following error while I am creating 5 different vector field in milvusdb.MilvusException: <MilvusException: (code=65535, message=maximum vector field's number should be limited to 4)>Just in case I asked this question. I think the reason is milvus db is supporting maximum 4 vector field in the db. But any case, I want to ask this question.how can I increase the field size more than 4 in milvus db for vector fields??", "answers": [{"content": "https://github.com/milvus-io/milvus/blob/a5d1135512b42f702a2eba29fb79be8377d0bfe1/configs/milvus.yaml#L209You can config the number of the max vector fieldIf you install Milvus by the helm, you can follow this doc:https://milvus.io/docs/configure-helm.md", "votes": 2}, {"content": "Multiple vector fields (up to 10) per collection are supported since Milvus >= version 2.4.  Older versions of Milvus will only support a single vector index at a time.https://milvus.io/docs/multi-vector-search.md#:~:text=Since%20Milvus%202.4%2C%20we%20introduced,10)%20into%20a%20single%20collection.", "votes": 1}], "num_vote": "1", "num_answer": "2", "num_view": "274"}, {"url": "/questions/78410364/milvusexception-milvusexception-code-65535-message-efconstruction-out-of-ra", "question_title": "MilvusException: <MilvusException: (code=65535, message=efConstruction out of range: [1, 2147483647])>", "question": "I have the following error:MilvusException: <MilvusException: (code=65535, message=efConstruction out of range: [1, 2147483647])>while I am creating HNSW indexing on milvus db. any suggestion would be great. nothing on the internet.My vector size is 30552. maybe that can be the problem. I am not sure. just looking for a logical explanation.UPDATE-1: I have updated my create index parameters for efconstruction parameter. here is the configuration:{'metric_type': 'ss', 'index_type': 'bb', 'params': {'nlist': 5, 'efConstruction': 40}}after that I am getting the following error:MilvusException: <MilvusException: (code=65535, message=M out of range: [1, 2048])>thanks", "answers": [{"content": "After adding efconstruction and M parameters to my creat_index code. Problem is solved for me.here is the my latest index parameter:{'metric_type': 'IP', 'index_type': 'HNSW', 'params': {'nlist': 1024, 'efConstruction': 40, 'M': 1024}}Note: you may not need nlist parameter. just ignore that one.", "votes": 0}, {"content": "'nlist' is a hyperparameter of IVF_flat index.'M', 'efConstruction', 'ef' are hyperparameters for HNSW index.HNSW best practice params. Start with M: 4~64, larger M for larger data and larger embedding lengths. Then ef = efConstruction = M * 2.References:Milvus docs:https://milvus.io/docs/index.mdHNSW docs:https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md", "votes": 0}, {"content": "delete existing vector DB. Start a fresh.mostly the previous indexes created cause a conflict", "votes": 0}], "num_vote": "0", "num_answer": "3", "num_view": "825"}, {"url": "/questions/78357497/trouble-deploying-milvus-on-my-google-cloud-server-and-connect-to-it-as-a-client", "question_title": "Trouble deploying Milvus on my Google Cloud server and connect to it as a client", "question": "I am hosting a virtual machine on Google Cloud and run a Docker container to host Milvus server on that. I use VSCode's Remote Explorer. When I access to it as a client, it prints out errors like this:[search] retry:19, cost: 3.00s, reason: <_InactiveRpcError: StatusCode. UNAVAILABLE, failed to connect to all addresses; last error: UNKNOWN: ipv6:%5B::1%5D:19530: Failed to connect to remote host: Connection refused>The side effect was that I couldn't connect to the server anymore, including all services I deployed. It keeps connecting forever.Image of my VS Code TerminalI tried turn off and on the machine multiple times but the problem wasn't solved. I hope someone would help me to find the solution for my problem.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "53"}, {"url": "/questions/78352556/milvus-py-pymilvus-exceptions-milvusexception-milvusexception-code-1-messa", "question_title": "milvus.py: pymilvus.exceptions.MilvusException: <MilvusException: (code=1, message=Field full_length is not in the hit entity)>", "question": "I query a collection in a zilliz milvus db like this:documents = vector_store.similarity_search_with_score(query)The query is successful but in line 777 of milvus.py the valueresult.full_lengthis retrieved, which is not available:for result in res[0]:\n            data = {x: result.entity.get(x) for x in output_fields}\n            doc = self._parse_document(data)\n            pair = (doc, result.full_length)\n            ret.append(pair)which then leads to this exceptionFile \"/Users/tilman/LangchainCorsera/venv/lib/python3.9/site-packages/langchain_community/vectorstores/milvus.py\", line 644, in similarity_search\n    res = self.similarity_search_with_score(\n  File \"/Users/tilman/LangchainCorsera/venv/lib/python3.9/site-packages/langchain_community/vectorstores/milvus.py\", line 717, in similarity_search_with_score\n    res = self.similarity_search_with_score_by_vector(\n  File \"/Users/tilman/LangchainCorsera/venv/lib/python3.9/site-packages/langchain_community/vectorstores/milvus.py\", line 777, in similarity_search_with_score_by_vector\n    pair = (doc, result.full_length)\n  File \"/Users/tilman/LangchainCorsera/venv/lib/python3.9/site-packages/pymilvus/client/abstract.py\", line 588, in __getattr__\n    raise MilvusException(message=f\"Field {item} is not in the hit entity\")\npymilvus.exceptions.MilvusException: <MilvusException: (code=1, message=Field full_length is not in the hit entity)>Any clues?", "answers": [{"content": "I think you are trying to fetch full_length field but it's not defined in the output field of milvus.Check the code how to get output_fields", "votes": 0}, {"content": "Turned out this was a bug in langchain-community, that was resolved with an update", "votes": 0}, {"content": "The callstack shows:File \"/Users/tilman/LangchainCorsera/venv/lib/python3.9/site-packages/langchain_community/vectorstores/milvus.py\", line 644, in similarity_search\n    res = self.similarity_search_with_score(\n  File \"/Users/tilman/LangchainCorsera/venv/lib/python3.9/site-packages/langchain_community/vectorstores/milvus.py\", line 717, in similarity_search_with_score\n    res = self.similarity_search_with_score_by_vector(\n  File \"/Users/tilman/LangchainCorsera/venv/lib/python3.9/site-packages/langchain_community/vectorstores/milvus.py\", line 777, in similarity_search_with_score_by_vector\n    pair = (doc, result.full_length)But in the langchian source code, the line 777 of milvus.py is \"pair = (doc, result.score)\"https://github.com/langchain-ai/langchain/blob/v0.1.16/libs/community/langchain_community/vectorstores/milvus.pyDid you change your local code of langchain?", "votes": 0}], "num_vote": "0", "num_answer": "3", "num_view": "679"}, {"url": "/questions/78255035/langchain4j-how-to-get-document-by-id", "question_title": "langchain4j: How to get document by id", "question": "I can query a milvus vector database like thisval relevant: List<EmbeddingMatch<TextSegment>> = milvusEmbeddingStore.findRelevant(queryEmbedding, 10)So I then get a list of documents with ids.\nCan I somehow request one of these documents by it's id? Or by a metadata property?", "answers": [{"content": "Yes, but seems langchain4j doesn't include the ability. You can use thequery()method of the Milvus Java SDK,link to doc. I assume langchain4j has already included the Milvus Java SDK in its dependencies so you don't need to worry about introducing a new one.Actually langchain4j is already using thisquery()method itself but only to retrieve the raw embeddings as a side effect of searching for docs and does not expose it externally. Seehere. All you need to do is to addTEXT_FIELD_NAMEinto the output fields list.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "200"}, {"url": "/questions/78238530/rag-response-using-milvus-and-llama-index", "question_title": "RAG response using Milvus and llama index", "question": "I have created a vector embeddings collection using milvus locally and trying to generate RAG assisted responses. But for some reason its giving me error. I'm new to this so might be missing some thing.from llama_index.core import GPTVectorStoreIndex, StorageContext, VectorStoreIndex\nfrom llama_index.vector_stores.milvus import MilvusVectorStore\nimport openai\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\ndef chatbot(input_text):\n\n    vector_store = MilvusVectorStore(\n       uri=\"http://localhost:19530\",\n       collection_name = \"test\"\n    )\n    index = VectorStoreIndex.\n    query_engine = index.as_query_engine()\n    response = query_engine.query(input_text)\n    return response\n\n\ninput_text = input('Enter Query')\n\nprint(chatbot(input_text))Error:RPC error: [search], <MilvusException: (code=0, message=fail to search on all shard leaders, err=All attempts results:\nattempt #1:code: UnexpectedError, error: fail to Search, QueryNode ID=1, reason=Search 1 failed, reason [UnexpectedError] Assert \"field_meta.get_sizeof() == element.line_sizeof_\" at /go/src/github.com/milvus-io/milvus/internal/core/src/query/Plan.cpp:48\n => vector dimension mismatch err %!w(<nil>)\nattempt #2:context canceled\n)>, <Time:{'RPC start': '2024-03-28 18:14:01.950316', 'RPC error': '2024-03-28 18:14:02.181546'}>\nFailed to search collection: testMilvus collection:enter image description hereenter image description hereAny help will be greatly appreciated!I tried Running the above code to get a RAG assisted response from OpenAI.", "answers": [{"content": "It seems your vector dimension mismatch, this means your collection's vector is not fit your model. What embeeding model are you using. for example, text-embedding-3-large is 512 dimension, and your collection's vector demision is 768, then conflict happens", "votes": 2}], "num_vote": "1", "num_answer": "1", "num_view": "524"}, {"url": "/questions/78214919/pymilvus-search-embeddings-by-json", "question_title": "pymilvus search embeddings by JSON", "question": "I'm using pymilvus 2.2.13.\nI have a JSON field in my collection:\nmetadata:{\"firld_a\": \"x\", \"list_field\": [\"A\", \"B\", \"C\"]}I want to get all the embeddings that have A in their list field,\nI tried to use json_contains like the following example:self.collection.query(\n     expr=f'json_contains(metadata[\"list_field\"],\"A\")', \n     output_fields=['metadata']\n)and got a milvus error, how can I get the requested result?", "answers": [{"content": "You should usejson_contains_anyinstead ofjson_contains. Note thatjson_containsregards the value of each key as a whole, whilejson_contains_anychecks the elements.For details, refer toAdvanced Operatorshere.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "63"}, {"url": "/questions/78197621/optimizing-data-insertion-efficiency-in-milvus-collections-addressing-delays-wi", "question_title": "Optimizing Data Insertion Efficiency in Milvus Collections: Addressing Delays with Minimal Data Sets", "question": "When inserting minimal data into the Milvus collection, a significant amount of time is consumed, and the reason for this delay is not apparent. Could you kindly provide recommendations for resolving this issue?my collection details:fields = [FieldSchema(name='id', dtype=DataType.INT64, description='ids', max_length=200, is_primary=True, auto_id=False),\nFieldSchema(name='human_id', dtype=DataType.INT64, description='human_ids', max_length=2000, is_primary=False, auto_id=False),\nFieldSchema(name='human_name', dtype=DataType.VARCHAR, description='names of human classes', max_length=1000, auto_id=False),\n            FieldSchema(name='vector', dtype=DataType.FLOAT_VECTOR, description='embedding vectors', dim=embedding_dimensions),]\n\nschema = CollectionSchema(fields=fields, description='classification')\ncollection = Collection(name=collection_name, schema=schema)\nindex_params = {\n 'metric_type': metric_type,\n 'index_type': index_type,\n 'params':  {\"nlist\": parameter_number}}\ncollection.create_index(field_name=\"vector\", index_params=index_params)\n\nMETRIC_TYPE = \"COSINE\"\nINDEX_TYPE = \"IVF_FLAT\"\nPARAMETER_NUMBER = 128\n\ncollection = Collection(collection_name)\ncollection.insert(data)`Adjusting Shared Number Multiplicity for Improved Loading Efficiency in Collection Management", "answers": [{"content": "It looks like the code wasn't exactly pasted.  Here are some typical places to check for slow writes:What consistency did you specify in index-building?  \"Bounded\" is the default, but \"Eventually\" is the fastest.https://milvus.io/docs/consistency.md#:~:text=Milvus%20supports%20four%20consistency%20levels,it%20best%20suit%20your%20applicationYou can set different consistency levels for writing vs searching.  The reason is maybe you want to write fast, but after that be more careful about reading only updated data (usually only applies to bank-type data with many simultaneous users).Another common mistake is to insert small batches of data and .flush() every time after insert.  This will slow things down.  .flush() is only needed before a backup to S3 or after all the data has been inserted.If you're creating/deleting/recreating a collection repeatedly, did you set overwrite=True?\nmc.create_collection(COLLECTION_NAME,\nEMBEDDING_DIM,\nconsistency_level=\"Eventually\",\nauto_id=True,\noverwrite=True,)The system defaults for sharding are usually best.  Sometimes people change those, which could slow things down.  Here is ablog about it.If none of these things help, can youfile an issue here?  Please attach your code and error logs.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "180"}, {"url": "/questions/78041617/how-to-properly-optimize-spark-and-milvus-to-handle-big-data", "question_title": "How to properly optimize Spark and Milvus to handle big data?", "question": "I have a spark dataframe of 2 columns: id and vector.The vector column is a list of floats 20,000 elements long.Dataframe itself is 2,500,000 rows long.I use Spark-Milvus connector to insert the data as I tried variety of ways formatting small pieces of data and trying to insert it into Milvus collection to no avail.When I create a collection in Milvus and try to insert a batch of 200,0000 rows from spark dataframe it takes more than 10 minutes and sometimes crashes.Loading Milvus collection of 200,000 records takes more than 1h and is never ended.Once the batch is inserted it takes nearly 10 minutes to assign indices to vector column.I wonder if there's a general practice on dealing with big batches on how to optimize the processing time to insert and index.What setup of Spark and Milvus should I use to achieve best performance possible?And how to properly transform data before inserting into Milvus collection? Should data be presented as numpy array or any other format?A random row when collected from my spark dataframe would look like this: [1005, [0.01, ..., 0.78] where 1005 is an id and a list of floats is a 20,000 long vector.Here's my spark setup:spark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName(\"collab_filter_test_on_local\") \\\n    .config(\"spark.driver.extraClassPath\", '/data/notebook_files/clickhouse-native-jdbc-shaded-2.6.5.jar') \\\n    .config(\"spark.jars\", \"/data/notebook_files/spark-milvus-1.0.0-SNAPSHOT.jar\") \\\n    .config(\"spark.driver.memory\", \"16g\") \\\n    .getOrCreate()Here's my Milvus setup:connections.connect(alias=\"default\", host=\"localhost\", port=19530)\n\nfields = [\n    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=False),\n    FieldSchema(name='vec', dtype=DataType.FLOAT_VECTOR, dim=dim_size)\n]\n\nschema = CollectionSchema(fields, 'data')\n\ndata = Collection('data', schema)", "answers": [{"content": "I'm the developer of Spark-milvus connector. Glad to hear you are using it. Please create issue on github so that we can reply you more timel.For your question:1, 20000 dim vector is quite large, it will obviously takes more resource in all steps. Please consider whether it is necessary and whether we can reduce the dim2, Insert 200,0000 rows in 10 mins ~ about 3k/s. Actually it is not too bad. Please upload the error msg to github when you get crash.", "votes": 1}], "num_vote": "0", "num_answer": "1", "num_view": "193"}, {"url": "/questions/78018125/how-do-i-run-a-startup-script-to-create-a-milvus-db-on-a-gke-private-cluster-via", "question_title": "How do I run a startup script to create a Milvus DB on a GKE private cluster via Terraform?", "question": "I am trying to create a Milvus DB on a private GKE cluster. I have created the cluster using Terraform. I now want to run a .sh script that will create a Milvus DB on the cluster that follows this script (https://milvus.io/docs/gcp.md). However, I can't run the script because the cluster is private.I have tried:Using local-exec and remote-exec provisionerBastion HostBut, none of these work.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "50"}, {"url": "/questions/78004569/getting-exception-when-trying-to-delete-entity-in-milvis-vector-database-using-p", "question_title": "Getting exception when trying to delete entity in Milvis vector database using pymilvus", "question": "I am working on the python application and I am storing data in Milvus vector database. I have written the following code to delete entity in n Milvus vector database using pymilvus libaray.def delete_entities(self, collection_name, entity_id):\n        expr = \"obj_id in [\" + entity_id + \"]\"\n        collection = Collection(collection_name)\n        collection.delete(expr)When I execute the code, I am getting this error:RPC error: [delete], <MilvusException: (code=65535, message=failed to create expr plan, expr = obj_id in [65ce930d39989b871863b5dd])>, <Time:{'RPC start': '2024-02-16 00:53:20.651576', 'RPC error': '2024-02-16 00:53:20.653786'}>\nException in thread Thread-9:\nTraceback (most recent call last):\n  File \"C:\\Users\\usajid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 980, in _bootstrap_inner\n    self.run()\n  File \"C:\\Users\\usajid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 917, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\usajid\\Desktop\\semanticsearch\\src\\backend\\components\\consumer.py\", line 61, in subscribe_topic\n    self.COOPERANTS.delete_entities(collection_name, payload['documentKey']['_id']['$oid'])\n  File \"C:\\Users\\usajid\\Desktop\\semanticsearch\\src\\backend\\components\\clients.py\", line 50, in delete_entities\n    collection.delete(expr)\n  File \"c:\\Users\\usajid\\Desktop\\semanticsearch\\.venv\\lib\\site-packages\\pymilvus\\orm\\collection.py\", line 573, in delete        \n    res = conn.delete(self._name, expr, partition_name, timeout=timeout, **kwargs)\n  File \"c:\\Users\\usajid\\Desktop\\semanticsearch\\.venv\\lib\\site-packages\\pymilvus\\decorators.py\", line 135, in handler\n    raise e from e\n  File \"c:\\Users\\usajid\\Desktop\\semanticsearch\\.venv\\lib\\site-packages\\pymilvus\\decorators.py\", line 131, in handler\n    return func(*args, **kwargs)\n  File \"c:\\Users\\usajid\\Desktop\\semanticsearch\\.venv\\lib\\site-packages\\pymilvus\\decorators.py\", line 170, in handler\n    return func(self, *args, **kwargs)\n  File \"c:\\Users\\usajid\\Desktop\\semanticsearch\\.venv\\lib\\site-packages\\pymilvus\\decorators.py\", line 110, in handler\n    raise e from e\n  File \"c:\\Users\\usajid\\Desktop\\semanticsearch\\.venv\\lib\\site-packages\\pymilvus\\decorators.py\", line 74, in handler\n    return func(*args, **kwargs)\n  File \"c:\\Users\\usajid\\Desktop\\semanticsearch\\.venv\\lib\\site-packages\\pymilvus\\client\\grpc_handler.py\", line 602, in delete   \n    raise err from err\n  File \"c:\\Users\\usajid\\Desktop\\semanticsearch\\.venv\\lib\\site-packages\\pymilvus\\client\\grpc_handler.py\", line 596, in delete   \n    check_status(response.status)\n  File \"c:\\Users\\usajid\\Desktop\\semanticsearch\\.venv\\lib\\site-packages\\pymilvus\\client\\utils.py\", line 54, in check_status     \n    raise MilvusException(status.code, status.reason, status.error_code)\npymilvus.exceptions.MilvusException: <MilvusException: (code=65535, message=failed to create expr plan, expr = obj_id in [65ce930d39989b871863b5dd])>Can anyone tell me how to fix this issue?", "answers": [{"content": "I forgot to add '' in this lineexpr = \"obj_id in ['\" + entity_id + \"']\"Now the code is working fine", "votes": 0}], "num_vote": "-1", "num_answer": "1", "num_view": "322"}, {"url": "/questions/77929699/milvus-2-node-js-sdk-error-typeerror-class-extends-value-undefined-is-not-a-c", "question_title": "Milvus 2 Node.js SDK error: \"TypeError: Class extends value undefined is not a constructor or null\"", "question": "I'm working on myNuxt.jsapp. For the database, I have chosenMilvus. I want to do a similarity search, but I encountered an error in the browser console.I have the following code:app.vue// Other code\n\n<script setup lang=\"ts\">\nimport { MilvusClient } from \"@zilliz/milvus2-sdk-node\";\n\n// I also have other code not related to Milvus here, but I don't think it's relevant\n\n(async () => {\n  const milvusClient = new MilvusClient({\n    address: \"localhost:19530\",\n    username: \"\",\n    password: \"\",\n  });\n\n  // Similarity search\n  const test = await milvusClient.search({\n    collection_name: \"xxxxx\", // My collection name here\n    vector: [ // My vector here ],\n  });\n\n  // Sort the results by score in descending order\n  const sortedResults = test.results.sort((a, b) => b.score - a.score);\n\n  // Display the descriptions with scores\n  sortedResults.forEach((result) => {\n    console.log(`[${result.score}]: ${result.description}`);\n  });\n})();\n</script>package.json{\n  \"name\": \"nuxt-app\",\n  \"private\": true,\n  \"type\": \"module\",\n  \"scripts\": {\n    \"build\": \"nuxt build\",\n    \"dev\": \"nuxt dev\",\n    \"generate\": \"nuxt generate\",\n    \"preview\": \"nuxt preview\",\n    \"postinstall\": \"nuxt prepare\"\n  },\n  \"devDependencies\": {\n    \"autoprefixer\": \"^10.4.16\",\n    \"nuxt\": \"^3.9.3\",\n    \"postcss\": \"^8.4.33\",\n    \"tailwindcss\": \"^3.4.1\",\n    \"vue\": \"^3.4.6\",\n    \"vue-router\": \"^4.2.5\"\n  },\n  \"dependencies\": {\n    \"@nuxt/ui\": \"^2.10.0\",\n    \"@zilliz/milvus2-sdk-node\": \"^2.3.5\",\n    \"nuxt-security\": \"^1.1.0\"\n  }\n}As you can see inapp.vueabove, I'm using theMilvus 2 Node.js SDK, but I'm getting the following error in the browser console:TypeError: Class extends value undefined is not a constructor or null\n    at node_modules/@grpc/grpc-js/build/src/call.js (@zilliz_milvus2-sdk-node.js?v=bfe1cc22:10658:54)\n    at __require (chunk-PIXRU2QS.js?v=bfe1cc22:10:50)\n    at node_modules/@grpc/grpc-js/build/src/client.js (@zilliz_milvus2-sdk-node.js?v=bfe1cc22:11182:18)\n    at __require (chunk-PIXRU2QS.js?v=bfe1cc22:10:50)\n    at node_modules/@grpc/grpc-js/build/src/make-client.js (@zilliz_milvus2-sdk-node.js?v=bfe1cc22:11562:20)\n    at __require (chunk-PIXRU2QS.js?v=bfe1cc22:10:50)\n    at node_modules/@grpc/grpc-js/build/src/channelz.js (@zilliz_milvus2-sdk-node.js?v=bfe1cc22:11663:25)\n    at __require (chunk-PIXRU2QS.js?v=bfe1cc22:10:50)\n    at node_modules/@grpc/grpc-js/build/src/subchannel.js (@zilliz_milvus2-sdk-node.js?v=bfe1cc22:12227:22)\n    at __require (chunk-PIXRU2QS.js?v=bfe1cc22:10:50)I've searched this error in general and found multiple answers on StackOverflow saying that this is probably caused by circular dependency. I useddpdmto find potential circular dependency.I ran the following command:dpdm app.vueBut no circular dependency was found:\u2714 [0/0] Analyze done!\n\u2022 Dependencies Tree\n  - 0) app.vue\n\n\u2022 Circular Dependencies\n  \u2705 Congratulations, no circular dependency was found in your project.\n\n\u2022 WarningsI've been trying for quite some time, following official examples, but I'm not able to solve this error.", "answers": [{"content": "The library@grpc/grpc-jsdoesn't work on the browser, it only works on Node.js. It looks like you are getting it as a transitive dependency of the Milvus 2 Node.js SDK, which also states in its README that it requires Node.js.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "69"}, {"url": "/questions/77904452/milvus-hybrid-search-cannot-find-record-i-want", "question_title": "milvus hybrid search cannot find record I want", "question": "I am using milvus for a vector dababase. My data field is like [mailno, address, embedding], where mailno and address are string type and embedding is a float vector.\nI insert about 1300k records into the collection without any partition of the data, build index and load the index.Firstly I tried to search a vector, the return hit is too far away from the query and the distance value is also not small. Because my data is address data, it is easy to identify the similarity between two address string.Then I tried to find some record which is close to the query (e.g., same city, town, road etc)\nand use the 'mailno like \"xxx\"' as query to run a hybrid search. My purpose is to limit the search to the special record, so that I can check its distance value. However, the query filter is not working, the return hit is the same to the one without the query filter.I run the query alone, the correct record is returned, this means the query params are correct.Now I am confused, why the hybrid search is not working? Is it because the dataset is too large?\nI have tried the search on a small dataset, the embedding and distance is ok. The embedding algorithm takes into account of the geo information.", "answers": [{"content": "Now Milvus 2.4 support fuzzy match as wellfilter=\"mailno like '%xxx%'\"", "votes": 1}, {"content": "Try \"... like 'prefix%' \"filter=\"mailno like 'xxx%'\",More info:https://milvus.io/blog/2022-08-08-How-to-use-string-data-to-empower-your-similarity-search-applications.mdThe fastest metadata text filtering will be prefix matching or my_string in list, \"my_varchar_metada_column in ['list', 'of', 'strings'] \".More examples in this bootcamp, seehttps://github.com/milvus-io/bootcamp/blob/master/bootcamp/RAG/readthedocs_zilliz_langchain.ipynb> Cell#14", "votes": 0}], "num_vote": "0", "num_answer": "2", "num_view": "261"}, {"url": "/questions/77856774/milvus-backup-restore-very-slow-why-and-how-to-improve-performance", "question_title": "milvus-backup restore very slow , why and how to improve performance?", "question": "I try to backup and restore milvus 2.3.3 with milvus-backup 0.4 , making backup of collection with 160M entities seems pretty fast in 30min , but restore take more then 9h , please, advice if there is options to improve , the command I am using to restore is:milvus-backup restore -n mycollection --restore_indexP.S.\nI see milvus during restore is almost not using resource , only the pod with the backup-tool seems to do something:k top pods -n milvus --sort-by=cpu\nNAME                                   CPU(cores)   MEMORY(bytes)\nmilvus-backup-mycluster               1002m          330Mi\nmycluster-datanode-5788666898-jb47f    777m          855Mi\nmycluster-rootcoord-687cf4cb9-wp6n8     76m          132Mi\nmycluster-querycoord-c47b99b8c-hflsc    40m          137Mi\nmycluster-datacoord-84657c87df-zf9ld    27m          207Mi\nmycluster-proxy-78d487487b-v4x22        20m          127Mi\nmycluster-proxy-78d487487b-ntsjk        20m          134Mi\nmycluster-etcd-2                        20m          148Mi\nmycluster-etcd-1                        19m          137Mi\nmycluster-etcd-0                        19m          134Mi\nmycluster-pulsar-broker-0               19m          1465Mi", "answers": [{"content": "You can increase the datanode number and change parallelism config in backup.yaml.backup:\n  parallelism: \n    # Collection level parallelism to restore\n    restoreCollection: 2", "votes": 1}, {"content": "The recovery speed depends on bulk insert performance. During recovery, increase the number of datanode to improve bulk insert speed", "votes": 1}, {"content": "In config/backup.yamlgcPause:\n  address: http://localhost:9091I don't know what to fill in here so the backup can run. Currently the backup is still running but will report a not found error", "votes": 0}], "num_vote": "1", "num_answer": "3", "num_view": "287"}, {"url": "/questions/77648597/error-connecting-milvus-to-milvus-cloud-database-need-assistance", "question_title": "Error Connecting Milvus to Milvus Cloud Database: Need Assistance", "question": "Everyone I am facing issue with milvus connection and getting this error:\"pymilvus.exception.MilvusException: <MilvusException: (code = 2, message = Fail connecting to server on \"URI\" . Timout)>someone please help me with this.def connect:\n    URI = os.getenv('URI')\n    TOKEN = os.getenv('TOKEN')\n    connections.connect(uri = URI, token = TOKEN, secure = True)", "answers": [{"content": "Does curl work?curl --request GET--urlhttps://yoururl.api.gcp-us-west1.zillizcloud.com/v1/vector/collections--header 'accept: application/json'--header 'authorization: Bearer 'Did you try this code:\ngit clonehttps://github.com/zilliztech/cloud-vectordb-examples.gitAre you on a recent version of PyMilvus?\npip3 install pymilvus==2.4.3", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "337"}, {"url": "/questions/77647209/how-can-i-map-a-domain-to-milvus-vector-database", "question_title": "How can I map a domain to Milvus Vector Database?", "question": "I'm trying to install Milvus vector database on a server and set a domain name to it (using traefik preferably). So far no success: I cannot connect to that Milvus domain from Python script (when run locally: the connection works).What is your experience with setting up Milvus behind a domain? Thanks.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "41"}, {"url": "/questions/77631271/is-there-a-way-to-print-all-the-data-stored-in-milvus-db", "question_title": "Is there a way to print all the data stored in Milvus DB", "question": "I want to print all the data that is stored in Milvus without any filters in java.I tried going through the Milvus official documentation but couldn't find anything that works without filters. I am not looking for hacky that build filters such that all the data is returned.", "answers": [{"content": "Do either of these approaches work?dump a collection as-is withhttps://github.com/zilliztech/milvus-backupor -We have example Python code to create an iterator over the collection to read out all the rowshttps://milvus.io/docs/with_iterators.md", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "510"}, {"url": "/questions/77526525/milvus-vector-database-similarity-search", "question_title": "Milvus vector database similarity search", "question": "Does Milvus supports partial loading of collection in memory to perform similarity search? I mean, based on the input vector, will it be able to identify and auto-load clusters of vectors which most likely are similar?If no, is there any vectordb (like faiss, nmslib etc) which supports partial loading of indexes in memory?", "answers": [{"content": "You're probably looking for indexes like DiskANN - it holds ~10% of the vectors in memory and the rest on diskI believe IVF Flat also holds only the centroids and then does a flat search on the vectors within the closest centroids.", "votes": 0}], "num_vote": "-1", "num_answer": "1", "num_view": "198"}, {"url": "/questions/77506652/unable-to-connect-to-milvus-managed-cluster-via-milvusclient-python-api", "question_title": "Unable to connect to Milvus managed cluster via MilvusClient Python API", "question": "I have a very simple python code that attempts to connect to a managed Milvus cluster on GCP:client = MilvusClient(\n    uri=MY_URI,\n    token=MY_API_KEY)Though I keep getting this exception:pymilvus.exceptions.MilvusException: <MilvusException: (code=2, message=Fail connecting to server on MY_URI:443. Timeout)>I am however able to ping this instance from the terminal and see it as running on the Milvus dashboard. Any ideas or hints on what might be going on?Thanks!", "answers": [{"content": "Are you using free serverless Zilliz cloud? It uses GCP by default.\nSee this bootcamp for a RAG example using Zilliz.https://github.com/milvus-io/bootcamp/blob/master/bootcamp/RAG/readthedocs_zilliz_langchain.ipynb> Cell#4import os, pymilvus \nfrom pymilvus import connections\n\n# Connect to Zilliz cloud using endpoint URI and API key TOKEN.\nTOKEN = os.getenv(\"ZILLIZ_API_KEY\")\nCLUSTER_ENDPOINT=\"https://in03-xxxx.api.gcp-us-west1.zillizcloud.com:443\"\nconnections.connect(\n  alias='default',\n  uri=CLUSTER_ENDPOINT,\n  token=TOKEN,\n)", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "365"}, {"url": "/questions/77486580/is-pymilvus-client-thread-safe-fork-safe", "question_title": "Is PyMilvus client thread-safe & fork-safe?", "question": "I'm thinking about using Milvus vector storage in my Flask based project and looking at the PyMilvus (Python SDK) documentation. I haven't found any information yet about:Is PyMilvus thread-safe?Is PyMilvus fork-safe?How does connection pooling work in the SDK?Could you help me to sort it out?The official documentation doesn't contain too much information.", "answers": [{"content": "Currently PyMilvus version(v2.3.x) doesn't provide a thread pool or connection pool. Basically, PyMilvus has a global object \"connections\" to maintain client-to-server connections.User calls connections.connect() To create a connection:from pymilvus import (\n    connections,\n)\nconnections.connect(host=HOST, port=PORT, alias=\"xxx\")This method has a parameter \"alias\", it is the name of the connection.\nThe \"connections\" object internally maintains a map of name-to-connection. If you didn't provide the \"alias\", it will use \"default\" as the name of the connection.When you declare a collection, there is a parameter \"using\" to specify a connection name. If you didn't provide the \"using\", it will use \"default\" connection. All the interfaces of this Collection will work via this connection.collection = Collection(name=collection_name, using=\"xxx\")\ncollection.insert()\ncollection.search()\n......The connection object is thread-safe, which means you can call the collection's interfaces from different threads.\nBut the connection object cannot be shared by multiple sub-processes. So, if you fork a sub-process, you should ensure each subprocess creates its own connection by connections.connect().", "votes": 2}], "num_vote": "0", "num_answer": "1", "num_view": "191"}, {"url": "/questions/77453859/getting-error-while-starting-milvus-cli-on-centos7-dlopen-usr-lib64-libm-so-6", "question_title": "Getting Error while starting milvus_cli on Centos7 (dlopen: /usr/lib64/libm.so.6: version `GLIBC_2.29' not found)", "question": "Getting below error while running milvus-cli on Centos7./milvus_cli-v0.2.0-beta.2-Linux \n\n[1476] Error loading Python lib '/tmp/_MEIF9oMNS/libpython3.8.so.1.0': dlopen: /usr/lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/_MEIF9oMNS/libpython3.8.so.1.0)when we run ./milvus_cli-v0.2.0-beta.2-Linux, it should connect us to milvus_cli prompt.Required python version is already installed and available under /usr/local/bin/python3.11.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "55"}, {"url": "/questions/77448092/how-to-retrive-document-from-pymilvus-given-ids", "question_title": "How to retrive document from PyMilvus given ids?", "question": "usingconnections.connect(\"default\", host=cfg.db.connection.host, port=cfg.db.connection.port)\ncollection = Collection(name=\"ibss\")I can connect to Milvus database and select the collection.usingquery_vector = SentenceTransformerEmbeddings().embed_query(SOME_TEXT_HERE)\nsearch_params = {\n    \"metric_type\": \"L2\",\n    \"params\": {\"nprobe\": 10},\n}\n\nresults = collection.search(data=[query_vector], param=search_params, anns_field = \"vector\", limit = 3, output_fields = [])\n\n# the question is how to get the text out of ID \n# that can be used to remove\nfor hits in results:\n    for hit in hits: \n        print(hit)\n        breakI can get some output likeid: 445499747977925765, distance: 0.24354277551174164, entity: {}it turned out the entity is always empty regardless of the column - so I decided to leave it as[]to include all fields.now given the id \"445499747977925765\" I would like to retrieve the document;so I triedentity_id = 445499747977925765\n\n# Define the filter condition\nfilter_expr = f\"id == {entity_id}\"\n\n# Search with the filter condition\na = collection.search(\n    data=[],\n    anns_field=\"\",\n    param={\"filter\": filter_expr},\n    limit=1\n)but theaempty !to give you a full picture I am populating the database usingfrom langchain.vectorstores import Milvus\nvector_db = Milvus.from_documents(\n    deduplicated_documents,\n    SentenceTransformerEmbeddings(),\n    connection_args={\"host\": cfg.db.connection.host, \"port\": cfg.db.connection.port},\n    collection_name=cfg_data.target.collection.name,\n    drop_old=cfg_data.target.collection.renew\n)so I appreciate to hear how to retrial the document given ID", "answers": [{"content": "According tomilvus documentation, the condition should be passed within the keyexpr. like this:a = collection.search(\n    data=[],\n    anns_field=\"\",\n    expr=filter_expr,\n    limit=1\n)but as you're not trying to conduct vector search, it's better to use themethodqueryinstead ofsearch:a = collection.query(\n    expr=filter_expr,\n    limit=1\n)", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "319"}, {"url": "/questions/77407078/unable-to-delete-a-record-in-milvus-collection", "question_title": "Unable to delete a record in Milvus collection", "question": "The below code I have executed to delete a record from Milvus collection. There is no error but the respective record also not deleted.Please recommend solution for thisfrom pymilvus import connections,Collection,db,utility,FieldSchema, CollectionSchema\n\ndef connectVectorDB(**conStrings):\n    try:\n        milvusDB = connections.connect(\n            #db_name=conStrings['dbName'],\n            host=conStrings['hostname'],\n            port=conStrings['port']\n            )\n        db.using_database(conStrings['dbName'])\n        return milvusDB\n    except Exception as err:\n        raise err\n\ndbConnections = connectVectorDB(dbName='default',hostname=\"localhost\",port='19530')\nmyCol = Collection('DocStrings')\n\nexpression = \"docID == 445341931317291845\"\nmyCol.delete(expr=expression)\nmyCol.flush()", "answers": [{"content": "A couple of questions first:What Milvus and PyMilvus verion are you using?IsdocIddefined as a primary key in the collection?Also in the example above you're using==which is not allowed before 2.3.2.Before version 2.3.2:Milvus only supports deleting entities with clearly specified primary keys, which can be achieved merely with the term expressionin.I.e. One may use boolean expressions likeexpr = \"book_id in [0,1]\"wherebook_idis defined as primary key.Since 2.3.2it is possible to delete entities not only by primary key, but also using complex boolean expressions:Milvus supports deleting entities by primary key or complex boolean expressions. Deleting entities by primary key is much faster and lighter than deleting them by complex boolean expressions. This is because Milvus executes queries first when deleting data by complex boolean expressions.It worth to mention that:Deleted entities can still be retrieved immediately after the deletion if the consistency level is set lower thanStrong.Entities deleted beyond the pre-specified span of time for Time Travel cannot be retrieved again.Frequent deletion operations will impact the system performance.\nBefore deleting entities by comlpex boolean expressions, make sure the collection has been loaded.Deleting entities by complex boolean expressions is not an atomic operation. Therefore, if it fails halfway through, some data may still be deleted.Deleting entities by complex boolean expressions is supported only when the consistency is set toBounded.See:https://milvus.io/docs/delete_data.md", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "665"}, {"url": "/questions/77395962/exception-is-not-catched-in-nestjs", "question_title": "Exception is not catched in NestJS", "question": "The following is my class:import {\n  Injectable\n} from '@nestjs/common';\nimport {\n  MilvusClient\n} from '@zilliz/milvus2-sdk-node';\n\n@Injectable()\nexport class EmbeddingService {\n  private client: MilvusClient;\n\n  constructor() {\n    try {\n      this.client = new MilvusClient('localhost:19530');\n    } catch (error) {\n      console.error('Failed to connect to Milvus:', error);\n    }\n  }\n}My problem is that when I stop the milvus service (docker container) the abovenew MilvusClientcommand raises an error but the error is not catched by the catch block.I tried to wrap the try-catch inside an IIFE but the same problem and the catch block does not work.", "answers": [{"content": "this has nothing to do with nestjs nor nodejs itself, it just how thatMilvusClientwas implemented. Looks like they didn't expose any way to handle errors on theconstructormethod (which is pretty bad)I'd suggest you to report that in their repository:https://github.com/milvus-io/milvus-sdk-node/issues`", "votes": 2}, {"content": "I solved the error with the following approach:const milvusClient = new MilvusClient(`invalid-address`);\n\nmilvusClient.connectPromise.catch((e)=>{\n  console.log('connect failed')\n});", "votes": 0}], "num_vote": "0", "num_answer": "2", "num_view": "80"}, {"url": "/questions/77389051/when-using-the-milvus-standalone-server-is-is-possible-to-get-metrics", "question_title": "When using the Milvus standalone server, is is possible to get metrics?", "question": "TheMilvus documentationstates that metrics are available, but I've only seen examples of it working with the K8S deployment, and I'm using the standalone server. Is it possible to get metrics from the standalone deployment?", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "66"}, {"url": "/questions/77386596/how-to-improve-accuracy-of-the-large-milvus-index", "question_title": "How to improve accuracy of the large Milvus Index?", "question": "We are using the Milvus index with 300K documents. The accuracy of the results is not good. When we use the small index of around 30K the accuracy of the results was good. How to improve the accuracy for a larger Milvus index?", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "72"}, {"url": "/questions/77311990/milvus-pulsar-bookies-relation-to-s3-minio", "question_title": "Milvus pulsar bookies relation to S3 (Minio)", "question": "I have deployed Milvus cluster 2.2.2 in k8s and after some hardware problem two of my three pulsar bookies have their pv/pvc corrupted and failing to start.I see there is 50 ledgers underreplicated in the healthy bookie1 but when I try to execute:bookie1# booker shell recover bookie2(failed)\n bookie1# booker shell recover bookie3(failed)there is errors.So my question: I have loaded the data completely few months ago and I see it in the S3/Minio storage, but can I delete the ledgers and start the bookies from 0, will this affect data availability if the data inside the bookies is lost?Error 1:09:43:22.800 [main] ERROR org.apache.bookkeeper.bookie.Bookie - There \n are directories without a cookie, and this is neither a new \n environment, nor is storage expansion enabled. Empty directories are \n [/pulsar/data/bookkeeper/journal/current]\n 09:43:22.800 [main] INFO  \n org.apache.bookkeeper.proto.BookieNettyServer - Shutting down \n BookieNettyServer\n 09:43:22.829 [main] ERROR org.apache.bookkeeper.server.Main - Failed \n to build bookie server\n org.apache.bookkeeper.bookie.BookieException$InvalidCookieException:\n    at org.apache.bookkeeper.bookie.Bookie.checkEnvironmentWithStorageExpansion(Bookie.java:494) ~[org.apache.bookkeeper-bookkeeper-server-4.14.3.jar:4.14.3]\n    at org.apache.bookkeeper.bookie.Bookie.checkEnvironment(Bookie.java:273) ~[org.apache.bookkeeper-bookkeeper-server-4.14.3.jar:4.14.3]\n    at org.apache.bookkeeper.bookie.Bookie.<init>(Bookie.java:731) ~[org.apache.bookkeeper-bookkeeper-server-4.14.3.jar:4.14.3]\n    at org.apache.bookkeeper.proto.BookieServer.newBookie(BookieServer.java:152) ~[org.apache.bookkeeper-bookkeeper-server-4.14.3.jar:4.14.3]\n    at org.apache.bookkeeper.proto.BookieServer.<init>(BookieServer.java:120) ~[org.apache.bookkeeper-bookkeeper-server-4.14.3.jar:4.14.3]\n    at org.apache.bookkeeper.server.service.BookieService.<init>(BookieService.java:52) ~[org.apache.bookkeeper-bookkeeper-server-4.14.3.jar:4.14.3]\n    at org.apache.bookkeeper.server.Main.buildBookieServer(Main.java:304) ~[org.apache.bookkeeper-bookkeeper-server-4.14.3.jar:4.14.3]\n    at org.apache.bookkeeper.server.Main.doMain(Main.java:226) [org.apache.bookkeeper-bookkeeper-server-4.14.3.jar:4.14.3]\n    at org.apache.bookkeeper.server.Main.main(Main.java:208) [org.apache.bookkeeper-bookkeeper-server-4.14.3.jar:4.14.3]", "answers": [{"content": "From another thread:\nIf all the pulsar messages were consumed by milvus, then you can relaunch a new/clean pulsar service and stop the old pulsar service. Change the pulsar address in milvus.yaml to the new pulsar service and restart the milvus cluster. Milvus will work well with the new pulsar service.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "199"}, {"url": "/questions/77288578/error-parsing-text-format-milvus-proto-schema-collectionschema-103-expected-i", "question_title": "Error parsing text-format milvus.proto.schema.CollectionSchema: 10:3: Expected identifier, got: / unmarshal schema string failed", "question": "I am trying to create a collection in Milvus, load it in order to search over its data but get the following error:[libprotobuf ERROR /go/src/github.com/milvus-io/milvus/cmake_build/thirdparty/protobuf/protobuf-src/src/google/protobuf/text_format.cc:324] Error parsing text-format milvus.proto.schema.CollectionSchema: 10:3: Expected identifier, got: /\nunmarshal schema string failed\nAssert \"schema->get_primary_field_id().has_value()\" at /go/src/github.com/milvus-io/milvus/internal/core/src/common/Schema.cpp:86\nprimary key should be specified\nterminate called after throwing an instance of 'milvus::SegcoreError'\nwhat():  Assert \"schema->get_primary_field_id().has_value()\" at /go/src/github.com/milvus-io/milvus/internal/core/src/common/Schema.cpp:86\nprimary key should be specified\nSIGABRT: abort\nPC=0x40022d4e87 m=9 sigcode=18446744073709551610The following is the code by which I create the collection:async createDefaultCollection(): Promise<any> {\n    const params = {\n      collection_name: \"intentions\",\n      fields: [\n        {\n          name: \"intention\",\n          data_type: 101,\n          description: \"\",\n          type_params: {\n            dim: 1536,\n          },\n        },\n        {\n          name: \"intention_id\",\n          data_type: 5,\n          description: \"\",\n          is_primary_key: true,\n        },\n      ],\n    };\n\n    await this.client.createCollection(params);\n  }And the following is the code by which I am trying to load the collection:await this.client.loadCollection({\n        collection_name: \"intentions\"\n      });As soon as I call theloadCollectionI get the abovementioned error in the milvus-standalone logs.", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "27"}, {"url": "/questions/77110665/pymilvus-exceptions-paramerror-paramerror-code-1-message-field-data-size", "question_title": "pymilvus.exceptions.ParamError: <ParamError: (code=1, message=('Field data size misaligned", "question": "I want to  inport some data into milvus database\u3002sample data:{'CN': '\u8ba1\u8d39\u529e\u6cd5\u540d\u79f0', 'EN': 'Fee Method Name', 'ID': '0867', 'DESC': '\u8ba1\u8d39\u529e\u6cd5\u7684\u540d\u79f0\u3002'}but I got this error:Failed to insert batch starting at entity: 0/808\nTraceback (most recent call last):\nFile \"/home/itzuser/pythonProject/load_to_milvus.py\", line 97, in \\<module\\>\nMilvusWrapper.from_texts(texts,embed_model=embed_model,\nFile \"/home/itzuser/pythonProject/milvus_wrapper.py\", line 811, in from_texts\nvector_db.add_texts(texts=texts, metadatas=metadatas)\nFile \"/home/itzuser/pythonProject/milvus_wrapper.py\", line 456, in add_texts\nraise e\nFile \"/home/itzuser/pythonProject/milvus_wrapper.py\", line 450, in add_texts\nres = self.col.insert(insert_list, timeout=timeout, \\*\\*kwargs)\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/orm/collection.py\", line 497,   in insert\nres = conn.batch_insert(\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/decorators.py\", line 127, in  handler\nraise e from e\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/decorators.py\", line 123, in handler\nreturn func(\\*args, \\*\\*kwargs)\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/decorators.py\", line 162, in handler\nreturn func(self, \\*args, \\*\\*kwargs)\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/decorators.py\", line 102, in handler\nraise e from e\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/decorators.py\", line 68, in handler\nreturn func(\\*args, \\*\\*kwargs)\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py\", line 572, in batch_insertraise err from err\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py\", line 552, in batch_insert\nrequest = self._prepare_batch_insert_request(\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/client/grpc_handler.py\", line 536, in _prepare_batch_insert_request\nelse Prepare.batch_insert_param(collection_name, entities, partition_name, fields_info)\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/client/prepare.py\", line 520, in batch_insert_param\nreturn cls._parse_batch_request(request, entities, fields_info, location)\nFile \"/usr/local/python/lib/python3.10/site-packages/pymilvus/client/prepare.py\", line 488, in _parse_batch_requestraise ParamError(\npymilvus.exceptions.ParamError: \\<ParamError: (code=1, message=('Field data size misaligned for field \\[ID\\] ', 'got size=\\[807\\] ', 'alignment size=\\[808\\]'))\\>What is this reason for this error.", "answers": [{"content": "The error message here is particularly unhelpful, but this error relates to the fact that thedifferent rows of your insert have different fields in them.As an example of how this might happen, you might have used a splitter on a corpus of data and that splitter adds metadata to the resultant splits, and then you're aiming to put the splits, their metadata, and their embedded vectors into the database,but this splitter doesn't always add all the same metadata fields to every single split.pymilvustakes the first row of data, uses that as the canonical length (808 in your case) and then validates that every other row is that length. It comes across a row of length 807 in your case, and that makes it error.Why doespymilvusdo this? Ultimately, it comes down to this: Milvusdoesn't support nullable fields at the moment. Every field must be present in every row of the data you're adding.", "votes": 1}], "num_vote": "0", "num_answer": "1", "num_view": "3k"}, {"url": "/questions/77079589/why-localhost19530-shows-404-page-not-found-for-milvus-connection", "question_title": "Why localhost:19530 shows 404 page not found for Milvus connection", "question": "I'm trying to build a Milvus database by Docker and I follow the steps of this link:https://milvus.io/docs/install_standalone-docker.mdto make connection. However, when I look up localhost:19530 on Chrome, it always shows a \"404 page is not found\". is that normal or did I miss some steps", "answers": [{"content": "The 19530 is the grpc service port for milvus. And gprc is HTTP based protocol, but you cannot get an index page on it like a web service does, you should use a milvus client such as pymilvus to connect it for use Milvus.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "507"}, {"url": "/questions/76943934/how-to-deal-with-interference-in-large-model-driven-vector-databases-for-textual", "question_title": "How to deal with Interference in Large Model-Driven Vector Databases for Textual Similarity\uff1f", "question": "I'm using a vector database, and the more I use it, the more I realize there might be an issue.Currently, I'm using OpenAI's embedding interface to convert text into vectors and store them in the vector database. However, it seems that shorter texts are causing a lot of interference in the results.For example:Query: What is B of A?Text1: A's xxxx [dozens of texts here], B is xxx.Text2: A's cText3: d's BIn terms of vector similarity, the results might suggest that text2 and text3 are more similar. However, the expectation is definitely to return text1.Could you please provide any suggestions on how to address this issue?I am currently using Euclidean distance (L2). Should I replace it?", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "30"}, {"url": "/questions/76899118/how-much-memory-is-allocated-to-query-nodes-of-milvus", "question_title": "How much memory is allocated to query node(s) of Milvus?", "question": "I am working with Milvus standalone (v2.2.6). In Milvus limitations it is specified that the data to be loaded has to be less than 90% of the memory allocated to the query node(s). I have a requirement where I need to know how much memory is allocated to the query node(s). I tried searching online but couldn't get one. How to find out what is the total memory allocated to the query node(s)?", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "104"}, {"url": "/questions/76857199/how-to-use-refresh-flag-in-loadcollectionparam-properly", "question_title": "How to use 'refresh' flag in LoadCollectionParam properly?", "question": "In the code I found the next comment:// refresh:\n//   This flag must be set to FALSE when first time call the loadCollection().\n//   After loading a collection, call loadCollection() again with refresh=TRUE,\n//   the server will look for new segments that are not loaded yet and tries to load them up.Questions:How do I know when a new segment is added? Settingrefreshtotrueslows down the search significantly. That's why I want to do it when it's\nactually necessary.Do I need to load collection before every search? If not, when?I usemilvus-sdk-java 2.2.9", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "72"}, {"url": "/questions/76789673/attributeerror-while-using-milvus", "question_title": "AttributeError while using Milvus", "question": "[AttributeError: 'list' object has no attribute 'reshape'. Can anyone help to resolve why I'm getting this error in last for loop. Here I'm using open Ai api and getting embedded data for pdf text and now I want to store it on milvus database which is already running in background on docker. I'm beginner did all this from internet help but here I'm stuck please help me guys.]\n(https://i.sstatic.net/cxfWv.png)", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "161"}, {"url": "/questions/76739380/setting-username-password-for-milvus-attu-server", "question_title": "Setting username password for Milvus ATTU server", "question": "I have used docker-compose to install milvus, attu on ec2 machine.\nBelow is my docker-compose file.version: '3.5'\nservices:\n  etcd:\n    container_name: milvus-etcd\n    image: quay.io/coreos/etcd:v3.5.5\n    environment:\n      - ETCD_AUTO_COMPACTION_MODE=revision\n      - ETCD_AUTO_COMPACTION_RETENTION=1000\n      - ETCD_QUOTA_BACKEND_BYTES=4294967296\n      - ETCD_SNAPSHOT_COUNT=50000\n    volumes:\n      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd\n    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd\n  attu:\n    container_name: attu\n    image: zilliz/attu:v2.2.3\n    environment:\n      MILVUS_URL: milvus-standalone:19530\n      common.security.authorizationEnabled: \"true\"\n      # ATTU_ENABLE_AUTH: \"true\"\n      # ATTU_USERNAME: rohan  \n      # ATTU_PASSWORD: Viscaria@2567\n    ports:\n      - \"8001:3000\"\n    depends_on:\n      - \"standalone\"\n  minio:\n    container_name: milvus-minio\n    image: minio/minio:RELEASE.2022-03-17T06-34-49Z\n    environment:\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n    volumes:\n      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data\n    command: minio server /minio_data\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n  # nginx-auth:\n  #   container_name: nginx-auth\n  #   image: nginx:latest\n  #   volumes:\n  #     - ./etc/nginx/nginx.conf:/etc/nginx/nginx.conf\n  #   ports:\n  #     - \"82:80\"\n  #   depends_on:\n  #     - attu\n  standalone:\n    container_name: milvus-standalone\n    image: milvusdb/milvus:v2.2.8\n    command: [\"milvus\", \"run\", \"standalone\"]\n    environment:\n      ETCD_ENDPOINTS: etcd:2379\n      MINIO_ADDRESS: minio:9000\n    volumes:\n      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus\n    ports:\n      - \"19530:19530\"\n      - \"9091:9091\"\n    depends_on:\n      - \"etcd\"\n      - \"minio\"\n\nnetworks:\n  milvus:I want to setup username and password for the attu server, so that only users i want can access to the database, because currently if we click on connect, it connects without any authentication.Below is the screenshot of Attu UI that comes up.", "answers": [{"content": "You need to enable authentication in themilvus configurationfirst. If milvus is protected, attu requires you to use the credentials defined in themilvus/milvus.yaml.You can enable authentication inmilvus.yaml, according to this guide for docker:https://milvus.io/docs/configure-docker.md?tab=component", "votes": 0}], "num_vote": "2", "num_answer": "1", "num_view": "673"}, {"url": "/questions/76693928/how-can-i-get-more-context-aware-results-from-vector-similarity-search", "question_title": "How can I get more 'context-aware' results from Vector Similarity Search", "question": "I'm writing a document QA program that ingests a pdf document, runs an OpenAI ADA embedding over the documents and then stores the document in a vector db for later querying.The problem is that when I run queries on the index, it gets some of the questions right, and fails at others.An example: 'Who is the author of this book?' returns random authors from the 'references' section of the paper, when there is a sufficiently clear 'About the author section'.What is the best practise for making your queries and results more contextually aligned?Thank you!", "answers": [], "num_vote": "2", "num_answer": "0", "num_view": "345"}, {"url": "/questions/76549942/ways-to-include-context-in-the-score-generated-against-the-document", "question_title": "Ways to include context in the score generated against the document", "question": "I have a usecase with me. If you have any ideas or approaches on how to solve it (partially or fully) please let me know.I am using vector databasemilvusfor my application. Currently, the database contains columnscontentandembeddings. For each chunk (a chunk generally contains 2-3 lines of text), i am generating embeddings and storing them inembeddingscolumn.Now, coming to the use case. I have documents that speak about onboarding, how to do discord onboarding, are stored in vector db. I will query vector dbfind all docs that contains slack onboarding.Based on the cosine similarity between the query embedding and the list of docs embedding, vector db picks the docs that has minimum distance between them.So, i got list of docs which has onboarding and discord onboarding and a high score along with it. I am not aware of how the score is calculated. When i say high score assigned to each document listed in the resultant set, the scores are 83.5, 85.5 approx.Since my query contains the termonboarding, i got all onboarding docs. The query also containsslack. Is it possible to penalise all docs from the resultant set that doesn't have slack along with onboarding with it.If so, i wish to know how can it be done.", "answers": [], "num_vote": "1", "num_answer": "0", "num_view": "53"}, {"url": "/questions/76512038/this-version-of-sdk-is-incompatible-with-server-please-downgrade-your-sdk-or-u", "question_title": "'this version of sdk is incompatible with server, please downgrade your sdk or upgrade your server' error when I try to connect to the milvus server", "question": "I'm trying out Milvus implementations on a jupyter notebook, but I run into an error when I try to connect to the Milvus server. I'm attaching the cell and the corresponding error for reference. Note that I'm using pymilvus 2.2.9.Connect to Milvus service\nconnections.connect(host=HOST, port=PORT)Create collection\ncollection = create_milvus_collection(COLLECTION_NAME, DIM)\nprint(f'A new collection created: {COLLECTION_NAME}')Error:2023-06-19 03:51:10,646 - 8380245248 - decorators.py-decorators:108 - ERROR: RPC error: [__internal_register], <MilvusException: (code=1, message=this version of sdk is incompatible with server, please downgrade your sdk or upgrade your server )>, <Time:{'RPC start': '2023-06-19 03:51:10.636182', 'RPC error': '2023-06-19 03:51:10.646730'}>_InactiveRpcError                         Traceback (most recent call last)\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/decorators.py:163, in upgrade_reminder..handler(*args, **kwargs)\n162 try:\n163     return func(*args, **kwargs)\n164 except grpc.RpcError as e:File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/client/grpc_handler.py:1359, in GrpcHandler.__internal_register(self, user, host)\n1358 req = Prepare.register_request(user, host)\n1359 response = self._stub.Connect(request=req)\n1360 if response.status.error_code != common_pb2.Success:File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_interceptor.py:247, in _UnaryUnaryMultiCallable.call(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n240 defcall(self,\n241              request: Any,\n242              timeout: Optional[float] = None,\n(...)\n245              wait_for_ready: Optional[bool] = None,\n246              compression: Optional[grpc.Compression] = None) -> Any:\n247     response, ignored_call = self._with_call(request,\n248                                              timeout=timeout,\n249                                              metadata=metadata,\n250                                              credentials=credentials,\n251                                              wait_for_ready=wait_for_ready,\n252                                              compression=compression)\n253     return responseFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_interceptor.py:290, in _UnaryUnaryMultiCallable._with_call(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n287 call = self._interceptor.intercept_unary_unary(continuation,\n288                                                client_call_details,\n289                                                request)\n290 return call.result(), callFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_channel.py:379, in _InactiveRpcError.result(self, timeout)\n378 \"\"\"See grpc.Future.result.\"\"\"\n379 raise selfFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_interceptor.py:274, in _UnaryUnaryMultiCallable._with_call..continuation(new_details, request)\n273 try:\n274     response, call = self._thunk(new_method).with_call(\n275         request,\n276         timeout=new_timeout,\n277         metadata=new_metadata,\n278         credentials=new_credentials,\n279         wait_for_ready=new_wait_for_ready,\n280         compression=new_compression)\n281     return _UnaryOutcome(response, call)File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_channel.py:1043, in _UnaryUnaryMultiCallable.with_call(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n1041 state, call, = self._blocking(request, timeout, metadata, credentials,\n1042                               wait_for_ready, compression)\n1043 return _end_unary_response_blocking(state, call, True, None)File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_channel.py:910, in _end_unary_response_blocking(state, call, with_call, deadline)\n909 else:\n910     raise _InactiveRpcError(state)_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.UNIMPLEMENTED\ndetails = \"unknown method Connect for service milvus.proto.milvus.MilvusService\"\ndebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:19530 {created_time:\"2023-06-19T03:51:10.642309+05:30\", grpc_status:12, grpc_message:\"unknown method Connect for service milvus.proto.milvus.MilvusService\"}\"The above exception was the direct cause of the following exception:MilvusException                           Traceback (most recent call last)\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/client/grpc_handler.py:120, in GrpcHandler._wait_for_channel_ready(self, timeout)\n119 grpc.channel_ready_future(self._channel).result(timeout=timeout)\n120 self._setup_identifier_interceptor(self._user)\n121 returnFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/client/grpc_handler.py:220, in GrpcHandler._setup_identifier_interceptor(self, user)\n219 host = socket.gethostname()\n220 self._identifier = self.__internal_register(user, host)\n221 self._identifier_interceptor = interceptor.header_adder_interceptor([\"identifier\"], [str(self._identifier)])File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/decorators.py:109, in error_handler..wrapper..handler(*args, **kwargs)\n108     LOGGER.error(f\"RPC error: [{inner_name}], {e}, Time:{record_dict}\")\n109     raise e\n110 except grpc.FutureTimeoutError as e:File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/decorators.py:105, in error_handler..wrapper..handler(*args, **kwargs)\n104     record_dict[\"RPC start\"] = str(datetime.datetime.now())\n105     return func(*args, **kwargs)\n106 except MilvusException as e:File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/decorators.py:136, in tracing_request..wrapper..handler(self, *args, **kwargs)\n135     self.set_onetime_request_id(req_id)\n136 ret = func(self, *args, **kwargs)\n137 return retFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/decorators.py:85, in retry_on_rpc_failure..wrapper..handler(self, *args, **kwargs)\n84     else:\n85         raise e\n86 except Exception as e:File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/decorators.py:50, in retry_on_rpc_failure..wrapper..handler(self, *args, **kwargs)\n49 try:\n50     return func(self, *args, **kwargs)\n51 except grpc.RpcError as e:\n52     # DEADLINE_EXCEEDED means that the task wat not completed\n53     # UNAVAILABLE means that the service is not reachable currently\n54     # Reference:https://grpc.github.io/grpc/python/grpc.html#grpc-status-codeFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/decorators.py:168, in upgrade_reminder..handler(*args, **kwargs)\n166     msg = \"this version of sdk is incompatible with server, please downgrade your sdk or upgrade your \"167           \"server \"\n168     raise MilvusException(message=msg) from e\n169 raise eMilvusException: <MilvusException: (code=1, message=this version of sdk is incompatible with server, please downgrade your sdk or upgrade your server )>The above exception was the direct cause of the following exception:MilvusException                           Traceback (most recent call last)\nCell In[7], line 2\n1 # Connect to Milvus service\n2 connections.connect(host=HOST, port=PORT)\n4 # Create collection\n5 collection = create_milvus_collection(COLLECTION_NAME, DIM)File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/orm/connections.py:349, in Connections.connect(self, alias, user, password, db_name, token, **kwargs)\n345         if parsed_uri.scheme == \"https\":\n346             kwargs[\"secure\"] = True\n349     connect_milvus(**kwargs, user=user, password=password, token=token, db_name=db_name)\n350     return\n352 # 2nd Priority, connection configs from envFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/orm/connections.py:282, in Connections.connect..connect_milvus(**kwargs)\n279 t = kwargs.get(\"timeout\")\n280 timeout = t if isinstance(t, (int, float)) else Config.MILVUS_CONN_TIMEOUT\n282 gh._wait_for_channel_ready(timeout=timeout)\n283 kwargs.pop('password')\n284 kwargs.pop(\"token\", None)File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pymilvus/client/grpc_handler.py:123, in GrpcHandler._wait_for_channel_ready(self, timeout)\n121         return\n122     except (grpc.FutureTimeoutError,  MilvusException) as e:\n123         raise MilvusException(Status.CONNECT_FAILED,\n124                               f'Fail connecting to server on {self._address}. Timeout') from e\n126 raise MilvusException(Status.CONNECT_FAILED, 'No channel in handler, please setup grpc channel first')MilvusException: <MilvusException: (code=2, message=Fail connecting to server on 127.0.0.1:19530. Timeout)>I'm stuck, I had a grpcio issue when I was trying in colab asking me to downgrade my version to 1.53.0 and that did not work after downgrading either. I moved to jupyter and ran into what seems like a grpcio as well.", "answers": [{"content": "As @FakeAlcohol mentioned fromhttps://milvus.io/docs/install-pymilvus.mdIt is recommended to install a PyMilvus version that matches the version of the Milvus server you installed. For more information, see Release Notes.At least the major.minor versions need to match between pymilvus and milvus server.Like this, I can see my pymilvus: 2.3.6!pip install pymilvus\nimport pymilvus\nprint(pymilvus.__version__)For my local server, I can check Docker Desktop, click on the Container, mine is 2.3.7.2.3.6 will run on 2.3.7 because major.minor versions match.In case you're running your Milvus on Zilliz,https://stackoverflow.com/a/78143787/11494504, you can check server version like this:import os\nfrom pymilvus import connections, utility\n\nTOKEN = os.getenv(\"ZILLIZ_API_KEY\")\n# Connect to Zilliz cloud using endpoint URI and API key TOKEN.\nCLUSTER_ENDPOINT=\"https://in03-48a5b11fae525c9.api.gcp-us-west1.zillizcloud.com:443\"\nconnections.connect(\n    alias='default',\n    uri=CLUSTER_ENDPOINT,\n    token=TOKEN,\n) \n\nprint(f\"Server version: {utility.get_server_version()}\")", "votes": 1}, {"content": "I had a similar issue with pymilvus==2.2.9. I used Jupyter Notebook too.pip install pymilvus==2.2.5 solved the issue for me.\n(If your. milvus version is 2.2.*)https://pypi.org/project/pymilvus/", "votes": 0}, {"content": "The similar issue was resolved by upgrading the Milvus server to version 2.2.10 and reinstalling the related libraries:pip3 install protobuf==3.20.0,pip3 install grpcio-tools, andpip3 install pymilvus==2.2.11.It is recommended to install a PyMilvus version that matches the version of the Milvus server you installed. For more information, see Release Notes.fromhttps://milvus.io/docs/install-pymilvus.md", "votes": 0}], "num_vote": "1", "num_answer": "3", "num_view": "3k"}, {"url": "/questions/76260835/milvus-2-2-8-stuck-at-create-index-why", "question_title": "Milvus 2.2.8 stuck at create_index, why?", "question": "enter image description hereHow should I solve it\uff1f", "answers": [{"content": "Would be nice to have more context, ie the rest of the code block as well as the output and/or the error message you're seeing. From what I can tell, I think changingself.milvus.flush_()toself.milvus.flush()might help fix it", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "111"}, {"url": "/questions/76032576/rpc-error-batch-insert-milvusexception-code-1", "question_title": "RPC error: [batch_insert], <MilvusException: code=1", "question": "I'm inserting 20 million data into milvus database in linux server. while inserting the embedding vectors the the following error is showing up :RPC error: [batch_insert], <MilvusException: (code=1, message=<_MultiThreadedRendezvous of RPC that terminated with:\n        status = StatusCode.RESOURCE_EXHAUSTED\n        details = \"grpc: received message larger than max (704699372 vs. 67108864)\"\n        debug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:19530 {grpc_message:\"grpc: received message larger than max (704699372 vs. 67108864)\", grpc_status:8, created_time:\"2023-04-17T11:41:39.477035873+05:30\"}\"\n>)>, <Time:{'RPC start': '2023-04-17 11:41:10.531339', 'RPC error': '2023-04-17 11:41:39.548799'}>i tried updating the MAX_MESSAGE_LENGTH but that doesn't work.>>>MAX_MESSAGE_LENGTH =256*1024*1024\n>>>server = grpc.server(futures.ThreadPoolExecutor(max_workers=10), options=[('grpc.max_send_message_length',MAX_MESSAGE_LENGTH), ('grpc.max_receive_message_length',MAX_MESSAGE_LENGTH)])\n>>>server.add_insecure_port('[::]:19530')i am new to the milvus database.", "answers": [{"content": "It seems like it's not a problem with your server but with Milvus asthis test-casesuggests that there's a 64mb limit for insertions.", "votes": 0}, {"content": "I had the same problem, and I found that is was related to the configuration variablesproxy.grpc.serverMaxRecvSizeandproxy.grpc.serverMaxSendSize.Adjusting these fixed the problem for me.", "votes": 0}], "num_vote": "1", "num_answer": "2", "num_view": "2k"}, {"url": "/questions/76001133/milvus-multiple-vectors-in-a-row", "question_title": "Milvus multiple vectors in a row", "question": "Is it possible to have multiple vectors in a row on milvus. What is needed is a custom score that is like a weighted average of the individual vector scores. Is there a limit on the number of vectors we can have on milvus? If not possible on milvus, any other open source vector db with this facility? Possible to index all vector columns?Thanks", "answers": [{"content": "The multiple vector support afterMilvue 2.4, allows storing, indexing, and applying reranking strategies to multiple vector fields of different types in a single collection.PerMulti-Vector Search docSince Milvus 2.4, we introduced multi-vector support and a hybrid search framework, which means users can bring in several vector fields (max to 10) into one collection. Different vector fields can represent different aspects, different embedding models or even different modalities of data characterizing the same entity, which greatly expands the richness of information.Sample codescollection = Collection(name='{your_collection_name}') \n\n\nres = collection.hybrid_search(\n    reqs=[\n        AnnSearchRequest(\n            data=[['{your_text_query_vector}']],  \n            anns_field='{text_vector_field_name}',  # Textual data vector field\n            param={\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}}, # Search parameters\n            limit=2\n        ),\n        AnnSearchRequest(\n            data=[['{your_image_query_vector}']],  \n            anns_field='{image_vector_field_name}',  # Image data vector field\n            param={\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}}, # Search parameters\n            limit=2\n        )\n    ],\n\n    # Use WeightedRanker to combine results with specified weights\n    rerank=WeightedRanker(0.8, 0.2), # Assign weights of 0.8 to text search and 0.2 to image search\n    # Alternatively, use RRFRanker for reciprocal rank fusion reranking\n    # rerank=RRFRanker(),\n    limit=2\n)", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "510"}, {"url": "/questions/75968898/unable-to-access-milvus-application", "question_title": "Unable to access Milvus application", "question": "I installed Milvus on my mac using the documentationhttps://milvus.io/docs/install_standalone-docker.mdAfter following all the steps I am able to see in my docker desktop under milvus name 3\nimages are running and the standalone is running on 19530 and 9091.However when I open localhost:19530 nothing shows up and in case of localhost:9091 it shows 404 page not found.I also see fail connecting to server when I tried using a python script.What could be the issue and how to resolve it?", "answers": [{"content": "19530 is the grpc port, and 9091 is for web service(API). So you could not use a browser to visit via those ports.\nIf you want to use UI tools to manage Milvus, you could tryAttu", "votes": 0}], "num_vote": "-1", "num_answer": "1", "num_view": "990"}, {"url": "/questions/75845820/can-we-deploy-milvus-standalone-in-aws-ec2", "question_title": "can we deploy milvus standalone in aws ec2? [closed]", "question": "Closed.This question isnot about programming or software development. It is not currently accepting answers.This question does not appear to be abouta specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic onanother Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.Closed3 months ago.Improve this questioni was following the documentation, but there is a process for only milvus cluster.But I want to deploy milvus standalone in my aws ec2. Is there any specific process?", "answers": [{"content": "You can tryhttps://cloud.zilliz.com, which currently supports AWS and GCP. Zilliz Cloud is a SaaS service provided by the Milvus team.", "votes": 3}, {"content": "Have you checked the docs on the below link:https://milvus.io/docs/install_standalone-docker.mdIt should describe clearly how to install a standalone Milvus server with docker.And if you could also try usingEmbedded Milvusfor testing purposes.", "votes": 1}, {"content": "Milvus Standalone supported installations using  for earlier versions without Kubernetes and Docker.https://milvus.io/docs/v2.0.x/install_standalone-aptyum.mdThey have removed support for this in the newer versions. It can now be installed only using Kubernetes or Docker.", "votes": 0}], "num_vote": "-1", "num_answer": "3", "num_view": "2k"}, {"url": "/questions/75680183/milvus-similarity-search-by-cosine", "question_title": "Milvus, similarity search by cosine", "question": "How to apply cosine similarity calculation in Milvus when search for Float Vectors, without normalizing the vectors in-place ?", "answers": [], "num_vote": "0", "num_answer": "0", "num_view": "722"}, {"url": "/questions/75316894/binary-classification-text-based-on-embedding-distance", "question_title": "Binary Classification [Text] based on Embedding Distance?", "question": "I have 30 news articles that I love and would like to find more of. I createdembeddingsusing DistilBert and saved to Faiss and Milvus in dbs calledILoveTheseArticles(I'm trying both out). These feature vectors have the same dimensions and max characters. As I bring in more news I would like to vectorize each new article and find the nearest top article in theILoveTheseArticlesto get the distance. Based on that distance I would like to keep or discard the new article almost as a binary classifier where I don't need to constantly train a kernel every time I add new similar articles.Figure 1Figure 1 - medium articleAs a Cosine Similarity example (Figure1) if OA and OB exist inILoveTheseArticlesand I search with a new embedding OC I would get OB closest to OC at 0.86 and if the threshold for keeping is say 0.51 I would keep the 0C article as it was similar to an article that I love.As an L2 example (Figure1) if A' and B' exist inILoveTheseArticlesand I search for C' with a threshold of say 10.5 I would reject C' as B' is closest to C' at 20.62.Is it possible to infer similar news articles using this approach with embeddings and distance? I second guess this approach when I read confusing answers to a similar-ishquestion. Is Cosine Similarity or IP better then L2 or vice versa in this scenario?", "answers": [{"content": "This is a great way of finding similar articles. When it comes to the different distance calculations, there isn't too much of a difference between them as the embeddings from distilbert aren't based on word frequencies anymore, but on the weights assigned by the model for the overall text. Whichever you chose should return similar rankings as an output of the similarity search.The harder part will be figuring out where to set your cutoff and I believe this will come down to manually checking results to see what a good limit would be.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "354"}, {"url": "/questions/75031333/docker-compose-volume-definition-using-environment-variable", "question_title": "Docker compose volume definition using environment variable", "question": "I'm running a containerized Milvus Standalone database (Milvus) and I'm trying to find the location of items added to the database. In the docker-compose.yml file, the volume location is defined as follows:volumes:\n      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcdChecking my docker server, I do not find an environment variable named DOCKER_VOLUME_DIRECTORY.\nWhat does this definition mean? Also, what does the:-.part mean?", "answers": [{"content": "It is usingShell Parameter expansion:${parameter:-word}Ifparameteris unset or null, thenwordis used as a default value.In this case, asDOCKER_VOLUME_DIRECTORYis not set, the default value of.(the current directory) is used.$ echo ${DOCKER_VOLUME_DIRECTORY:-.}\n.So the volume will effectively be:volumes:\n  - ./volumes/etcd:/etcd", "votes": 2}], "num_vote": "1", "num_answer": "1", "num_view": "1k"}, {"url": "/questions/74859121/pod-has-unbound-immediate-persistentvolumeclaims-i-deployed-milvus-server-using", "question_title": "pod has unbound immediate PersistentVolumeClaims, I deployed milvus server using", "question": "etcd:\nenabled: true\nname: etcd\nreplicaCount: 3\npdb:\ncreate: false\nimage:\nrepository: \"milvusdb/etcd\"\ntag: \"3.5.0-r7\"\npullPolicy: IfNotPresentservice:\ntype: ClusterIP\nport: 2379\npeerPort: 2380auth:\nrbac:\nenabled: falsepersistence:\nenabled: true\nstorageClass:\naccessMode: ReadWriteOnce\nsize: 10GiEnable auto compactioncompaction by every 1000 revisionautoCompactionMode: revision\nautoCompactionRetention: \"1000\"Increase default quota to 4GextraEnvVars:name: ETCD_QUOTA_BACKEND_BYTES\nvalue: \"4294967296\"name: ETCD_HEARTBEAT_INTERVAL\nvalue: \"500\"name: ETCD_ELECTION_TIMEOUTenter code herevalue: \"2500\"Configuration values for the pulsar dependencyref:https://github.com/apache/pulsar-helm-chartenter image description hereI am trying to run the milvus cluster using kubernete in ubuntu server.\nI used helm menifesthttps://milvus-io.github.io/milvus-helm/Values.yamlhttps://raw.githubusercontent.com/milvus-io/milvus-helm/master/charts/milvus/values.yamlI checked PersistentValumeClaim their was an error\nno persistent volumes available for this claim and no storage class is set", "answers": [{"content": "This error comes because you dont have a Persistent Volume.\nA pvc needs a a pv with at least the same capacity of the pvc.This can be done manually or with a Volume provvisioner.The most easy way someone would say is to use the local storageClass, which uses the diskspace from the node where the pod is instanciated, adds a pod affinity so that the pod starts allways on the same node and can use the volume on that disk. In your case you are using 3 replicas. Allthough its possible to start all 3 instances on the same node, this is mostlikly not what you want to achieve with Kubernetes. If that node breaks you wont have any other instance running on another node.You need first to thing about the infrastructure of your cluster. Where should the data of the volumes be stored?An Network File System, nfs, might be a could solution.\nIn this case you have an nfs somewhere in your infrastructure and all the nodes can reach it.So you can create a PV which is accessible from all your node.To not allocate a PV always manualy you can install a Volumeprovisioner inside your cluster.I use in some cluster this one here:https://github.com/kubernetes-sigs/nfs-subdir-external-provisionerAs i said you must have already an nfs and configure the provvisioner.yaml with the path.it looks like this:# patch_nfs_details.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nfs-client-provisioner\n  name: nfs-client-provisioner\nspec:\n  template:\n    spec:\n      containers:\n        - name: nfs-client-provisioner\n          env:\n            - name: NFS_SERVER\n              value: <YOUR_NFS_SERVER_IP>\n            - name: NFS_PATH\n              value: <YOUR_NFS_SERVER_SHARE>\n      volumes:\n        - name: nfs-client-root\n          nfs:\n            server: <YOUR_NFS_SERVER_IP>\n            path: <YOUR_NFS_SERVER_SHARE>If you use an nfs without provvisioner, you need to define a storageClass which is linked to your nfs.There are a lot of solutions to hold persitent volumes.Here you can find a list of StorageClasses:https://kubernetes.io/docs/concepts/storage/storage-classes/At the end it depends also where your cluster is provvisioned if you are not managing it by yourself.", "votes": 1}], "num_vote": "1", "num_answer": "1", "num_view": "454"}, {"url": "/questions/74793751/database-installation-in-kubernete-using-helm-chart-got-an-error-no-matches-for", "question_title": "Database installation in Kubernete using helm chart got an error no matches for kind \"PodDisruptionBudget\" in version \"policy/v1beta1\"", "question": "Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"PodDisruptionBudget\" in version \"policy/v1beta1\"minikube version : v1.28.0\nhelm version: v3.5.4+g1b5edb6\nOS: UbuntuI am trying to install milvus cluster database in kubernete using helm chart.", "answers": [{"content": "Search for the filepoddisruptionbudget.ymlwho should likely be located in folder ../cp-zookeeper/templateschangeapiVersion: policy/v1beta1 \nto \napiVersion: policy/v1", "votes": 3}, {"content": "It's possible that you need to update your helm repo, to get a version of the helm chart that has been updated to work with the latest kubernetes.  E.g. if you are dealing with elasticsearch, you might dohelm repo update elastic.", "votes": 0}], "num_vote": "1", "num_answer": "2", "num_view": "7k"}, {"url": "/questions/74520985/connect-to-milvus-standalone-server-in-docker-container-from-another-docker-co", "question_title": "Connect to Milvus standalone server (in docker container) from another docker container on the same host", "question": "When I run Milvus in standalone mode on docker (by executing docker-compose on the default Milvus docker-compose.yml file, resulting in the three containers being created), I cannot connect to the Milvus server from a task running in another docker container on the same host. I have configured this container to be on the same network as the Milvus server, and I can ping the Milvus server from this container via the Milvus server's IP.In the task container I run:connections.connect(\n    alias=\"default\", \n    host='192.168.192.4', \n    port='19530',\n    secure=False\n    )The error log shows:Traceback (most recent call last):\nFile \"task.py\", line 45, in\nsecure=True\nFile \"/usr/local/lib/python3.7/site-packages/pymilvus/orm/connections.py\", line 262, in connect\nconnect_milvus(**kwargs, password=password)\nFile \"/usr/local/lib/python3.7/site-packages/pymilvus/orm/connections.py\", line 233, in connect_milvus\ngh._wait_for_channel_ready()\nFile \"/usr/local/lib/python3.7/site-packages/pymilvus/client/grpc_handler.py\", line 118, in _wait_for_channel_ready\nraise MilvusException(Status.CONNECT_FAILED, f'Fail connecting to server on {self._address}. Timeout')\npymilvus.exceptions.MilvusException: <MilvusException: (code=2, message=Fail connecting to server on 192.168.192.4:19530. Timeout)>192.168.192.4 is the ip address of the milvus-standalone container.", "answers": [{"content": "Turns out this is not a Milvus issue. The problem was caused by our corporate network, and the proxy requirement. In the dockerfile, I need to set the proxy settings to be able to pull images. However, this sets the proxy settings during builld and for the container. These proxy settings prevented communication between the containers. The proxy setting needs to be reset in the dockerfile. The fix looked like this:FROM python:3.9.12\n\nENV https_proxy <proxy settings>\n\nCOPY requirements.txt /\n\nRUN pip3 install --proxy <proxy settings> -r requirements.txt\n\nCOPY task.py /\n\nENV https_proxy \"\"\n\nCMD [\"python3\", \"-u\", \"task.py\"]", "votes": 1}], "num_vote": "2", "num_answer": "1", "num_view": "2k"}, {"url": "/questions/74372964/statuscode-unavailable-internal-milvus-proxy-is-not-ready-yet-milvus-vecto", "question_title": "\"StatusCode.UNAVAILABLE, internal: Milvus Proxy is not ready yet\" - Milvus Vector Search Issue", "question": "Milvus was working fine for a while and I wanted to insert data. When I released the collection, milvus crashed with the error \"StatusCode.UNAVAILABLE, internal: Milvus Proxy is not ready yet\". I recreated the docker image several times and even tried to reinstall docker but the error persists .\nDetails:\nMilvus: 2.1.4\nETCD: 3.5.0\nminio: RELEASE.2022-03-17T06-34-49Z\nSystem: Ubuntu 22.04LTS\nThe docker compose file has been taken from:https://github.com/milvus-io/milvus/releases/download/v2.1.4/milvus-standalone-docker-compose.ymlRan the following command to drop a collection:utility.drop_collection(\"Data\")The expected output is to drop a collection without an errorthe actual output:\n[drop_collection] retry:4, cost: 0.27s, reason: <_MultiThreadedRendezvous: StatusCode.UNAVAILABLE, internal: Milvus Proxy is not ready yet. please wait>\n[drop_collection] retry:5, cost: 0.81s, reason: <_MultiThreadedRendezvous: StatusCode.UNAVAILABLE, internal: Milvus Proxy is not ready yet. please wait>\n[drop_collection] retry:6, cost: 2.43s, reason: <_MultiThreadedRendezvous: StatusCode.UNAVAILABLE, internal: Milvus Proxy is not ready yet. please wait>", "answers": [{"content": "try to remove thevolumesdir in root directory you are running the docker compose and compose againsudo docker-compose down\nsudo rm -rf  volumes\nsudo docker-compose up -d", "votes": 1}], "num_vote": "1", "num_answer": "1", "num_view": "1k"}, {"url": "/questions/74347672/cant-connect-to-milvus-using-pymilvus-inside-docker-milvusexception-code-2", "question_title": "Can't connect to Milvus using Pymilvus inside docker. MilvusException: (code=2, message=Fail connecting to server on localhost:19530. Timeout)", "question": "I'm trying to connect to a Milvus server using Pymilvus. The server is up and running but I can't connect to it:MilvusException: (code=2, message=Fail connecting to server on localhost:19530. Timeout)I'm running both using docker compose:version: \"3.5\"\n\nservices:\n\n  etcd:\n    container_name: milvus-etcd\n    image: quay.io/coreos/etcd:v3.5.0\n    networks:\n      app_net:\n    environment:\n      - ETCD_AUTO_COMPACTION_MODE=revision\n      - ETCD_AUTO_COMPACTION_RETENTION=1000\n      - ETCD_QUOTA_BACKEND_BYTES=4294967296\n    volumes:\n      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd\n    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd\n\n  minio:\n    container_name: milvus-minio\n    image: minio/minio:RELEASE.2022-03-17T06-34-49Z\n    networks:\n      app_net:\n    environment:\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n    volumes:\n      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data\n    command: minio server /minio_data\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n  \n  standalone:\n    container_name: milvus-standalone\n    image: milvusdb/milvus:v2.1.4\n    networks:\n      app_net:\n        ipv4_address: 172.16.238.10\n    command: [\"milvus\", \"run\", \"standalone\"]\n    environment:\n      ETCD_ENDPOINTS: etcd:2379\n      MINIO_ADDRESS: minio:9000\n    volumes:\n      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus\n    ports:\n      - \"19530:19530\"\n    depends_on:\n      - \"etcd\"\n      - \"minio\"\n\n\n  fastapi:\n    build: ./fastapi\n    command: uvicorn app.main:app --host 0.0.0.0\n    restart: always\n    networks:\n      app_net:\n        ipv4_address: 172.16.238.12\n    environment:\n      MILVUS_HOST: '172.16.238.10'\n    depends_on:\n      - standalone\n    ports:\n      - \"80:80\"\n    volumes:\n      - pfindertest:/data/fast\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://127.0.0.1:80\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n  \n\n\nnetworks:\n  app_net:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.16.238.0/24\n          gateway: 172.16.238.1\n    \n\nvolumes:\n  pfindertest:DockerfileFROM python:3.8\n\nWORKDIR /code\n\nCOPY ./requirements.txt /code/requirements.txt\n\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n\nCOPY ./app /code/appmain.pyfrom fastapi import FastAPI\nimport uvicorn\nfrom pymilvus import connections\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\nconnections.connect(\n  alias=\"default\", \n  host='localhost', \n  port='19530'\n)I'm getting the following error:milvus-1-fastapi-1  | Traceback (most recent call last):\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/pymilvus/client/grpc_handler.py\", line 115, in _wait_for_channel_ready\nmilvus-1-fastapi-1  |     grpc.channel_ready_future(self._channel).result(timeout=3)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/grpc/_utilities.py\", line 139, in result\nmilvus-1-fastapi-1  |     self._block(timeout)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/grpc/_utilities.py\", line 85, in _block\nmilvus-1-fastapi-1  |     raise grpc.FutureTimeoutError()\nmilvus-1-fastapi-1  | grpc.FutureTimeoutError\nmilvus-1-fastapi-1  | \nmilvus-1-fastapi-1  | During handling of the above exception, another exception occurred:\nmilvus-1-fastapi-1  | \nmilvus-1-fastapi-1  | Traceback (most recent call last):\nmilvus-1-fastapi-1  |   File \"/usr/local/bin/uvicorn\", line 8, in <module>\nmilvus-1-fastapi-1  |     sys.exit(main())\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__\nmilvus-1-fastapi-1  |     return self.main(*args, **kwargs)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1055, in main\nmilvus-1-fastapi-1  |     rv = self.invoke(ctx)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke\nmilvus-1-fastapi-1  |     return ctx.invoke(self.callback, **ctx.params)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 760, in invoke\nmilvus-1-fastapi-1  |     return __callback(*args, **kwargs)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/uvicorn/main.py\", line 404, in main\nmilvus-1-fastapi-1  |     run(\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/uvicorn/main.py\", line 569, in run\nmilvus-1-fastapi-1  |     server.run()\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/uvicorn/server.py\", line 60, in run\nmilvus-1-fastapi-1  |     return asyncio.run(self.serve(sockets=sockets))\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/asyncio/runners.py\", line 44, in run\nmilvus-1-fastapi-1  |     return loop.run_until_complete(main)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/asyncio/base_events.py\", line 616, in run_until_complete\nmilvus-1-fastapi-1  |     return future.result()\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/uvicorn/server.py\", line 67, in serve\nmilvus-1-fastapi-1  |     config.load()\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/uvicorn/config.py\", line 474, in load\nmilvus-1-fastapi-1  |     self.loaded_app = import_from_string(self.app)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/uvicorn/importer.py\", line 21, in import_from_string\nmilvus-1-fastapi-1  |     module = importlib.import_module(module_str)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/importlib/__init__.py\", line 127, in import_module\nmilvus-1-fastapi-1  |     return _bootstrap._gcd_import(name[level:], package, level)\nmilvus-1-fastapi-1  |   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\nmilvus-1-fastapi-1  |   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\nmilvus-1-fastapi-1  |   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\nmilvus-1-fastapi-1  |   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\nmilvus-1-fastapi-1  |   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\nmilvus-1-fastapi-1  |   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\nmilvus-1-fastapi-1  |   File \"/code/./app/main.py\", line 11, in <module>\nmilvus-1-fastapi-1  |     connections.connect(\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/pymilvus/orm/connections.py\", line 262, in connect\nmilvus-1-fastapi-1  |     connect_milvus(**kwargs, password=password)\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/pymilvus/orm/connections.py\", line 233, in connect_milvus\nmilvus-1-fastapi-1  |     gh._wait_for_channel_ready()\nmilvus-1-fastapi-1  |   File \"/usr/local/lib/python3.8/site-packages/pymilvus/client/grpc_handler.py\", line 118, in _wait_for_channel_ready\nmilvus-1-fastapi-1  |     raise MilvusException(Status.CONNECT_FAILED, f'Fail connecting to server on {self._address}. Timeout')\nmilvus-1-fastapi-1  | pymilvus.exceptions.MilvusException: <MilvusException: (code=2, message=Fail connecting to server on localhost:19530. Timeout)>\nmilvus-1-fastapi-1 exited with code 1The Milvus server appears to be working so that's not the problem.NAME                 COMMAND                  SERVICE             STATUS              PORTS\nmilvus-1-fastapi-1   \"uvicorn app.main:ap\u2026\"   fastapi             restarting             0.0.0.0:80->80/tcp\nmilvus-etcd          \"etcd -advertise-cli\u2026\"   etcd                running             2379-2380/tcp\nmilvus-minio         \"/usr/bin/docker-ent\u2026\"   minio               running (healthy)   9000/tcp\nmilvus-standalone    \"/tini -- milvus run\u2026\"   standalone          running             0.0.0.0:9091->9091/tcp, 0.0.0.0:19530->19530/tcpI'm running Docker on Mac if that is important. I tried using gitpod.io but the error remains.", "answers": [{"content": "use pymilvus==2.2.0 workedfrom pymilvus import (\nconnections,\nutility,\nFieldSchema,\nCollectionSchema,\nDataType,\nCollection,\n)\nconnections.connect(\"default\", host=\"localhost\", port=\"19530\")print(\"connected...\")", "votes": 1}, {"content": "For me the following solution worked:Use the container's IP address:Obtain the Milvus container's IP address usingdocker inspect milvus_container_name.Modify your Streamlit connection string to use this IP explicitly (e.g.,\"milvus://<container_ip>:19530\").Docker networking:If your Streamlit and Milvus containers reside on different Docker networks, address this:Default bridge network:Make sure both containers are part of the default bridge network.Custom network:Create a custom network (e.g.,docker network create my_network) and connect both containers to it.Explanation:Docker containers function within their own network namespaces. These potential causes are addressed as follows:IP address:Using the IP directly sidesteps possible DNS resolution issues within the containerized environment.Networking:Establishing proper communication between containers is essential for distributed applications.", "votes": 0}], "num_vote": "5", "num_answer": "2", "num_view": "6k"}, {"url": "/questions/73893882/is-there-a-way-to-mock-milvus-database", "question_title": "Is there a way to mock milvus database?", "question": "I'm working on a project for semantic search using Transfomers + milvus (1.1.1) database.\nIs there a way to mock milvus database, or run tests without having to create a milvus server.", "answers": [{"content": "According to my understanding currently, you may need to implement a mocked Milvus, and the test code for pymilvus-1.x is based on a real Milvus server. Maybe the code for pymilvus-2.x which on the master branch could be a good reference for implementing the mocked Milvus from scratch.You could find the pymilvus's Mocked Milvus here:https://github.com/milvus-io/pymilvus/blob/master/tests/mock_milvus.py", "votes": 0}], "num_vote": "-1", "num_answer": "1", "num_view": "529"}, {"url": "/questions/73775948/milvus-query-returns-only-one-result", "question_title": "milvus query returns only one result", "question": "Milvus 2.1.2I create one collection of entities made of one vector (vector) and one string (im_id).In other words, I associate one image id to each vector. I'm able to make research over the vector. I'm trying to make a query on the image id.Here is the code:#!venv/bin/python3\n\nfrom pymilvus import (\n    connections,\n    utility,\n    FieldSchema,\n    CollectionSchema,\n    DataType,\n    Collection,\n)\n\nport = 30100\nconnections.connect(\"default\", host=\"localhost\", port=port)\nutility.drop_collection(\"hello_milvus\")\nfields = [\n    FieldSchema(name=\"im_id\", dtype=DataType.VARCHAR,\n                is_primary=True, auto_id=False,\n                max_length=200),\n    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=2)\n]\nschema = CollectionSchema(fields)\nhello_milvus = Collection(\"hello_milvus\", schema)\nhello_milvus.load()\n\nvect1 = [10, 20]\nvect2 = [3, 4]\nvect3 = [5, 6]\nim_id1 = \"foo\"\nim_id2 = \"bar\"\nim_id3 = \"foo\"\n\nhello_milvus.insert([[im_id1], [vect1]]) # this is a foo\nhello_milvus.insert([[im_id2], [vect2]]) # this is a bar\nhello_milvus.insert([[im_id3], [vect3]]) # this is a foo\n\nprint(hello_milvus.num_entities)  # 3 entities, ok\n\nresults = hello_milvus.query(expr='im_id == \"foo\"',\n                             output_fields=[\"vector\"])\n\nprint(\"results\", results)This prints :[{'im_id': 'foo', 'vector': [10.0, 20.0]}].I've tried some variations, but I always get only one result.Any ideas ? What am I doing wrong ?", "answers": [{"content": "results = hello_milvus.query(expr='im_id like \"foo%\"',\noutput_fields=[\"vector\"])Maybe you can try this way to get all the qualified results.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "1k"}, {"url": "/questions/73741473/milvus-similarity-search-by-vector-id", "question_title": "Milvus, similarity search by vector id", "question": "I am trying to conduct a vector similarity search via vector's raw id (VarChar type).For example, a vector consists of three fields :auto_id (int64), userId (VarChar), vectorField (FloatVector).One possible solution in my mind is like:Retrieve the vector field vector1 of user1 by a query;Conduct another search operation over vector to retrieve the topK\nvectors in milvus.Is is possible that,given userId = \"uid1\", retrieve the topK vectors bya single query/search", "answers": [{"content": "milvus does not currently support search by id.Mainly milvus is used to do ann calculation, search by id function is more suitable for key-value database", "votes": 1}], "num_vote": "1", "num_answer": "1", "num_view": "1k"}, {"url": "/questions/73697958/milvus-connection-error-in-gcp-cloud-function", "question_title": "Milvus Connection Error in GCP Cloud Function", "question": "I have created a GCP cloud function (gen 1) in python, which connects to a Milvus deployment deployed on the same GCP project where the cloud function is created.When connecting to the Milvus deployment from my local PC through telepresence I have no issues, but there seems to be a connection error when the cloud function is triggered and runs the same code.connections.connect(alias=\"default\", host='10.127.255.140', port='19530')Specifically, when the above line is executed, it should connect to the Milvus server at the internal IP above. But the following timeout error occurs.pymilvus.exceptions.MilvusException: <MilvusException: (code=2, message=Fail connecting to server on 10.127.255.140:19530. Timeout)Has anyone had similar problems with Milvus and GCP cloud functions? I would much appreciate it if someone could help me out. Thank you!P.S. Also it is the first time uploading a question here, so please let me know if you need more info about my situation. Thanks again!", "answers": [{"content": "I met the same problem.I tried to connect with:connections.connect(alias='default', uri=MILVUS_HOST + ':' + MILVUS_PORT,\n                    user=USER, password=PASSWORD, secure=True)MILVUS_HOSTshould behttps://url", "votes": 1}], "num_vote": "2", "num_answer": "1", "num_view": "937"}, {"url": "/questions/72575531/without-k8s-cluster-how-to-deploy-milvus-distributed-cluster-on-multiple-machin", "question_title": "Without k8s cluster, how to deploy milvus distributed cluster on multiple machines?", "question": "If I don't have a k8s cluster, I need to deploy a Milvus distributed cluster on multiple machines. How should I do this? Is it possible to use the docker-compose way?", "answers": [], "num_vote": "3", "num_answer": "0", "num_view": "131"}, {"url": "/questions/72475567/why-is-my-vector-precision-lost-in-milvus", "question_title": "Why is my vector precision lost in milvus?", "question": "I am using version 2.0.2 of Milvus. When I insert 100,000 double-precision floating-point vectors into Milvus. I later use the query interface to get the original vector based on the id. But the obtained vector, I found that has become a single-precision floating-point type. why is that?", "answers": [{"content": "https://milvus.io/docs/v2.0.0/create_collection.mdhttps://milvus.io/docs/v2.0.x/schema.mdThe vector field only supports BINARY_FLOAT and FLOAT_VECTOR types.\nThey don't state it anywhere in the docs, but it is safe to assume FLOAT_VECTOR refers to a single precision floating point or np.float32 type.\nIn most machine learning applications, float32 is used to reduce the size of the neural network's memory, since most of the time float64 offers almost no benefit to accuracy.Basically, you are probably limited to using float32 since milvus doesn't seem to support float64 in the vector field at this time.", "votes": 1}], "num_vote": "0", "num_answer": "1", "num_view": "227"}, {"url": "/questions/72229445/the-distributed-version-of-milvus-2-0-loads-data-without-load-balancing", "question_title": "The distributed version of Milvus 2.0 loads data without load balancing", "question": "For the distributed version of Milvus 2.0.2, I tested to increase the number of querynodes online. It seems that the data will not be load balanced, and the memory usage of newly started nodes is very low. It will be balanced after restarting the service. What is the reason for this?", "answers": [{"content": "How much data do you have? each 512M data is a segment, and it only balanced by segment level.If you have smaller data amount and you want to scale your qps, memory replica is exactly you are looking for and we will release on 2.1", "votes": 1}], "num_vote": "0", "num_answer": "1", "num_view": "156"}, {"url": "/questions/72057041/does-milvus2-0-support-restful-api", "question_title": "Does milvus2.0 support restful API?", "question": "Milvus1.x can support restful API. I can find some docs fromhere.\nBut I don't find restful API docs for Milvus2.0.", "answers": [{"content": "I found out from thispagethat restful API is under development.", "votes": 0}], "num_vote": "-1", "num_answer": "1", "num_view": "355"}, {"url": "/questions/71526068/what-is-included-in-the-standalone-vs-cluster-milvus-deployments", "question_title": "What is included in the \u2018standalone\u2019 vs \u2018cluster\u2019 Milvus deployments?", "question": "What is included in the \u2018standalone\u2019 vs \u2018cluster\u2019 Milvus deployments?\nStandalone makes it sound like it is a single instance, but as far as I can tell it still requires deploying ETCD and MinIO.", "answers": [{"content": "A description of embedded Milvus is given here:Using Embedded Milvus to Instantly Install and Run Milvus with PythonA summary of the differences between Embedded, Standalone, Cluster and Cloud is also given.", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "1k"}, {"url": "/questions/71524810/does-milvus-auto-flush-inserted-data-to-object-storage-or-do-we-have-to-manuall", "question_title": "Does milvus auto-flush inserted data to object storage? or do we have to manually call flush?", "question": "I used the milvus1.x version in the past. After I insert data, I need to call the flush interface to ensure that the data is placed on the disk, so as to ensure that the newly inserted data can be searched. But when I used milvus2.0, I didn't find the flush() interface. Can Milvus 2.0 automatically flush newly inserted data now? Or do I have to call it manually too?", "answers": [{"content": "I'm not with the core dev team, but just confirmedhttps://github.com/milvus-io/milvus/discussions/27689All the milvus 2.x versions support automatic flush. Don't call flush() after each insert action.Flushing data isn't necessary for it to be searched. Milvus does it brute-force.\nDiving deeper, it seems milvus datanode has a buffer for 1024 rows, and flush will be automatically triggered if it's reached (or with some other conditions), and a new 1024-sized buffer will be allocated.", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "472"}, {"url": "/questions/71435540/if-i-set-auto-id-true-then-i-insert-data-into-milvus-concurrently-will-the-id", "question_title": "If I set auto_id=true, then I insert data into milvus concurrently. Will the id in Milvus be repeated?", "question": "If I set auto_id=true, then I insert data into milvus concurrently.  Will the id in Milvus be repeated?I am using Milvus2.0.", "answers": [{"content": "All returned IDs are unique, for my understanding, internally all data are inserted in sequence.Here is an official Q&A for this question:https://github.com/milvus-io/milvus/discussions/3860", "votes": 1}], "num_vote": "0", "num_answer": "1", "num_view": "2k"}, {"url": "/questions/71348408/did-anyone-deploy-milvus-on-kubernetes-cluster-on-premise", "question_title": "Did anyone deploy Milvus on kubernetes cluster on-premise?", "question": "I want to deploy Milvus to my kubernetes cluster on-premise andI follow this guideStep 1: I setup nfs server on my server with nfs-kernel-server (I'm able to mount to pod and mount to my other machine in the same network too)Step 2: I install helm chartcharts/stable/nfs-client-provisioner(I add the node selector in the end of deployment in template folder to choose which server can be used)$ helm install nfs-client . -n milvus\n  WARNING: This chart is deprecated\n  NAME: nfs-client\n  LAST DEPLOYED: Tue Mar  1 10:12:12 2022\n  NAMESPACE: milvus\n  STATUS: deployed\n  REVISION: 1\n  TEST SUITE: None\n\n$ helm list -n milvus\n  NAME          NAMESPACE   REVISION    UPDATED                                 STATUS      CHART                           APP VERSION\n  nfs-client    milvus      1           2022-03-01 10:12:12.969901256 +0700 +07 deployed    nfs-client-provisioner-1.2.11   3.1.0\n\n$ kubectl get pod -n milvus\n  NAME                                                 READY   STATUS    RESTARTS   AGE\n  nfs-client-nfs-client-provisioner-7685d96cbc-wxfkb   1/1     Running   0          6m18sStep 3: Clone branch 0.11.0 ofhttps://github.com/milvus-io/milvus-helm.git$ cd milvus-helm/charts/milvus\n$ helm install --set cluster.enabled=true --set persistence.enabled=true --set mysql.enabled=true my-release  . -n milvus\n  NAME: my-release\n  LAST DEPLOYED: Tue Mar  1 10:33:17 2022\n  NAMESPACE: milvus\n  STATUS: deployed\n  REVISION: 1\n  TEST SUITE: None\n  NOTES:\n  The Milvus server can be accessed via port 19530 on the following DNS name from within your cluster:\n  my-release-milvus.milvus.svc.cluster.local\n\n  Get the Milvus server URL by running these commands in the same shell:\n    export POD_NAME=$(kubectl get pods --namespace milvus -l \"app.kubernetes.io/name=milvus,app.kubernetes.io/instance=my-release,component=mishards\" -o jsonpath=\"{.items[0].metadata.name}\")\n    kubectl --namespace milvus port-forward $POD_NAME 19530 19121\n\n  For more information on running Milvus, visit:\n  https://milvus.io/\n\n$ kubectl get pod -n milvus\n  NAME                                                 READY   STATUS     RESTARTS   AGE\n  my-release-milvus-mishards-7cb6574bb5-jrcf4          0/1     Init:0/1   0          6m23s\n  my-release-milvus-readonly-8588bdd49-7wtwt           0/1     Pending    0          6m23s\n  my-release-milvus-writable-6db7bfc647-qrd69          0/1     Pending    0          6m23s\n  my-release-mysql-5f69d5bd87-99zd5                    0/1     Pending    0          6m23s\n  nfs-client-nfs-client-provisioner-7685d96cbc-wxfkb   1/1     Running    0          21mAs you can see pod stuck and can not running. Did anyone deploy Milvus on kubernetes cluster on-premise. Please help me!!!", "answers": [{"content": "Something to check is if you have a custom kubernetes cluster domain. Milvus is hard coded with the default kubernetes cluster domain ofcluster.local.  The problem with this is that it breaks service discovery as the etcd nodes can't find each other.", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "935"}, {"url": "/questions/71346180/after-milvus-is-started-where-is-the-newly-inserted-data-stored", "question_title": "After milvus is started, where is the newly inserted data stored?", "question": "I follow thislinkto start Milvus1.1.1.\nAfter I started it successfully, I inserted some data into it.\nBut I don't know where my data is saved.", "answers": [{"content": "For milvus1.1, the storage path is defined in server_config.yaml,storage.path, there should be like below block in configurations:storage:\n  path: /var/lib/milvusSo the default data storage location is /var/lib/milvus", "votes": 0}], "num_vote": "1", "num_answer": "1", "num_view": "263"}, {"url": "/questions/71217767/how-to-export-data-from-milvus", "question_title": "How to export data from Milvus?", "question": "We are using Milvus2.0 for vector search. And Milvus is positioned as a database. So it should provide an efficient way to export data from it.After reading Milvus Website, I only find an API named \"query\" to obtain vectors from Milvus. Then I test this API. But I found it is too slow to use it.How do I quickly export large quantities of data from Milvus?", "answers": [{"content": "Query is not designed to pull massive amount of data from Milvus for the following reasons...2 Gb rpc transfer limit per call and pagination is not implemented yetperformance is slower than pulling directly from driveThere is a tool called Milvus Data Migration (MilvusDM) tool, however, it does not support Milvus 2.x yet. The community is working on this and it will be available in future release. This is the MEP document:https://wiki.lfaidata.foundation/display/MIL/MEP+24+--+Support+bulk+load", "votes": 2}], "num_vote": "2", "num_answer": "1", "num_view": "2k"}, {"url": "/questions/71074651/does-anyone-know-the-benchmark-of-milvus-2-0-ga-standalone", "question_title": "Does anyone know the benchmark of Milvus 2.0 GA standalone?", "question": "When running Milvus 2.0 ga standalone using docker compose, what is the level of performance that can be expected for a collection of 100M 256-dimensional vectors (with an IVF_SQ8 index, nlist = 65536), in terms of # queries/second? The current performance I observe seems very slow (cannot surpass a peak of 1 req/second in the very best case, on a very large EC2 server with abundant cpu and memory). Is a max of 1 request/second the expected performance for a Milvus 2.0 standalone instance with a collection like mine, or should it be much faster?", "answers": [{"content": "The official community has released the benchmarkhttps://milvus.io/docs/v2.0.x/benchmark.md#Milvus-20-Benchmark-Test-Report", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "420"}, {"url": "/questions/70954157/modulenotfounderror-no-module-named-milvus", "question_title": "ModuleNotFoundError: No module named 'milvus'", "question": "Goal: to run this Auto LabellingNotebookon AWS SageMaker Jupyter Labs.Kernels tried:conda_pytorch_p36,conda_python3,conda_amazonei_mxnet_p27.! pip install farm-haystack -q\n# Install the latest master of Haystack\n!pip install grpcio-tools==1.34.1 -q\n!pip install git+https://github.com/deepset-ai/haystack.git -q\n!wget --no-check-certificate https://dl.xpdfreader.com/xpdf-tools-linux-4.03.tar.gz\n!tar -xvf xpdf-tools-linux-4.03.tar.gz && sudo cp xpdf-tools-linux-4.03/bin64/pdftotext /usr/local/bin\n!pip install git+https://github.com/deepset-ai/haystack.git -q# Here are the imports we need\nfrom haystack.document_stores.elasticsearch import ElasticsearchDocumentStore\nfrom haystack.nodes import PreProcessor, TransformersDocumentClassifier, FARMReader, ElasticsearchRetriever\nfrom haystack.schema import Document\nfrom haystack.utils import convert_files_to_dicts, fetch_archive_from_http, print_answersTraceback:02/02/2022 10:36:29 - INFO - faiss.loader -   Loading faiss with AVX2 support.\n02/02/2022 10:36:29 - INFO - faiss.loader -   Could not load library with AVX2 support due to:\nModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\",)\n02/02/2022 10:36:29 - INFO - faiss.loader -   Loading faiss.\n02/02/2022 10:36:29 - INFO - faiss.loader -   Successfully loaded faiss.\n02/02/2022 10:36:33 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n<ipython-input-4-6ff421127e9c> in <module>\n      1 # Here are the imports we need\n----> 2 from haystack.document_stores.elasticsearch import ElasticsearchDocumentStore\n      3 from haystack.nodes import PreProcessor, TransformersDocumentClassifier, FARMReader, ElasticsearchRetriever\n      4 from haystack.schema import Document\n      5 from haystack.utils import convert_files_to_dicts, fetch_archive_from_http, print_answers\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/__init__.py in <module>\n      3 import pandas as pd\n      4 from haystack.schema import Document, Label, MultiLabel, BaseComponent\n----> 5 from haystack.finder import Finder\n      6 from haystack.pipeline import Pipeline\n      7 \n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/finder.py in <module>\n      6 from collections import defaultdict\n      7 \n----> 8 from haystack.reader.base import BaseReader\n      9 from haystack.retriever.base import BaseRetriever\n     10 from haystack import MultiLabel\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/reader/__init__.py in <module>\n----> 1 from haystack.reader.farm import FARMReader\n      2 from haystack.reader.transformers import TransformersReader\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/reader/farm.py in <module>\n     22 \n     23 from haystack import Document\n---> 24 from haystack.document_store.base import BaseDocumentStore\n     25 from haystack.reader.base import BaseReader\n     26 \n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/document_store/__init__.py in <module>\n      2 from haystack.document_store.faiss import FAISSDocumentStore\n      3 from haystack.document_store.memory import InMemoryDocumentStore\n----> 4 from haystack.document_store.milvus import MilvusDocumentStore\n      5 from haystack.document_store.sql import SQLDocumentStore\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/document_store/milvus.py in <module>\n      5 import numpy as np\n      6 \n----> 7 from milvus import IndexType, MetricType, Milvus, Status\n      8 from scipy.special import expit\n      9 from tqdm import tqdm\n\nModuleNotFoundError: No module named 'milvus'pip install milvusimport milvusTraceback:---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n<ipython-input-3-91c33e248077> in <module>\n----> 1 import milvus\n\nModuleNotFoundError: No module named 'milvus'", "answers": [{"content": "I was facing the same problem and I got around it by simply uninstalling pymilvus and reinstalling the older version.pip listshowed pymilvus v2.0.0pip uninstall pymilvusremoved the current versionpip install pymilvus==1.1.2installed the older version and voil\u00e0;IT WORKS PEACEFULLY", "votes": 5}, {"content": "I would recommend to downgrade your milvus version to a version before the 2.0 release just a week ago. Here is a discussion on that topic:https://github.com/deepset-ai/haystack/issues/2081", "votes": 3}], "num_vote": "6", "num_answer": "2", "num_view": "7k"}, {"url": "/questions/70828416/i-am-not-able-to-install-mlivus-standalone-through-helm", "question_title": "I am not able to install Mlivus standalone through Helm", "question": "my-release-etcd-0                               1/1     Running   0          3m58smy-release-milvus-standalone-8587d4796d-r579n   0/1     Running   0          3m58smy-release-minio-54fc79dbdf-gzlsh               1/1     Running   0          3m58s[2022/01/21 07:30:00.210 +00:00] [DEBUG] [client.go:82] [\"DataCoordClient, not existed in msess \"] [key=datacoord] [\"len of msess\"=0]\n[2022/01/21 07:30:00.209 +00:00] [ERROR] [client.go:115] [\"failed to get client address\"] [error=\"number of datacoord is incorrect, 0\"] [stack=\"github.com/milvus-io/milvus/internal/util/grpcclient.(*ClientBase).connect\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/grpcclient/client.go:115\\ngithub.com/milvus-io/milvus/internal/util/grpcclient.(*ClientBase).GetGrpcClient\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/grpcclient/client.go:87\\ngithub.com/milvus-io/milvus/internal/util/grpcclient.(*ClientBase).callOnce\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/grpcclient/client.go:177\\ngithub.com/milvus-io/milvus/internal/util/grpcclient.(*ClientBase).ReCall\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/grpcclient/client.go:217\\ngithub.com/milvus-io/milvus/internal/distributed/datacoord/client.(*Client).GetComponentStates\\n\\t/go/src/github.com/milvus-io/milvus/internal/distributed/datacoord/client/client.go:110\\ngithub.com/milvus-io/milvus/internal/util/funcutil.WaitForComponentStates.func1\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/funcutil/func.go:50\\ngithub.com/milvus-io/milvus/internal/util/retry.Do\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/retry/retry.go:34\\ngithub.com/milvus-io/milvus/internal/util/funcutil.WaitForComponentStates\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/funcutil/func.go:74\\ngithub.com/milvus-io/milvus/internal/util/funcutil.WaitForComponentHealthy\\n\\t/go/src/github.com/milvus-io/milvus/internal/util/funcutil/func.go:89\\ngithub.com/milvus-io/milvus/internal/distributed/querycoord.(*Server).init\\n\\t/go/src/github.com/milvus-io/milvus/internal/distributed/querycoord/service.go:183\\ngithub.com/milvus-io/milvus/internal/distributed/querycoord.(*Server).Run\\n\\t/go/src/github.com/milvus-io/milvus/internal/distributed/querycoord/service.go:95\\ngithub.com/milvus-io/milvus/cmd/components.(*QueryCoord).Run\\n\\t/go/src/github.com/milvus-io/milvus/cmd/components/query_coord.go:50\\ngithub.com/milvus-io/milvus/cmd/roles.(*MilvusRoles).runQueryCoord.func1\\n\\t/go/src/github.com/milvus-io/milvus/cmd/roles/roles.go:178\"]", "answers": [{"content": "Can you deploy with docker-compose or Minikube?https://milvus.io/docs/v2.0.0/install_standalone-docker.mdAlso, there is a slack channel (https://slack.milvus.io/) for the Milvus community, where a lot of active community members solve Milvus-related problems together. Please join if you need further troubleshooting. Cheers!", "votes": 1}, {"content": "I had the same error and when I did a kubernetes describe pod on milvus-minio-0, I noticed the following error:Events:\nType     Reason            Age                 From               Message\n----     ------            ----                ----               -------\nWarning  FailedScheduling  55s (x12 over 14m)  default-scheduler  0/1 nodes are available: 1 Insufficient memory.I think you haven't allocated enough memory to your kubernetes cluster.", "votes": 1}], "num_vote": "0", "num_answer": "2", "num_view": "937"}, {"url": "/questions/67210869/a-question-about-the-milvus-docker-installation-port", "question_title": "A question about the Milvus docker installation port", "question": "I know that when dockers installs Milvus, port19530is the Milvus port. I would like to know what port19121does and if it can be installed without mapping this port\uff1f\nThe dockers command to install milvus is as follows\uff1asudo docker run -d --name milvus_gpu_1.0.0 --gpus all \\\n    -p 19530:19530 \\\n    -p 19121:19121 \\\n    -v /home/$USER/milvus/db:/var/lib/milvus/db \\\n    -v /home/$USER/milvus/conf:/var/lib/milvus/conf \\\n    -v /home/$USER/milvus/logs:/var/lib/milvus/logs \\\n    -v /home/$USER/milvus/wal:/var/lib/milvus/wal \\\n    milvusdb/milvus:1.0.0-gpu-d030521-1ea92e", "answers": [{"content": "Both of them seem to be used:https://milvus.io/docs/v0.10.1/milvus_config.md#Section-networkParameter   Description                               Type      Default\n-----------------------------------------------------------------------\nbind.port   Port that Milvus server monitors.         Integer   19530\nhttp.port   Port that Milvus HTTP server monitors.    Integer   19121", "votes": 0}], "num_vote": "0", "num_answer": "1", "num_view": "655"}, {"url": "/questions/67207898/a-question-about-milvus-distance-calculation", "question_title": "A question about Milvus distance calculation", "question": "I have a question about distance calculation in Milvus. In Milvus, I use the L2 distance calculation to query a vector for top1 and Milvus returns a distance of 9.340524. whereas the distance I get between the query vector and the return using the L2 formula is 2.156227.\nWhy is the formula for distance calculated differently from the result returned by Milvus?", "answers": [{"content": "The L2 distance is returned by FAISS, it is a square value.  For example, vector1=[1,2], vector2=[0, 4], the returned L2 distance is 5.You got L2 distance 9.340524, but it is not equal to 2.156227*2.156227, I think there must be some mistakes. You can do the steps to verify:create a new collectioninsert the vector that you got from the previous query(the top1 vector)query again", "votes": 3}], "num_vote": "0", "num_answer": "1", "num_view": "625"}, {"url": "/questions/65048476/im-using-gpu-enabled-milvus-but-why-it-seems-to-me-searching-is-sometimes-slow", "question_title": "I'm using GPU-enabled Milvus. But why it seems to me searching is sometimes slower than CPU-only Milvus?", "question": "I'm using GPU-enabled Milvus. But why it seems to me searching is sometimes slower than CPU-only Milvus? Are there any specific settings specifically for GPU-enabled Milvus?", "answers": [{"content": "how about your dataset counts?GPU performance better than CPU usually happens to large dataset number.", "votes": 0}, {"content": "it depends on the number of queries (nq). Before searching, Milvus needs to load the dataset into its gpu and this take significant amount of time. As nq increases to a certain value that faster search time compensates load time, gpu becomes faster than cpu.", "votes": 0}, {"content": "Extract milvus gpu executable file from dockerDownload Milvus GPUwget https://ymu.dl.osdn.jp/datasets/75612/milvus-gpu-1.1.0.tar\nor\nwget https://ymu.dl.osdn.jp/datasets/75612/milvus-gpu-1.1.1.tartar -xzvf milvus-gpu-1.1.0.tar\nor\ntar -xzvf milvus-gpu-1.1.1.tarInstall and Start Milvus GPUapt-get update\napt-get install mysql-server\nwget https://launchpadlibrarian.net/212189159/libmysqlclient18_5.6.25-0ubuntu1_amd64.deb\nwget http://launchpadlibrarian.net/212189147/libmysqlclient-dev_5.6.25-0ubuntu1_amd64.deb\ndpkg -i libmysqlclient-dev_5.6.25-0ubuntu1_amd64.deb libmysqlclient18_5.6.25-0ubuntu1_amd64.deb\napt-get install libopenblas-dev\nchmod +x /var/lib/milvus/bin/milvus_server\n./var/lib/milvus/bin/milvus_server -c ./var/lib/milvus/conf/server_config.yamlHello Milvuspip install pymilvus==1.1.0\npip install grpcio==1.37.0\npip install grpcio-tools==1.37.0wget https://raw.githubusercontent.com/milvus-io/pymilvus/v1.1.0/examples/example.py\nor\nwget https://raw.githubusercontent.com/milvus-io/pymilvus/v1.1.1/examples/example.pypython example.py\u4e2d\u6587\u53c2\u8003\uff1a\u77e9\u6c60\u4e91\u4e0a\u5b89\u88c5\u53ca\u4f7f\u7528Milvus\u6559\u7a0b", "votes": 0}], "num_vote": "1", "num_answer": "3", "num_view": "660"}, {"url": "/questions/64940901/milvus-train-and-search-on-separte-machines", "question_title": "Milvus : train and search on separte machines", "question": "I'm usingmilvusto make image similarity research in a dataset of around one million images.The basic setting is :milvus in one dockermysql in an other dockermilvus reads/writes in the mysql database.Question : is it reasonable to have mysql running on a remote machine ? How many mysql requests does Milvus do for performing one research ?If not, I guess that the most reasonable path is to copy the mysql docker with its data from the \"train machine\" to the \"search machine\". Correct ?", "answers": [{"content": "It's not a good choice to run mysql on a remote machine. There are many mysql requests when milvus does search task.\u2014\u2014\"If not, I guess that the most reasonable path is to copy the mysql docker with its data from the \"train machine\" to the \"search machine\". Correct ?\"\u2014\u2014Yes.", "votes": 1}], "num_vote": "0", "num_answer": "1", "num_view": "222"}]